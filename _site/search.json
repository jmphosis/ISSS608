[
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "Climate change is the defining global issue of our time – the world faces an increasingly urgent need to mitigate its underlying causes and adapt to its far-reaching impacts. In this regard, Singapore aims to achieve net zero emissions by 2050.\nAt the same time, Singapore is not spared from the impact of climate change. According to the National Climate Change Secretariat (NCCS), the annual mean temperature increased by 1.1°C from 26.9°C to 28.0°C between 1980 and 2020, and annual total rainfall increased at an average rate of 6.7mm per year between 1980 and 2019. In the future, NCCS expects climate change to “lead to a temperature increase of 1.4°C to 4.6°C and a rise in sea level by up to about 1m by the end of the century”. Also, the “contrast between the wet months (November to January) and dry months (February and June to September) is likely to be more pronounced” and the “intensity and frequency of heavy rainfall events is expected to increase as the world gets warmer”.\n\n\n\n\nIn this take-home exercise, the objective is to use the appropriate interactive visualisation techniques to enhance user experience in the discovery of Singapore’s weather data.\nThe key analytical questions are:\n\nHow have the mean, maximum, and minimum temperatures changed over the years?\nHow have the daily rainfall and total rainfall changed over the years?\n\n\n\n\n\n\n\nThe R packages used in this take-home exercise are:\n\ntidyverse (i.e. readr, tidyr, dplyr, ggplot2) for performing data science tasks such as importing, tidying, and wrangling data, as well as creating graphics based on The Grammar of Graphics;\nreshape2 for transforming data between wide and long formats;\nggthemr for aesthetic themes created by user, Ciarán Tobin;\nggtext for improved text rendering support for ggplot2;\nggridges for creating ridgeline plots;\nggpubr for creating publication ready ggplot2 plots;\nplotly for plotting interactive statistical graphs; and\nggstatsplot for creating visual graphics with rich statistical information.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed in the computer. If yes, they are then loaded into the R environment. If no, they are installed, and then loaded into the R environment.\n\npacman::p_load(tidyverse, reshape2,\n               ggthemr, ggtext,\n               ggridges, ggpubr,\n               plotly, ggstatsplot)\n\nThe ggthemr() function in the ggthemr package is used to set the default theme of this take-home exercise as “solarized”.\n\nggthemr(\"solarized\")\n\n\n\n\nThe Meteorological Service Singapore (MSS) provides historical daily records of temperature or rainfall data. For this take-home exercise, the month of December is chosen for the analysis given that it coincides with the Northeast Monsoon that brings about higher rainfall, and consequently, cooler temperatures. The data are taken from 1983, 1993, 2003, 2013, and 2023 (spanning 40 years). The Changi weather station is chosen for the analysis due to the comprehensive weather data collected since 1981/1982, as well as its proximity to Changi airport, which could be affected by changes in weather patterns.\n\n\n\n\n\n\nThe five datasets (one for each year) used in this take-home exercise are downloaded from MSS’ website. They are in the CSV file format.\nThe files are imported into the R environment using the read_csv() function in the readr package and stored as the R objects, weather1983, weather1993, weather2003, weather2013, and weather2023.\n\nweather1983 = read_csv(\"data/DAILYDATA_S24_198312.csv\", locale=locale(encoding=\"latin1\"))\nweather1993 = read_csv(\"data/DAILYDATA_S24_199312.csv\", locale=locale(encoding=\"latin1\"))\nweather2003 = read_csv(\"data/DAILYDATA_S24_200312.csv\", locale=locale(encoding=\"latin1\"))\nweather2013 = read_csv(\"data/DAILYDATA_S24_201312.csv\", locale=locale(encoding=\"latin1\"))\nweather2023 = read_csv(\"data/DAILYDATA_S24_202312.csv\")\n\nEach of the tibble data frames has 13 columns (variables) and 31 rows (observations).\n\n\n\nThe rbind() function in the base package is used to combine the five tibble data frames into a single tibble data frame, weather.\n\nweather = rbind(weather1983, \n                weather1993, \n                weather2003, \n                weather2013,\n                weather2023)\n\nrm(weather1983, weather1993, weather2003,\n   weather2013, weather2023)\n\nThe single tibble data frame, weather, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(weather, \"data/weather.rds\")\n\n\nweather = read_rds(\"data/weather.rds\")\n\n\n\n\nThe select() function in the dplyr package and the colnames() function in the base package are then used to obtain and rename the relevant columns respectively.\n\nweather = weather %&gt;% \n  select(c(2,4,5,9,10,11))\n\nnames = c(\"Year\", \"Day\",\n          \"Daily_Rainfall\",\n          \"Mean_Temp\",\n          \"Max_Temp\",\n          \"Min_Temp\")\n\ncolnames(weather) = names\n\nrm(names)\n\nAlso, the as.factor() function in the base package is used to convert the “Year” variable from numerical to factor data type.\n\nweather$Year = as.factor(weather$Year)\n\n\n\n\nThe dataset from MSS is expected to be relatively clean. Nevertheless, due diligence checks for duplicates and missing values are still made to confirm the assumption.\nThe duplicated() function in the base package is used to check for duplicates in weather. There are no duplicates in the tibble data frame.\n\nweather[duplicated(weather), ]\n\n# A tibble: 0 × 7\n# ℹ 7 variables: Year &lt;fct&gt;, Day &lt;dbl&gt;, Daily_Rainfall &lt;dbl&gt;, Mean_Temp &lt;dbl&gt;,\n#   Max_Temp &lt;dbl&gt;, Min_Temp &lt;dbl&gt;, Diurnal_Temp_Range &lt;dbl&gt;\n\n\nThe colSums() function in the base package is used to check for missing values in weather. There are no missing values in the tibble data frame.\n\ncolSums(is.na(weather))\n\n              Year                Day     Daily_Rainfall          Mean_Temp \n                 0                  0                  0                  0 \n          Max_Temp           Min_Temp Diurnal_Temp_Range \n                 0                  0                  0 \n\n\n\n\n\nA new variable, “Diurnal_Temp_Range” (i.e., difference between maximum and minimum daily temperatures) is then derived by subtracting the minimum daily temperatures from the maximum daily temperatures for each row.\n\nweather$Diurnal_Temp_Range = weather$Max_Temp - weather$Min_Temp\n\nThe finalised tibble data frame, weather, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(weather, \"data/weather.rds\")\n\n\nweather = read_rds(\"data/weather.rds\")\n\n\n\n\n\nExploratory data analysis (EDA) is conducted on the temperature and rainfall variables to obtain a preliminary understanding of the dataset.\n\n\nThe geom_tile() function in the ggplot2 package is used to plot the calendar heatmaps of the temperature and rainfall variables.\nSelection of Technique: The calendar heatmap is used to depict the continuous numerical values (i.e., daily temperatures or rainfall) for multiple groups (i.e., years) in chronological order from Day 1 to Day 31 in December. It provides an interesting way to visualise the variation in values within the month and across the years. They are helpful for visualising temporal trends in a compact and intuitive manner.\nDesign Principles: An informative title is provided, followed by a factual subtitle. The unit of measurement (i.e., °C or mm) is also indicated. Different colours are used for the heatmaps for different variables.\n\nMax Daily TempMin Daily TempDiurnal Temp RangeMean Daily TempDaily RainfallCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn = 1:31\n\n#Max Daily Temp\nggplot(weather,         \n       aes(Day,             \n           Year,             \n           fill = Max_Temp)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Max Temp\\n(°C)\",\n                      low = \"#FFCCCC\",\n                      high = \"#FF0000\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"More Days in Dec with Higher Max Daily Temp in 2023 than 1983\",      \n       subtitle = \"Max Temp by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5),       \n        legend.title = element_text(size = 8),       \n        legend.text = element_text(size = 6)) +   \n  scale_x_discrete(limits = c(n))\n\n#Min Daily Temp\nggplot(weather,\n       aes(Day, \n           Year,             \n           fill = Min_Temp)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Min Temp\\n(°C)\",                     \n                      low = \"light blue\",                      \n                      high = \"dark blue\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"Less Days in Dec with Lower Min Daily Temp in 2023 than 1983\",      \n       subtitle = \"Min Temp by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5),       \n        legend.title = element_text(size = 8),       \n        legend.text = element_text(size = 6)) +   \n  scale_x_discrete(limits = c(n))\n\n#Diurnal Temp Range\nggplot(weather,         \n       aes(Day,             \n           Year,             \n           fill = Diurnal_Temp_Range)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Diurnal Temp\\nRange (°C)\",                     \n                      low = \"#99CC99\",                      \n                      high = \"#006600\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"Diurnal Temp Range in Dec Largely Stable\",      \n       subtitle = \"Dirunal Temp Range by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5),       \n        legend.title = element_text(size = 8),       \n        legend.text = element_text(size = 6)) +   \n  scale_x_discrete(limits = c(n))\n\n#Mean Daily Temp\nggplot(weather,         \n       aes(Day,             \n           Year,             \n           fill = Mean_Temp)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Mean Temp\\n(°C)\",                     \n                      low = \"#CC99CC\",                      \n                      high = \"#660066\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"Mean Daily Temp in Dec Increased between 1983 and 2023\",      \n       subtitle = \"Mean Temp by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5),       \n        legend.title = element_text(size = 8),       \n        legend.text = element_text(size = 6)) +   \n  scale_x_discrete(limits = c(n))\n\n#Daily Rainfall\nggplot(weather,         \n       aes(Day,             \n           Year,             \n           fill = Daily_Rainfall)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Daily Rainfall\\n(mm)\",                     \n                      low = \"#CCCCCC\",                      \n                      high = \"black\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"Daily Rainfall in Dec Largely Stable\",      \n       subtitle = \"Daily Rainfall by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5),       \n        legend.title = element_text(size = 8),       \n        legend.text = element_text(size = 6)) +   \n  scale_x_discrete(limits = c(n))\n\n\n\n\n\nObservations:\n\nMaximum Daily Temperature: There are more days in December 2023 with higher maximum daily temperatures than in December 1983. This means that the hot period of a day in December is getting warmer over the years.\nMinimum Daily Temperature: There are less days in December 2023 with lower minimum daily temperatures than in December 1983. This means that the cool period of a day in December is getting warmer over the years.\nDiurnal Temperature Range: The diurnal temperature range in December has remained largely stable across the years. This is confirmed by the above two points that both the maximum and minimum daily temperatures have been increasing, which means that the range would remain more or less the same.\nMean Daily Temperature: The mean daily temperature in December has increased between 1983 and 2023.\nDaily Rainfall: The daily rainfall amounts in December has also remained largely stable across the years.\n\n\n\n\n\nThe melt() function in the reshape2 package is used to combine the various temperature variables’ values into a single column. The geom_jitter() function in the ggplot2 package is then used to create a dot plot of the temperature variables, with the use of different colour dots to differentiate between the different temperature variables. The geom_jitter() function is used in place of geom_point() to allow the dots to be more spread out and reduce overlaps.\nSelection of Technique: The dot plot is used to depict the individual numerical values (i.e., daily temperatures) for multiple groups (i.e., years). It provides an easy way to visualise the variation in values within the month and across the years. They are also helpful for visualising temporal trends.\nDesign Principles: An informative title is provided. Different colours are used for the dots for different temperature variables. A factual subtitle is included, which also doubles up as a legend for the different dot colours used (thereby removing the need for a legend). The unit of measurement (i.e., °C) is also indicated.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\ntemp = weather %&gt;%   \n  select(1,2,4,5,6) %&gt;%   \n  melt(id = c(\"Year\",\"Day\"))  \n\ncolnames(temp)[3] = \"Temp\"  \n\nggplot(temp,         \n       aes(x = Year,             \n           y = value)) +   \n  geom_jitter(aes(color = Temp), \n              size = 3, \n              alpha = 0.5,               \n              position = position_jitter(width = 0.2)) +   \n  labs(title = \"Overall, Between 1983 and 2023,\\nDaily Temperatures Appears To Be Increasing\",           \n       subtitle = \"&lt;span style='color:#1E90FF'&gt;Mean&lt;/span&gt;, \n       &lt;span style='color:#B22222'&gt;Max&lt;/span&gt;, and \n       &lt;span style='color:#2E8B57'&gt;Min&lt;/span&gt; Daily Temps in Dec\",           \n       x = \"Year\",           \n       y = \"Temp\\n(°C)\",        \n       colour = \"Temp\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1),         \n        plot.subtitle = element_markdown(),         \n        legend.position = \"none\" ) +   \n  scale_y_continuous(breaks = seq(20, 35))\n\n\n\n\n\nObservation: Based on the dot plot, it appears that overall, the temperatures (mean, maximum, and minimum) are increasing across the years. The dots also appear to cluster more closely in 2023 than in 1983.\n\n\n\n\nThe group_by() and summarise() functions in the dplyr package and the sum() function in the base package are used to derive a tibble data frame, rf, containing the total rainfall in December for the different years.\nThe geom_col() function in the ggplot2 package is then used to plot a bar graph of the total rainfall in December for the different years.\nSelection of Technique: The bar graph is used to depict the numerical values (i.e., total rainfall) for multiple groups (i.e., years). It provides an informative way to visualise the total values of a variable across different categories. It is also helpful for comparing the total values and for visualising temporal trends.\nDesign Principles: An informative title is provided, followed by a factual subtitle. The y-axis title is rotated for easier reading. The unit of measurement (i.e., mm) is also indicated. The exact amounts are also indicated on top of each bar for ease of reference.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nrf = weather %&gt;% \n  group_by(Year) %&gt;%\n  summarise(Total_Rainfall = sum(Daily_Rainfall))\n\nggplot(rf, \n       aes(x = Year,\n           y = Total_Rainfall)) +\n  geom_col() +\n  labs(title = \"Between 1983 and 2023,\\nTotal Rainfall in Dec has decreased\",\n          subtitle = \"Total Rainfall in Dec\",\n          x = \"Year\",\n          y = \"Total Rainfall\\n(mm)\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1)) +\n  geom_text(aes(label = Total_Rainfall), vjust = -0.5) +\n  coord_cartesian(ylim = c(0, 400))\n\n\n\n\n\nObservation: Between 1983 and 2023, the total rainfall in December has decreased. This was an interesting finding, and reflects the importance of slicing and dicing data from different angles when analysing a single issue, such as rainfall. The aggregate volume of rainfall in December has actually decreased, whereas there is widespread reports regarding increased rainfall intensities (i.e., large amounts of rain in short periods of time). This has implications for both water supply management (less rainfall overall in December) as well as drainage management (high rainfall within a short period).\n\n\n\n\nThe stat_density_ridges() function in the ggridges package are used to plot the density curves for the four temperature variables (i.e., maximum temperature, minimum temperature, diurnal temperature range, and mean temperature).\nSelection of Technique: The ridgeline plots is used to depict the distribution of continuous numerical values (i.e., temperature) for multiple groups (i.e., years). They provide a compact and informative way to visualise the distribution and shape of each group. They are helpful in identifying patterns, trends, or variations between groups.\nDesign Principles: An informative title is provided, followed by a factual subtitle. The y-axis title is rotated for easier reading. The use of “Quartiles” to colour the plots makes it easy to compare the different values such as median, 25th percentile, and 75th percentile. The unit of measurement (i.e., °C) is also indicated.\n\nMax Daily TempMin Daily TempDiurnal Temp RangeMean Daily TempCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Max Daily Temp\nggplot(weather,        \n       aes(x = Max_Temp,             \n           y = Year,             \n           fill = factor(stat(quantile)))) +   \n  stat_density_ridges(     \n    geom = \"density_ridges_gradient\",     \n    calc_ecdf = TRUE,      \n    quantiles = 4,     \n    quantile_lines = TRUE) +   \n  scale_fill_viridis_d(name = \"Quartiles\") +   \n  labs(title = \"Overall (between 1983 and 2023),\\nMax Daily Temp in Dec has increased\",      \n       subtitle = \"Distribution of Max Daily Temp\",           \n       x = \"Max Daily Temp (°C)\",           \n       y = \"Year\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Min Daily Temp\nggplot(weather,        \n       aes(x = Min_Temp,             \n           y = Year,             \n           fill = factor(stat(quantile)))) +   \n  stat_density_ridges(     \n    geom = \"density_ridges_gradient\",     \n    calc_ecdf = TRUE,      \n    quantiles = 4,     \n    quantile_lines = TRUE) +   \n  scale_fill_viridis_d(name = \"Quartiles\") +   \n  labs(title = \"Between 1983 and 2023,\\nMin Daily Temp in Dec has increased\",           \n       subtitle = \"Distribution of Min Daily Temp\",           \n       x = \"Min Daily Temp (°C)\",           \n       y = \"Year\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n#Diurnal Temp Range\nggplot(weather,        \n       aes(x = Diurnal_Temp_Range,             \n           y = Year,             \n           fill = factor(stat(quantile)))) +   \n  stat_density_ridges(     \n    geom = \"density_ridges_gradient\",     \n    calc_ecdf = TRUE,      \n    quantiles = 4,     \n    quantile_lines = TRUE) +   \n  scale_fill_viridis_d(name = \"Quartiles\") +   \n  labs(title = \"Between 1983 and 2023, Diurnal Temp Range\\nin Dec has become more concentrated at median value\",           \n       subtitle = \"Distribution of Diurnal Temp Range\",           \n       x = \"Diurnal Temp Range (°C)\",           \n       y = \"Year\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Mean Daily Temp\nggplot(weather,        \n       aes(x = Mean_Temp,             \n           y = Year,             \n           fill = factor(stat(quantile)))) +   \n  stat_density_ridges(     \n    geom = \"density_ridges_gradient\",     \n    calc_ecdf = TRUE,      \n    quantiles = 4,     \n    quantile_lines = TRUE) +   \n  scale_fill_viridis_d(name = \"Quartiles\") +   \n  labs(title = \"Between 1983 and 2023,\\nMean Daily Temp in Dec has increased\",           \n       subtitle = \"Distribution of Mean Daily Temp\",           \n       x = \"Mean Daily Temp (°C)\",           \n       y = \"Year\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n\n\n\n\nObservations:\n\nMaximum Daily Temperature: Overall (between 1983 and 2023), the maximum daily temperature in December has increased. The maximum daily temperature values were more spread out in the past, whereas the spread is now narrower.\nMinimum Daily Temperature: Between 1983 and 2023, the minimum daily temperature in December has also increased. In fact, the rise in the median minimum daily temperature is more obvious (i.e., greater) than that for median maximum daily temperature.\nDiurnal Temperature Range: Between 1983 and 2023, the median diurnal temperature range in December has not varied very much but has become more concentrated around the median value (i.e., narrower spread).\nMean Daily Temperature: Between 1983 and 2023, the daily mean temperature in December has increased. Again, the daily mean temperature values were more spread out in the past, whereas the spread is now narrower.\n\n\n\n\n\n\nConfirmatory data analysis (CDA) is then conducted on the temperature and rainfall variables to confirm the statistical significance of some of the observations obtained in the EDA.\nThe ggbetweenstats() function in the ggstatsplot package is used to conduct ANOVA tests to see if there are statistical significance for the variables across the different years.\nSelection of Technique: The combination of box and violin plots with jittered data points along with the statistical details provides confirmation and visualisation of the ANOVA tests between the values in the different years. The nonparametric test is used because, based on the calendar heatmaps and ridgeline plots, we cannot assume that the values are normally distributed.\nDesign Principles: An informative title is provided. The y-axis title is rotated for easier reading, and the unit of measurement (i.e., °C) is also indicated.\n\nMax Daily TempMin Daily TempDiurnal Temp RangeMean Daily TempDaily RainfallCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Max Daily Temp\nggbetweenstats(weather,\n               x = Year, \n               y = Max_Temp,\n               type = \"nonparametric\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE) +    \n  labs(title = \"Statistically Significant Differences in Max Daily Temp in Dec\",           \n       x = \"Year\",           \n       y = \"Max Daily\\nTemp (°C)\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Min Daily Temp\nggbetweenstats(weather,\n               x = Year, \n               y = Min_Temp,\n               type = \"nonparametric\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE) +    \n  labs(title = \"Statistically Significant Differences in Min Daily Temp in Dec\",           \n       x = \"Year\",           \n       y = \"Min Daily\\nTemp (°C)\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Diurnal Temp Range\nggbetweenstats(weather,\n               x = Year, \n               y = Diurnal_Temp_Range,\n               type = \"nonparametric\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE) +    \n  labs(title = \"No Statistically Significant Differences in Diurnal Temp Range in Dec\",           \n       x = \"Year\",           \n       y = \"Diurnal Temp\\nRange (°C)\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Mean Daily Temp\nggbetweenstats(weather,\n               x = Year, \n               y = Mean_Temp,\n               type = \"nonparametric\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE) +    \n  labs(title = \"Statistically Significant Differences in Mean Daily Temp in Dec\",\n       x = \"Year\",           \n       y = \"Mean Daily\\nTemp (°C)\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Total Rainfall\nggbetweenstats(weather,\n               x = Year, \n               y = Max_Temp,\n               type = \"nonparametric\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE) +    \n  labs(title = \"No Statistically Significant Differences in Daily Rainfall in Dec\",           \n       x = \"Year\",           \n       y = \"Daily Rainfall\\n(mm)\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n\n\n\n\nObservations:\n\nMaximum Daily Temperature: There are statistically significant differences in the maximum daily temperatures in December. Comparing pairwise, there are five pairs of years with statistically significant differences.\nMinimum Daily Temperature: There are statistically significant differences in the minimum daily temperatures in December. Comparing pairwise, there are sevenpairs of years with statistically significant differences.\nDiurnal Temperature Range: There are no statistically significant differences in the maximum daily temperatures in December.\nMean Daily Temperature: There are statistically significant differences in the mean daily temperatures in December. Comparing pairwise, there are seven pairs of years with statistically significant differences.\nDaily Rainfall: There are no statistically significant differences in the daily rainfall amounts in December.\n\n\n\n\n\nThe various plots are then put together in a single analytics-driven data visualisation to tell a story about temperature and rainfall at the Changi weather station in December across the five years. The functions used are ggarrange() and annotate_figure() from the ggpubr package.\nThe selection of techniques, design principles, and observations for each sub-plot are found at the respective sub-sections in sections 4 and 5 above.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nc = ggplot(weather,         \n       aes(Day,             \n           Year,             \n           fill = Diurnal_Temp_Range)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Diurnal Temp\\nRange (°C)\",                     \n                      low = \"#99CC99\",                      \n                      high = \"#006600\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"Diurnal Temp Range in Dec Largely Stable\",      \n       subtitle = \"Dirunal Temp Range by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5, size = 8), \n        plot.subtitle = element_text(size = 7),\n        axis.text = element_text(size = 5),          \n        legend.title = element_text(size = 5),       \n        legend.text = element_text(size = 5)) +   \n  scale_x_discrete(limits = c(n))\n\nd = ggplot(temp,         \n       aes(x = Year,             \n           y = value)) +   \n  geom_jitter(aes(color = Temp), \n              size = 3, \n              alpha = 0.5,               \n              position = position_jitter(width = 0.2)) +   \n  labs(title = \"Overall, Between 1983 and 2023,\\nDaily Temperatures Appears To Be Increasing\",           \n       subtitle = \"&lt;span style='color:#1E90FF'&gt;Mean&lt;/span&gt;, \n       &lt;span style='color:#B22222'&gt;Max&lt;/span&gt;, and \n       &lt;span style='color:#2E8B57'&gt;Min&lt;/span&gt; Daily Temps in Dec\",           \n       x = \"Year\",           \n       y = \"Temp\\n(°C)\",        \n       colour = \"Temp\") +   \n  theme(plot.title = element_text(hjust = 0.5, size = 8),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1),         \n        plot.subtitle = element_markdown(size = 7), \n        axis.text = element_text(size = 5),     \n        axis.title = element_text(size = 5),     \n        legend.position = \"none\" ) +\n  scale_y_continuous(breaks = seq(20, 35))\n\nb = ggplot(rf, \n       aes(x = Year,\n           y = Total_Rainfall)) +\n  geom_col() +\n  labs(title = \"Between 1983 and 2023,\\nTotal Rainfall in Dec has decreased\",\n          subtitle = \"Total Rainfall in Dec\",\n          x = \"Year\",\n          y = \"Total Rainfall\\n(mm)\") +\n  theme(plot.title = element_text(hjust = 0.5, size = 8),\n        plot.subtitle = element_text(size = 7),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1),\n        axis.text = element_text(size = 5),     \n        axis.title = element_text(size = 5)) +\n  geom_text(aes(label = Total_Rainfall), vjust = -0.5, size = 2.5) +\n  coord_cartesian(ylim = c(0, 400))\n\n\nr= ggplot(weather,        \n       aes(x = Max_Temp,             \n           y = Year,             \n           fill = factor(stat(quantile)))) +   \n  stat_density_ridges(     \n    geom = \"density_ridges_gradient\",     \n    calc_ecdf = TRUE,      \n    quantiles = 4,     \n    quantile_lines = TRUE) +   \n  scale_fill_viridis_d(name = \"Quartiles\") +   \n  labs(title = \"Overall (between 1983 and 2023),\\nMax Daily Temp in Dec has increased\",\n       subtitle = \"Distribution of Max Daily Temp\",           \n       x = \"Max Daily Temp (°C)\",           \n       y = \"Year\") +   \n  theme(plot.title = element_text(hjust = 0.5, size = 8),         \n        plot.subtitle = element_text(size = 7),\n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1),\n        axis.text = element_text(size = 5),     \n        axis.title = element_text(size = 5),          \n        legend.title = element_text(size = 5),       \n        legend.text = element_text(size = 5))\n\nplot = ggarrange(d, b, c, r,  \n          nrow = 2, \n          ncol = 2)\n          \nannotate_figure(plot,\n                top = text_grob('Changes in December Temperatures and Rainfall Across The Years'),\n                fig.lab.face = \"bold\")\n\n\n\n\n\n\n\nThe dot plot and calendar heatmaps are then converted into interactive plots using the ggplotly() function in the ggplotly package. This would allow users to explore the dataset in further detail. In addition, a time-series dot plot is also created to animate the changes in the mean, maximum, and minimum daily temperatures in December across the five years.\n\nDot PlotTime Series Dot PlotCalendar HeatmapCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Dot Plot\nggplotly(d)\n\n#Time Series Dot Plot\nid = ggplot(temp,         \n       aes(x = Year,             \n           y = value)) +   \n  geom_jitter(aes(color = Temp,\n                  frame = Year), \n              size = 3, \n              alpha = 0.5,               \n              position = position_jitter(width = 0.2)) +   \n  labs(title = \"Overall, Between 1983 and 2023,\\nDaily Temperatures Appears To Be Increasing\", \n       x = \"Year\",           \n       y = \"Temp\\n(°C)\",        \n       colour = \"Temp\") +\n  theme(plot.title = element_text(hjust = 0.5, size = 8),         \n        plot.subtitle = element_text(size = 7),\n        axis.title.y = element_text(angle=360,\n                                    vjust=.5, \n                                    hjust=1),\n        axis.text = element_text(size = 5),     \n        axis.title = element_text(size = 5),          \n        legend.title = element_text(size = 5),       \n        legend.text = element_text(size = 5))\n\nggplotly(id)\n\n#Calendar Heatmaps\ncal = ggplot(temp, \n       aes(Day, \n           Year, \n           fill = value)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  coord_equal() +\n  scale_fill_gradient(name = \"Temp\\n(°C)\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~Temp, ncol = 1) +\n  labs(x = NULL, y = NULL, \n     title = \"Daily Temperatures in Dec\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )\n\nggplotly(cal)\n\n\n\n\n\n\n\nIn conclusion, the ggplot2 package is a powerful package for exploratory data analysis through visualisation. The weather dataset from MSS is interesting and contains useful data for studying Singapore’s weather conditions. The insights gained from analysing the analytical questions posed in this take-home exercise provides a preview of the data analyses that can be conducted in further studies (e.g., expand to include other weather stations across Singapore) to better understand how the various weather conditions have varied across the years. The take-home exercise also highlighted the importance of examining variables from multiple angles (e.g., distribution, variance) and understanding their implications on people (e.g., maximum and minimum temperatures, and diurnal temperature ranges may reveal more about lived experiences as compared to mean temperature).\n\n\n\n\nR for Visual Analytics.\nR for Data Science.\nFundamentals of Data Visualisation.\n\n~~~ End of Take-home Exercise 3 ~~~"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#introduction",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#introduction",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "Climate change is the defining global issue of our time – the world faces an increasingly urgent need to mitigate its underlying causes and adapt to its far-reaching impacts. In this regard, Singapore aims to achieve net zero emissions by 2050.\nAt the same time, Singapore is not spared from the impact of climate change. According to the National Climate Change Secretariat (NCCS), the annual mean temperature increased by 1.1°C from 26.9°C to 28.0°C between 1980 and 2020, and annual total rainfall increased at an average rate of 6.7mm per year between 1980 and 2019. In the future, NCCS expects climate change to “lead to a temperature increase of 1.4°C to 4.6°C and a rise in sea level by up to about 1m by the end of the century”. Also, the “contrast between the wet months (November to January) and dry months (February and June to September) is likely to be more pronounced” and the “intensity and frequency of heavy rainfall events is expected to increase as the world gets warmer”.\n\n\n\n\nIn this take-home exercise, the objective is to use the appropriate interactive visualisation techniques to enhance user experience in the discovery of Singapore’s weather data.\nThe key analytical questions are:\n\nHow have the mean, maximum, and minimum temperatures changed over the years?\nHow have the daily rainfall and total rainfall changed over the years?"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#getting-started",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "The R packages used in this take-home exercise are:\n\ntidyverse (i.e. readr, tidyr, dplyr, ggplot2) for performing data science tasks such as importing, tidying, and wrangling data, as well as creating graphics based on The Grammar of Graphics;\nreshape2 for transforming data between wide and long formats;\nggthemr for aesthetic themes created by user, Ciarán Tobin;\nggtext for improved text rendering support for ggplot2;\nggridges for creating ridgeline plots;\nggpubr for creating publication ready ggplot2 plots;\nplotly for plotting interactive statistical graphs; and\nggstatsplot for creating visual graphics with rich statistical information.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed in the computer. If yes, they are then loaded into the R environment. If no, they are installed, and then loaded into the R environment.\n\npacman::p_load(tidyverse, reshape2,\n               ggthemr, ggtext,\n               ggridges, ggpubr,\n               plotly, ggstatsplot)\n\nThe ggthemr() function in the ggthemr package is used to set the default theme of this take-home exercise as “solarized”.\n\nggthemr(\"solarized\")\n\n\n\n\nThe Meteorological Service Singapore (MSS) provides historical daily records of temperature or rainfall data. For this take-home exercise, the month of December is chosen for the analysis given that it coincides with the Northeast Monsoon that brings about higher rainfall, and consequently, cooler temperatures. The data are taken from 1983, 1993, 2003, 2013, and 2023 (spanning 40 years). The Changi weather station is chosen for the analysis due to the comprehensive weather data collected since 1981/1982, as well as its proximity to Changi airport, which could be affected by changes in weather patterns."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#data-wrangling",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "The five datasets (one for each year) used in this take-home exercise are downloaded from MSS’ website. They are in the CSV file format.\nThe files are imported into the R environment using the read_csv() function in the readr package and stored as the R objects, weather1983, weather1993, weather2003, weather2013, and weather2023.\n\nweather1983 = read_csv(\"data/DAILYDATA_S24_198312.csv\", locale=locale(encoding=\"latin1\"))\nweather1993 = read_csv(\"data/DAILYDATA_S24_199312.csv\", locale=locale(encoding=\"latin1\"))\nweather2003 = read_csv(\"data/DAILYDATA_S24_200312.csv\", locale=locale(encoding=\"latin1\"))\nweather2013 = read_csv(\"data/DAILYDATA_S24_201312.csv\", locale=locale(encoding=\"latin1\"))\nweather2023 = read_csv(\"data/DAILYDATA_S24_202312.csv\")\n\nEach of the tibble data frames has 13 columns (variables) and 31 rows (observations).\n\n\n\nThe rbind() function in the base package is used to combine the five tibble data frames into a single tibble data frame, weather.\n\nweather = rbind(weather1983, \n                weather1993, \n                weather2003, \n                weather2013,\n                weather2023)\n\nrm(weather1983, weather1993, weather2003,\n   weather2013, weather2023)\n\nThe single tibble data frame, weather, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(weather, \"data/weather.rds\")\n\n\nweather = read_rds(\"data/weather.rds\")\n\n\n\n\nThe select() function in the dplyr package and the colnames() function in the base package are then used to obtain and rename the relevant columns respectively.\n\nweather = weather %&gt;% \n  select(c(2,4,5,9,10,11))\n\nnames = c(\"Year\", \"Day\",\n          \"Daily_Rainfall\",\n          \"Mean_Temp\",\n          \"Max_Temp\",\n          \"Min_Temp\")\n\ncolnames(weather) = names\n\nrm(names)\n\nAlso, the as.factor() function in the base package is used to convert the “Year” variable from numerical to factor data type.\n\nweather$Year = as.factor(weather$Year)\n\n\n\n\nThe dataset from MSS is expected to be relatively clean. Nevertheless, due diligence checks for duplicates and missing values are still made to confirm the assumption.\nThe duplicated() function in the base package is used to check for duplicates in weather. There are no duplicates in the tibble data frame.\n\nweather[duplicated(weather), ]\n\n# A tibble: 0 × 7\n# ℹ 7 variables: Year &lt;fct&gt;, Day &lt;dbl&gt;, Daily_Rainfall &lt;dbl&gt;, Mean_Temp &lt;dbl&gt;,\n#   Max_Temp &lt;dbl&gt;, Min_Temp &lt;dbl&gt;, Diurnal_Temp_Range &lt;dbl&gt;\n\n\nThe colSums() function in the base package is used to check for missing values in weather. There are no missing values in the tibble data frame.\n\ncolSums(is.na(weather))\n\n              Year                Day     Daily_Rainfall          Mean_Temp \n                 0                  0                  0                  0 \n          Max_Temp           Min_Temp Diurnal_Temp_Range \n                 0                  0                  0 \n\n\n\n\n\nA new variable, “Diurnal_Temp_Range” (i.e., difference between maximum and minimum daily temperatures) is then derived by subtracting the minimum daily temperatures from the maximum daily temperatures for each row.\n\nweather$Diurnal_Temp_Range = weather$Max_Temp - weather$Min_Temp\n\nThe finalised tibble data frame, weather, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(weather, \"data/weather.rds\")\n\n\nweather = read_rds(\"data/weather.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#exploratory-data-analysis",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#exploratory-data-analysis",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "Exploratory data analysis (EDA) is conducted on the temperature and rainfall variables to obtain a preliminary understanding of the dataset.\n\n\nThe geom_tile() function in the ggplot2 package is used to plot the calendar heatmaps of the temperature and rainfall variables.\nSelection of Technique: The calendar heatmap is used to depict the continuous numerical values (i.e., daily temperatures or rainfall) for multiple groups (i.e., years) in chronological order from Day 1 to Day 31 in December. It provides an interesting way to visualise the variation in values within the month and across the years. They are helpful for visualising temporal trends in a compact and intuitive manner.\nDesign Principles: An informative title is provided, followed by a factual subtitle. The unit of measurement (i.e., °C or mm) is also indicated. Different colours are used for the heatmaps for different variables.\n\nMax Daily TempMin Daily TempDiurnal Temp RangeMean Daily TempDaily RainfallCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn = 1:31\n\n#Max Daily Temp\nggplot(weather,         \n       aes(Day,             \n           Year,             \n           fill = Max_Temp)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Max Temp\\n(°C)\",\n                      low = \"#FFCCCC\",\n                      high = \"#FF0000\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"More Days in Dec with Higher Max Daily Temp in 2023 than 1983\",      \n       subtitle = \"Max Temp by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5),       \n        legend.title = element_text(size = 8),       \n        legend.text = element_text(size = 6)) +   \n  scale_x_discrete(limits = c(n))\n\n#Min Daily Temp\nggplot(weather,\n       aes(Day, \n           Year,             \n           fill = Min_Temp)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Min Temp\\n(°C)\",                     \n                      low = \"light blue\",                      \n                      high = \"dark blue\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"Less Days in Dec with Lower Min Daily Temp in 2023 than 1983\",      \n       subtitle = \"Min Temp by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5),       \n        legend.title = element_text(size = 8),       \n        legend.text = element_text(size = 6)) +   \n  scale_x_discrete(limits = c(n))\n\n#Diurnal Temp Range\nggplot(weather,         \n       aes(Day,             \n           Year,             \n           fill = Diurnal_Temp_Range)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Diurnal Temp\\nRange (°C)\",                     \n                      low = \"#99CC99\",                      \n                      high = \"#006600\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"Diurnal Temp Range in Dec Largely Stable\",      \n       subtitle = \"Dirunal Temp Range by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5),       \n        legend.title = element_text(size = 8),       \n        legend.text = element_text(size = 6)) +   \n  scale_x_discrete(limits = c(n))\n\n#Mean Daily Temp\nggplot(weather,         \n       aes(Day,             \n           Year,             \n           fill = Mean_Temp)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Mean Temp\\n(°C)\",                     \n                      low = \"#CC99CC\",                      \n                      high = \"#660066\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"Mean Daily Temp in Dec Increased between 1983 and 2023\",      \n       subtitle = \"Mean Temp by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5),       \n        legend.title = element_text(size = 8),       \n        legend.text = element_text(size = 6)) +   \n  scale_x_discrete(limits = c(n))\n\n#Daily Rainfall\nggplot(weather,         \n       aes(Day,             \n           Year,             \n           fill = Daily_Rainfall)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Daily Rainfall\\n(mm)\",                     \n                      low = \"#CCCCCC\",                      \n                      high = \"black\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"Daily Rainfall in Dec Largely Stable\",      \n       subtitle = \"Daily Rainfall by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5),       \n        legend.title = element_text(size = 8),       \n        legend.text = element_text(size = 6)) +   \n  scale_x_discrete(limits = c(n))\n\n\n\n\n\nObservations:\n\nMaximum Daily Temperature: There are more days in December 2023 with higher maximum daily temperatures than in December 1983. This means that the hot period of a day in December is getting warmer over the years.\nMinimum Daily Temperature: There are less days in December 2023 with lower minimum daily temperatures than in December 1983. This means that the cool period of a day in December is getting warmer over the years.\nDiurnal Temperature Range: The diurnal temperature range in December has remained largely stable across the years. This is confirmed by the above two points that both the maximum and minimum daily temperatures have been increasing, which means that the range would remain more or less the same.\nMean Daily Temperature: The mean daily temperature in December has increased between 1983 and 2023.\nDaily Rainfall: The daily rainfall amounts in December has also remained largely stable across the years.\n\n\n\n\n\nThe melt() function in the reshape2 package is used to combine the various temperature variables’ values into a single column. The geom_jitter() function in the ggplot2 package is then used to create a dot plot of the temperature variables, with the use of different colour dots to differentiate between the different temperature variables. The geom_jitter() function is used in place of geom_point() to allow the dots to be more spread out and reduce overlaps.\nSelection of Technique: The dot plot is used to depict the individual numerical values (i.e., daily temperatures) for multiple groups (i.e., years). It provides an easy way to visualise the variation in values within the month and across the years. They are also helpful for visualising temporal trends.\nDesign Principles: An informative title is provided. Different colours are used for the dots for different temperature variables. A factual subtitle is included, which also doubles up as a legend for the different dot colours used (thereby removing the need for a legend). The unit of measurement (i.e., °C) is also indicated.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\ntemp = weather %&gt;%   \n  select(1,2,4,5,6) %&gt;%   \n  melt(id = c(\"Year\",\"Day\"))  \n\ncolnames(temp)[3] = \"Temp\"  \n\nggplot(temp,         \n       aes(x = Year,             \n           y = value)) +   \n  geom_jitter(aes(color = Temp), \n              size = 3, \n              alpha = 0.5,               \n              position = position_jitter(width = 0.2)) +   \n  labs(title = \"Overall, Between 1983 and 2023,\\nDaily Temperatures Appears To Be Increasing\",           \n       subtitle = \"&lt;span style='color:#1E90FF'&gt;Mean&lt;/span&gt;, \n       &lt;span style='color:#B22222'&gt;Max&lt;/span&gt;, and \n       &lt;span style='color:#2E8B57'&gt;Min&lt;/span&gt; Daily Temps in Dec\",           \n       x = \"Year\",           \n       y = \"Temp\\n(°C)\",        \n       colour = \"Temp\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1),         \n        plot.subtitle = element_markdown(),         \n        legend.position = \"none\" ) +   \n  scale_y_continuous(breaks = seq(20, 35))\n\n\n\n\n\nObservation: Based on the dot plot, it appears that overall, the temperatures (mean, maximum, and minimum) are increasing across the years. The dots also appear to cluster more closely in 2023 than in 1983.\n\n\n\n\nThe group_by() and summarise() functions in the dplyr package and the sum() function in the base package are used to derive a tibble data frame, rf, containing the total rainfall in December for the different years.\nThe geom_col() function in the ggplot2 package is then used to plot a bar graph of the total rainfall in December for the different years.\nSelection of Technique: The bar graph is used to depict the numerical values (i.e., total rainfall) for multiple groups (i.e., years). It provides an informative way to visualise the total values of a variable across different categories. It is also helpful for comparing the total values and for visualising temporal trends.\nDesign Principles: An informative title is provided, followed by a factual subtitle. The y-axis title is rotated for easier reading. The unit of measurement (i.e., mm) is also indicated. The exact amounts are also indicated on top of each bar for ease of reference.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nrf = weather %&gt;% \n  group_by(Year) %&gt;%\n  summarise(Total_Rainfall = sum(Daily_Rainfall))\n\nggplot(rf, \n       aes(x = Year,\n           y = Total_Rainfall)) +\n  geom_col() +\n  labs(title = \"Between 1983 and 2023,\\nTotal Rainfall in Dec has decreased\",\n          subtitle = \"Total Rainfall in Dec\",\n          x = \"Year\",\n          y = \"Total Rainfall\\n(mm)\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1)) +\n  geom_text(aes(label = Total_Rainfall), vjust = -0.5) +\n  coord_cartesian(ylim = c(0, 400))\n\n\n\n\n\nObservation: Between 1983 and 2023, the total rainfall in December has decreased. This was an interesting finding, and reflects the importance of slicing and dicing data from different angles when analysing a single issue, such as rainfall. The aggregate volume of rainfall in December has actually decreased, whereas there is widespread reports regarding increased rainfall intensities (i.e., large amounts of rain in short periods of time). This has implications for both water supply management (less rainfall overall in December) as well as drainage management (high rainfall within a short period).\n\n\n\n\nThe stat_density_ridges() function in the ggridges package are used to plot the density curves for the four temperature variables (i.e., maximum temperature, minimum temperature, diurnal temperature range, and mean temperature).\nSelection of Technique: The ridgeline plots is used to depict the distribution of continuous numerical values (i.e., temperature) for multiple groups (i.e., years). They provide a compact and informative way to visualise the distribution and shape of each group. They are helpful in identifying patterns, trends, or variations between groups.\nDesign Principles: An informative title is provided, followed by a factual subtitle. The y-axis title is rotated for easier reading. The use of “Quartiles” to colour the plots makes it easy to compare the different values such as median, 25th percentile, and 75th percentile. The unit of measurement (i.e., °C) is also indicated.\n\nMax Daily TempMin Daily TempDiurnal Temp RangeMean Daily TempCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Max Daily Temp\nggplot(weather,        \n       aes(x = Max_Temp,             \n           y = Year,             \n           fill = factor(stat(quantile)))) +   \n  stat_density_ridges(     \n    geom = \"density_ridges_gradient\",     \n    calc_ecdf = TRUE,      \n    quantiles = 4,     \n    quantile_lines = TRUE) +   \n  scale_fill_viridis_d(name = \"Quartiles\") +   \n  labs(title = \"Overall (between 1983 and 2023),\\nMax Daily Temp in Dec has increased\",      \n       subtitle = \"Distribution of Max Daily Temp\",           \n       x = \"Max Daily Temp (°C)\",           \n       y = \"Year\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Min Daily Temp\nggplot(weather,        \n       aes(x = Min_Temp,             \n           y = Year,             \n           fill = factor(stat(quantile)))) +   \n  stat_density_ridges(     \n    geom = \"density_ridges_gradient\",     \n    calc_ecdf = TRUE,      \n    quantiles = 4,     \n    quantile_lines = TRUE) +   \n  scale_fill_viridis_d(name = \"Quartiles\") +   \n  labs(title = \"Between 1983 and 2023,\\nMin Daily Temp in Dec has increased\",           \n       subtitle = \"Distribution of Min Daily Temp\",           \n       x = \"Min Daily Temp (°C)\",           \n       y = \"Year\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n#Diurnal Temp Range\nggplot(weather,        \n       aes(x = Diurnal_Temp_Range,             \n           y = Year,             \n           fill = factor(stat(quantile)))) +   \n  stat_density_ridges(     \n    geom = \"density_ridges_gradient\",     \n    calc_ecdf = TRUE,      \n    quantiles = 4,     \n    quantile_lines = TRUE) +   \n  scale_fill_viridis_d(name = \"Quartiles\") +   \n  labs(title = \"Between 1983 and 2023, Diurnal Temp Range\\nin Dec has become more concentrated at median value\",           \n       subtitle = \"Distribution of Diurnal Temp Range\",           \n       x = \"Diurnal Temp Range (°C)\",           \n       y = \"Year\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Mean Daily Temp\nggplot(weather,        \n       aes(x = Mean_Temp,             \n           y = Year,             \n           fill = factor(stat(quantile)))) +   \n  stat_density_ridges(     \n    geom = \"density_ridges_gradient\",     \n    calc_ecdf = TRUE,      \n    quantiles = 4,     \n    quantile_lines = TRUE) +   \n  scale_fill_viridis_d(name = \"Quartiles\") +   \n  labs(title = \"Between 1983 and 2023,\\nMean Daily Temp in Dec has increased\",           \n       subtitle = \"Distribution of Mean Daily Temp\",           \n       x = \"Mean Daily Temp (°C)\",           \n       y = \"Year\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n\n\n\n\nObservations:\n\nMaximum Daily Temperature: Overall (between 1983 and 2023), the maximum daily temperature in December has increased. The maximum daily temperature values were more spread out in the past, whereas the spread is now narrower.\nMinimum Daily Temperature: Between 1983 and 2023, the minimum daily temperature in December has also increased. In fact, the rise in the median minimum daily temperature is more obvious (i.e., greater) than that for median maximum daily temperature.\nDiurnal Temperature Range: Between 1983 and 2023, the median diurnal temperature range in December has not varied very much but has become more concentrated around the median value (i.e., narrower spread).\nMean Daily Temperature: Between 1983 and 2023, the daily mean temperature in December has increased. Again, the daily mean temperature values were more spread out in the past, whereas the spread is now narrower."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#confirmatory-data-analysis",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#confirmatory-data-analysis",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "Confirmatory data analysis (CDA) is then conducted on the temperature and rainfall variables to confirm the statistical significance of some of the observations obtained in the EDA.\nThe ggbetweenstats() function in the ggstatsplot package is used to conduct ANOVA tests to see if there are statistical significance for the variables across the different years.\nSelection of Technique: The combination of box and violin plots with jittered data points along with the statistical details provides confirmation and visualisation of the ANOVA tests between the values in the different years. The nonparametric test is used because, based on the calendar heatmaps and ridgeline plots, we cannot assume that the values are normally distributed.\nDesign Principles: An informative title is provided. The y-axis title is rotated for easier reading, and the unit of measurement (i.e., °C) is also indicated.\n\nMax Daily TempMin Daily TempDiurnal Temp RangeMean Daily TempDaily RainfallCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Max Daily Temp\nggbetweenstats(weather,\n               x = Year, \n               y = Max_Temp,\n               type = \"nonparametric\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE) +    \n  labs(title = \"Statistically Significant Differences in Max Daily Temp in Dec\",           \n       x = \"Year\",           \n       y = \"Max Daily\\nTemp (°C)\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Min Daily Temp\nggbetweenstats(weather,\n               x = Year, \n               y = Min_Temp,\n               type = \"nonparametric\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE) +    \n  labs(title = \"Statistically Significant Differences in Min Daily Temp in Dec\",           \n       x = \"Year\",           \n       y = \"Min Daily\\nTemp (°C)\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Diurnal Temp Range\nggbetweenstats(weather,\n               x = Year, \n               y = Diurnal_Temp_Range,\n               type = \"nonparametric\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE) +    \n  labs(title = \"No Statistically Significant Differences in Diurnal Temp Range in Dec\",           \n       x = \"Year\",           \n       y = \"Diurnal Temp\\nRange (°C)\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Mean Daily Temp\nggbetweenstats(weather,\n               x = Year, \n               y = Mean_Temp,\n               type = \"nonparametric\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE) +    \n  labs(title = \"Statistically Significant Differences in Mean Daily Temp in Dec\",\n       x = \"Year\",           \n       y = \"Mean Daily\\nTemp (°C)\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Total Rainfall\nggbetweenstats(weather,\n               x = Year, \n               y = Max_Temp,\n               type = \"nonparametric\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE) +    \n  labs(title = \"No Statistically Significant Differences in Daily Rainfall in Dec\",           \n       x = \"Year\",           \n       y = \"Daily Rainfall\\n(mm)\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n\n\n\n\nObservations:\n\nMaximum Daily Temperature: There are statistically significant differences in the maximum daily temperatures in December. Comparing pairwise, there are five pairs of years with statistically significant differences.\nMinimum Daily Temperature: There are statistically significant differences in the minimum daily temperatures in December. Comparing pairwise, there are sevenpairs of years with statistically significant differences.\nDiurnal Temperature Range: There are no statistically significant differences in the maximum daily temperatures in December.\nMean Daily Temperature: There are statistically significant differences in the mean daily temperatures in December. Comparing pairwise, there are seven pairs of years with statistically significant differences.\nDaily Rainfall: There are no statistically significant differences in the daily rainfall amounts in December."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#composite-plot",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#composite-plot",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "The various plots are then put together in a single analytics-driven data visualisation to tell a story about temperature and rainfall at the Changi weather station in December across the five years. The functions used are ggarrange() and annotate_figure() from the ggpubr package.\nThe selection of techniques, design principles, and observations for each sub-plot are found at the respective sub-sections in sections 4 and 5 above.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nc = ggplot(weather,         \n       aes(Day,             \n           Year,             \n           fill = Diurnal_Temp_Range)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Diurnal Temp\\nRange (°C)\",                     \n                      low = \"#99CC99\",                      \n                      high = \"#006600\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"Diurnal Temp Range in Dec Largely Stable\",      \n       subtitle = \"Dirunal Temp Range by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5, size = 8), \n        plot.subtitle = element_text(size = 7),\n        axis.text = element_text(size = 5),          \n        legend.title = element_text(size = 5),       \n        legend.text = element_text(size = 5)) +   \n  scale_x_discrete(limits = c(n))\n\nd = ggplot(temp,         \n       aes(x = Year,             \n           y = value)) +   \n  geom_jitter(aes(color = Temp), \n              size = 3, \n              alpha = 0.5,               \n              position = position_jitter(width = 0.2)) +   \n  labs(title = \"Overall, Between 1983 and 2023,\\nDaily Temperatures Appears To Be Increasing\",           \n       subtitle = \"&lt;span style='color:#1E90FF'&gt;Mean&lt;/span&gt;, \n       &lt;span style='color:#B22222'&gt;Max&lt;/span&gt;, and \n       &lt;span style='color:#2E8B57'&gt;Min&lt;/span&gt; Daily Temps in Dec\",           \n       x = \"Year\",           \n       y = \"Temp\\n(°C)\",        \n       colour = \"Temp\") +   \n  theme(plot.title = element_text(hjust = 0.5, size = 8),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1),         \n        plot.subtitle = element_markdown(size = 7), \n        axis.text = element_text(size = 5),     \n        axis.title = element_text(size = 5),     \n        legend.position = \"none\" ) +\n  scale_y_continuous(breaks = seq(20, 35))\n\nb = ggplot(rf, \n       aes(x = Year,\n           y = Total_Rainfall)) +\n  geom_col() +\n  labs(title = \"Between 1983 and 2023,\\nTotal Rainfall in Dec has decreased\",\n          subtitle = \"Total Rainfall in Dec\",\n          x = \"Year\",\n          y = \"Total Rainfall\\n(mm)\") +\n  theme(plot.title = element_text(hjust = 0.5, size = 8),\n        plot.subtitle = element_text(size = 7),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1),\n        axis.text = element_text(size = 5),     \n        axis.title = element_text(size = 5)) +\n  geom_text(aes(label = Total_Rainfall), vjust = -0.5, size = 2.5) +\n  coord_cartesian(ylim = c(0, 400))\n\n\nr= ggplot(weather,        \n       aes(x = Max_Temp,             \n           y = Year,             \n           fill = factor(stat(quantile)))) +   \n  stat_density_ridges(     \n    geom = \"density_ridges_gradient\",     \n    calc_ecdf = TRUE,      \n    quantiles = 4,     \n    quantile_lines = TRUE) +   \n  scale_fill_viridis_d(name = \"Quartiles\") +   \n  labs(title = \"Overall (between 1983 and 2023),\\nMax Daily Temp in Dec has increased\",\n       subtitle = \"Distribution of Max Daily Temp\",           \n       x = \"Max Daily Temp (°C)\",           \n       y = \"Year\") +   \n  theme(plot.title = element_text(hjust = 0.5, size = 8),         \n        plot.subtitle = element_text(size = 7),\n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1),\n        axis.text = element_text(size = 5),     \n        axis.title = element_text(size = 5),          \n        legend.title = element_text(size = 5),       \n        legend.text = element_text(size = 5))\n\nplot = ggarrange(d, b, c, r,  \n          nrow = 2, \n          ncol = 2)\n          \nannotate_figure(plot,\n                top = text_grob('Changes in December Temperatures and Rainfall Across The Years'),\n                fig.lab.face = \"bold\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#interactive-plots",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#interactive-plots",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "The dot plot and calendar heatmaps are then converted into interactive plots using the ggplotly() function in the ggplotly package. This would allow users to explore the dataset in further detail. In addition, a time-series dot plot is also created to animate the changes in the mean, maximum, and minimum daily temperatures in December across the five years.\n\nDot PlotTime Series Dot PlotCalendar HeatmapCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Dot Plot\nggplotly(d)\n\n#Time Series Dot Plot\nid = ggplot(temp,         \n       aes(x = Year,             \n           y = value)) +   \n  geom_jitter(aes(color = Temp,\n                  frame = Year), \n              size = 3, \n              alpha = 0.5,               \n              position = position_jitter(width = 0.2)) +   \n  labs(title = \"Overall, Between 1983 and 2023,\\nDaily Temperatures Appears To Be Increasing\", \n       x = \"Year\",           \n       y = \"Temp\\n(°C)\",        \n       colour = \"Temp\") +\n  theme(plot.title = element_text(hjust = 0.5, size = 8),         \n        plot.subtitle = element_text(size = 7),\n        axis.title.y = element_text(angle=360,\n                                    vjust=.5, \n                                    hjust=1),\n        axis.text = element_text(size = 5),     \n        axis.title = element_text(size = 5),          \n        legend.title = element_text(size = 5),       \n        legend.text = element_text(size = 5))\n\nggplotly(id)\n\n#Calendar Heatmaps\ncal = ggplot(temp, \n       aes(Day, \n           Year, \n           fill = value)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  coord_equal() +\n  scale_fill_gradient(name = \"Temp\\n(°C)\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~Temp, ncol = 1) +\n  labs(x = NULL, y = NULL, \n     title = \"Daily Temperatures in Dec\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )\n\nggplotly(cal)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#conclusion",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "In conclusion, the ggplot2 package is a powerful package for exploratory data analysis through visualisation. The weather dataset from MSS is interesting and contains useful data for studying Singapore’s weather conditions. The insights gained from analysing the analytical questions posed in this take-home exercise provides a preview of the data analyses that can be conducted in further studies (e.g., expand to include other weather stations across Singapore) to better understand how the various weather conditions have varied across the years. The take-home exercise also highlighted the importance of examining variables from multiple angles (e.g., distribution, variance) and understanding their implications on people (e.g., maximum and minimum temperatures, and diurnal temperature ranges may reveal more about lived experiences as compared to mean temperature)."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#key-references",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#key-references",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "R for Visual Analytics.\nR for Data Science.\nFundamentals of Data Visualisation.\n\n~~~ End of Take-home Exercise 3 ~~~"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "Singapore has come a long way since gaining independence in 1965. Without natural resources, human capital development through a well-planned education system is a critical part of Singapore’s transformation from third world to first.\nDespite the success, there is still a correlation between socio-economic status and education achievement, as well as ingrained perceptions that some schools are better than others.\nHence, there is a need to use data to analyse the performance of Singapore students across different subjects, and identify any relationships between the performance in various subjects and factors such as gender, socioeconomic status, and type of school.\n\n\n\nIn this take-home exercise, the objective is to use the appropriate Exploratory Data Analysis (EDA) methods and ggplot2 functions (part of the amazing tidyverse ecosystem) to answer the following analytical questions:\n\nWhat is the distribution of Singapore students’ performance in Mathematics, Reading, and Science? What are the similarities and/or differences between the distributions for the three different subjects?\nAre there relationships between the students’ performance in the three subjects and factors such as their gender, socioeconomic status, and school? If yes, what kind of relationships are present?\n\nBased on the analysis and observations, this take-home exercise also briefly suggests the potential insights can be further studied in future research to better inform education policy planning.\n\n\n\n\n\n\nThe R packages used in this take-home exercise are:\n\nhaven for importing SAS files;\ntidyverse (i.e. readr, tidyr, dplyr, ggplot2) for performing data science tasks such as importing, tidying, and wrangling data, as well as creating graphics based on The Grammar of Graphics;\nreshape2 for transforming data between wide and long formats;\nggthemr for aesthetic themes created by user, Ciarán Tobin;\nggridges for creating ridgeline plots;\nggdist for visualising distributions and uncertainty; and\nggpubr for creating publication ready ggplot2 plots.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed in the computer. If yes, they are then loaded into the R environment. If no, they are installed, and then loaded into the R environment.\n\npacman::p_load(haven, tidyverse, reshape2,\n               ggthemr, ggridges, ggdist,\n               ggpubr)\n\nThe ggthemr() function in the ggthemr package is used to set the default theme of this take-home exercise as “solarized”.\n\nggthemr(\"solarized\")\n\n\n\n\nThe OECD Programme for International Student Assessment (PISA) measures how well 15-year-old students in different countries are “prepared to meet the challenges of today’s knowledge societies” by looking at “their ability to use their knowledge and skills to meet real-life challenges”. The PISA surveys take place every three years, the latest being conducted in 2022.\nThe PISA 2022 database contains the full set of responses from individual students, school principals, and parents. There are a total of five data files and their contents are as follows:\n\nStudent questionnaire data file;\nSchool questionnaire data file;\nTeacher questionnaire data file;\nCognitive item data file; and\nQuestionnaire timing data file.\n\nFor the purpose of this take-home exercise, the “Student questionnaire data file” is used.\n\n\n\n\n\n\nThe dataset used in this take-home exercise is the 2022 PISA student questionnaire data file, cy08msp_stu_qqq.sas7bdat, which is in the SAS file format.\nThe file is imported into the R environment using the read_sas() function in the haven package and stored as the R object, stu_qqq.\n\nstu_qqq = read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\nThe tibble data frame, stu_qqq, has 1,279 columns (variables) and 613,744 rows (observations).\n\n\n\nThere are 6,606 rows with the country code (i.e., CNT) value of “SGP”, which represents Singapore. This count is cross-verified by the information provided in the “CY08MSP_STU_QQQ” sheet in the codebook. The codebook also stated that Singapore students’ made up 1.0763% of the entire global student population who took part in the 2022 PISA.\nThe filter() function in the dplyr package is used to obtain these rows, and stored as the R object, stu_qqq_SG.\n\nstu_qqq_SG = stu_qqq %&gt;% filter(CNT == \"SGP\")\n\nThe tibble data frame, stu_qqq_SG, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(stu_qqq_SG, \"data/stu_qqq_SG.rds\")\n\n\nstu_qqq_SG = read_rds(\"data/stu_qqq_SG.rds\")\n\n\n\n\nOf the 1,279 variables (columns), the following 36 are preliminarily selected to answer the analytical questions:\n\nInternational School ID (“CNTSCHID”);\nInternational Student ID (“CNTSTUID”);\nType of School (“STRATUM”);\n\n“SGP01” is Public Secondary School.\n“SGP03” is Private Secondary School.\n\nStudent Gender (“ST004D01T”);\n\n“01” is Female.\n“02” is Male.\n\nIndex of Economic, Social, and Cultural Status (“ESCS”);\nNumber of Books at Home (“ST255Q01JA”);\n\n“01” is no books”.\n“02” is 1-10 books.\n“03” is 11-25 books.\n“04” is 26-100 books.\n“05” is 101-200 books.\n“06” is 201-500 books.\n“07” is more than 500 books.\n\nPlausible Values 1 to 10 in Mathematics (“PV1MATH” to “PV10MATH”);\nPlausible Values 1 to 10 in Reading (“PV1READ” to “PV10READ”); and\nPlausible Values 1 to 10 in Science (“PV1SCIE” to “PV10SCIE”).\n\nAgain, the select() function in the dplyr package is used to obtain these rows, and stored as the R object, stu_qqq_SG_final.\n\nstu_qqq_SG_final = stu_qqq_SG %&gt;% \n  select(\"CNTSCHID\", \"CNTSTUID\", \"STRATUM\", \"ST004D01T\", \"ESCS\", \"ST255Q01JA\",\n           \"PV1MATH\", \"PV2MATH\", \"PV3MATH\", \"PV4MATH\", \"PV5MATH\", \"PV6MATH\", \n           \"PV7MATH\", \"PV8MATH\", \"PV9MATH\", \"PV10MATH\", \"PV1READ\", \"PV2READ\", \n           \"PV3READ\", \"PV4READ\", \"PV5READ\", \"PV6READ\", \"PV7READ\", \"PV8READ\", \n           \"PV9READ\", \"PV10READ\", \"PV1SCIE\", \"PV2SCIE\", \"PV3SCIE\", \"PV4SCIE\", \n           \"PV5SCIE\", \"PV6SCIE\", \"PV7SCIE\", \"PV8SCIE\", \"PV9SCIE\", \"PV10SCIE\")\n\n\n\n\nThe dataset from PISA is expected to be relatively clean. Nevertheless, due diligence checks for duplicates and missing values are still made to confirm the assumption.\nThe duplicated() function in the base package is used to check for duplicates in stu_qqq_SG_final. There are no duplicates in the tibble data frame.\n\nstu_qqq_SG_final[duplicated(stu_qqq_SG_final), ]\n\n# A tibble: 0 × 36\n# ℹ 36 variables: CNTSCHID &lt;dbl&gt;, CNTSTUID &lt;dbl&gt;, STRATUM &lt;chr&gt;,\n#   ST004D01T &lt;dbl&gt;, ESCS &lt;dbl&gt;, ST255Q01JA &lt;dbl&gt;, PV1MATH &lt;dbl&gt;,\n#   PV2MATH &lt;dbl&gt;, PV3MATH &lt;dbl&gt;, PV4MATH &lt;dbl&gt;, PV5MATH &lt;dbl&gt;, PV6MATH &lt;dbl&gt;,\n#   PV7MATH &lt;dbl&gt;, PV8MATH &lt;dbl&gt;, PV9MATH &lt;dbl&gt;, PV10MATH &lt;dbl&gt;, PV1READ &lt;dbl&gt;,\n#   PV2READ &lt;dbl&gt;, PV3READ &lt;dbl&gt;, PV4READ &lt;dbl&gt;, PV5READ &lt;dbl&gt;, PV6READ &lt;dbl&gt;,\n#   PV7READ &lt;dbl&gt;, PV8READ &lt;dbl&gt;, PV9READ &lt;dbl&gt;, PV10READ &lt;dbl&gt;, PV1SCIE &lt;dbl&gt;,\n#   PV2SCIE &lt;dbl&gt;, PV3SCIE &lt;dbl&gt;, PV4SCIE &lt;dbl&gt;, PV5SCIE &lt;dbl&gt;, …\n\n\nThe colSums() function in the base package is used to check for missing values in stu_qqq_SG_final. There are no missing values in the tibble data frame for all most columns except for two:\n\nThe column with the Index of Economic, Social, and Cultural Status (“ESCS”) has 47 rows (observations) with NA values.\nThe column indicating the number of books at home (“ST255Q01JA” has 44 rows (observations) with NA values.\n\nIn total, there are 50 rows with one or more NA values. As this makes up only 0.757% of the 6,606 observations, we will remove them from the subsequent analysis. The na.omit() function in the stats package is used to remove them from stu_qqq_SG_final, which now has 6,556 observations and 36 variables. A confirmatory check is then made with the colSums() function in the base package.\n\ncolSums(is.na(stu_qqq_SG_final))\n\n  CNTSCHID   CNTSTUID    STRATUM  ST004D01T       ESCS ST255Q01JA    PV1MATH \n         0          0          0          0         47         44          0 \n   PV2MATH    PV3MATH    PV4MATH    PV5MATH    PV6MATH    PV7MATH    PV8MATH \n         0          0          0          0          0          0          0 \n   PV9MATH   PV10MATH    PV1READ    PV2READ    PV3READ    PV4READ    PV5READ \n         0          0          0          0          0          0          0 \n   PV6READ    PV7READ    PV8READ    PV9READ   PV10READ    PV1SCIE    PV2SCIE \n         0          0          0          0          0          0          0 \n   PV3SCIE    PV4SCIE    PV5SCIE    PV6SCIE    PV7SCIE    PV8SCIE    PV9SCIE \n         0          0          0          0          0          0          0 \n  PV10SCIE \n         0 \n\n\n\nstu_qqq_SG_final = stu_qqq_SG_final %&gt;%\n  na.omit()\n\n\ncolSums(is.na(stu_qqq_SG_final))\n\n  CNTSCHID   CNTSTUID    STRATUM  ST004D01T       ESCS ST255Q01JA    PV1MATH \n         0          0          0          0          0          0          0 \n   PV2MATH    PV3MATH    PV4MATH    PV5MATH    PV6MATH    PV7MATH    PV8MATH \n         0          0          0          0          0          0          0 \n   PV9MATH   PV10MATH    PV1READ    PV2READ    PV3READ    PV4READ    PV5READ \n         0          0          0          0          0          0          0 \n   PV6READ    PV7READ    PV8READ    PV9READ   PV10READ    PV1SCIE    PV2SCIE \n         0          0          0          0          0          0          0 \n   PV3SCIE    PV4SCIE    PV5SCIE    PV6SCIE    PV7SCIE    PV8SCIE    PV9SCIE \n         0          0          0          0          0          0          0 \n  PV10SCIE \n         0 \n\n\n\n\n\nFor ease of use, the columns are renamed accordingly using the rename() function in the dplyr package.\n\nstu_qqq_SG_final = stu_qqq_SG_final %&gt;%\n  rename(School = \"CNTSCHID\",\n         SchoolType = \"STRATUM\",\n         ID = \"CNTSTUID\",\n         Gender = \"ST004D01T\",\n         SocioeconStatus = \"ESCS\",\n         Books = \"ST255Q01JA\",\n         Math01 = \"PV1MATH\", Math02 = \"PV2MATH\", \n         Math03 = \"PV3MATH\", Math04 = \"PV4MATH\", \n         Math05 = \"PV5MATH\", Math06 = \"PV6MATH\", \n         Math07 = \"PV7MATH\", Math08 = \"PV8MATH\", \n         Math09 = \"PV9MATH\", Math10 = \"PV10MATH\", \n         Read01 = \"PV1READ\", Read02 = \"PV2READ\", \n         Read03 = \"PV3READ\", Read04 = \"PV4READ\", \n         Read05 = \"PV5READ\", Read06 = \"PV6READ\", \n         Read07 = \"PV7READ\", Read08 = \"PV8READ\", \n         Read09 = \"PV9READ\", Read10 = \"PV10READ\", \n         Sci01 = \"PV1SCIE\", Sci02 = \"PV2SCIE\", \n         Sci03 = \"PV3SCIE\", Sci04 = \"PV4SCIE\",\n         Sci05 = \"PV5SCIE\", Sci06 = \"PV6SCIE\", \n         Sci07 = \"PV7SCIE\", Sci08 = \"PV8SCIE\", \n         Sci09 = \"PV9SCIE\", Sci10 = \"PV10SCIE\")\n\nAlso, for the ease of use, the values for Gender, School Type, and Number of Books are replaced with characters using the ifelse() function in the base package.\n\nstu_qqq_SG_final$Gender = ifelse(\n  stu_qqq_SG_final$Gender == 01, \n  \"Female\", \"Male\")\nstu_qqq_SG_final$SchoolType = ifelse(\n  stu_qqq_SG_final$SchoolType == \"SGP01\", \n  \"Public\", \"Private\")\n\nstu_qqq_SG_final = stu_qqq_SG_final %&gt;%\n  mutate(Books = recode(Books, \n                        \"01\" = \"0 Book\",\n                        \"02\" = \"1-10 Books\",\n                        \"03\" = \"11-25 Books\",\n                        \"04\" = \"26-100 Books\",\n                        \"05\" = \"101-200 Books\",\n                        \"06\" = \"201-500 Books\",\n                        \"07\" = \"&gt;500 Books\"))\n\n\n\n\nThere are 10 Plausible Values (PVs) each for Mathematics, Reading, and Science. However, PISA cautions against averaging the PVs at the student level. Instead, it suggests that population statistics should be estimated using each PV separately - e.g., if one is interested in the correlation coefficient between the social index and the reading performance in PISA, 10 correlation coefficients should be computed and then averaged.\nA combination of half-density plot and box plot are plotted for the each of the 10 PVs for each subject. Firstly, the select() function in the dplyr package and the melt() function in the reshape2 package are used to select the relevant columns and transform them into a long table. The variables are then renamed using the rename() function in the dplyr package.\nThe ggplot(), geom_boxplot(), coord_flip() functions in the ggplot2 package, and the stat_halfeye() function in the ggdist package are used to create the plots. The ggtitle() and theme() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title, centre the plot title, and rotate the y-axis label.\n\nMathReadingScience\n\n\n\n\nCode\nmath = stu_qqq_SG_final %&gt;% \n  select(Math01, Math02, Math03, Math04, Math05,\n         Math06, Math07, Math08, Math09, Math10) %&gt;%\n  melt() %&gt;%\n  rename(\"PV\" = \"variable\", \"Score\" = \"value\")\n\nggplot(math, \n       aes(x = PV, \n           y = Score)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .15,\n               outlier.shape = NA) +\n  coord_flip() +\n  ggtitle(\"Distribution of Math Scores for Different PVs\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\n\n\n\n\n\nCode\nread = stu_qqq_SG_final %&gt;% \n  select(Read01, Read02, Read03, Read04, Read05,\n         Read06, Read07, Read08, Read09, Read10) %&gt;%\n  melt() %&gt;%\n  rename(\"PV\" = \"variable\", \"Score\" = \"value\")\n\nggplot(read, \n       aes(x = PV, \n           y = Score)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .15,\n               outlier.shape = NA) +\n  coord_flip() +\n  ggtitle(\"Distribution of Reading Scores for Different PVs\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\n\n\n\n\n\nCode\nsci = stu_qqq_SG_final %&gt;% \n  select(Sci01, Sci02, Sci03, Sci04, Sci05,\n         Sci06, Sci07, Sci08, Sci09, Sci10) %&gt;%\n  melt() %&gt;%\n  rename(\"PV\" = \"variable\", \"Score\" = \"value\")\n\nggplot(sci, \n       aes(x = PV, \n           y = Score)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .15,\n               outlier.shape = NA) +\n  coord_flip() +\n  ggtitle(\"Distribution of Science Scores for Different PVs\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\n\n\n\n\nBased on the plots, the 10 sets of PVs for each subject are broadly similar to one another (similar distributions and similar median values). Hence, for the purposes of this take-home exercise, the PV 1 values for each subject are used.\nThe select() and rename() functions in the dplyr package are used to further narrow down the number of variables chosen to 9 out of the preliminary 36 variables chosen previously, and rename some of the columns for easier identification.\n\nstu_qqq_SG_final = stu_qqq_SG_final %&gt;% \n  select(\"ID\", \"School\", \"SchoolType\", \n            \"Gender\", \"SocioeconStatus\", \n            \"Books\", \n            \"Math01\", \"Read01\", \"Sci01\") %&gt;%\n  rename(\"Mathematics\" = \"Math01\",\n         \"Reading\" = \"Read01\",\n         \"Science\" = \"Sci01\")\n\nThe finalised tibble data frame, stu_qqq_SG_final, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(stu_qqq_SG_final, \"data/stu_qqq_SG_final.rds\")\n\n\nstu_qqq_SG_final = read_rds(\"data/stu_qqq_SG_final.rds\")\n\n\n\n\n\n\n\nA ridgeline plot is created to visualise the distributions of the Singapore students’ performance for all three subjects within one plot.\nFirstly, the select() function in the dplyr package and the melt() function in the reshape2 package are used to select the relevant columns and transform them into a long table. The variables are then renamed using the rename() function in the dplyr package. The mutate() function in the dplyr package is then used with the “fct_relevel” argument to manually arrange the subjects’ order in the plot.\nThen, the ggplot() and scale_fill_viridis_d() functions in the ggplot2 package, and the stat_density_ridges() function in the ggridges package are used to create the plot. The ggtitle() and theme() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title and subtitle, centre the plot title, and rotate the y-axis label.\n\nPlot 1Code\n\n\n\n\n\n\n\n\n\n\ncomb = stu_qqq_SG_final %&gt;% \n  select(Mathematics, Reading, Science) %&gt;%\n  melt() %&gt;%\n  rename(\"Subject\" = \"variable\", \"Score\" = \"value\") %&gt;%\n  mutate(Subject = fct_relevel(Subject, \n                               \"Mathematics\", \n                               \"Science\", \n                               \"Reading\"))\n\nggplot(comb, \n       aes(x = Score, \n           y = Subject,\n           fill = factor(after_stat(quantile)))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\", alpha = 0.8) +\n  ggtitle(label = \"SG Students Generally Perform Better in\\nMathematics than Science & Reading\",\n          subtitle = \"Distribution of Mathematics, Science and Reading Scores\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\nObservation: Based on the plot, we can see that the scores for all three subjects resemble normal distribution. Also, based on the quartile lines, the scores for Mathematics tend to be higher than that for Science and Reading; while the scores of Science tend to be higher than that for Reading. This corresponds to general perceptions that Singapore students tend to be better at technical subjects (such as Mathematics and Science) as compared to subjects related to language skills (such as Reading).\nFuture research in this area may consider the historical trends in the differences between Singapore students’ performance in the three subjects and whether there are societal and economic forces behind such trends.\n\n\n\n\n\nThere is often a stereotype that men are better than women in the areas of science and technology (S&T). In this take-home exercise, we attempt to answer the question: “Are girls weaker than boys at Science in Singapore?” by creating a density plot of the Singapore students’ Science scores by gender.\nFirstly, the filter() function in the dplyr package is used to select the subsets of stu_qqq_SG_final containing the observations for each gender. The mutate() function in the dplyr package is then used with the “fct_relevel” argument to manually arrange the two gender’s order in the plot so that the Female density plot uses the stereotypical/associated colour of pink, while the Male density plot uses the stereotypical/associated colour of blue.\nThen, the ggplot(), geom_density() and geom_vline() functions in the ggplot2 package are used to create the plot and insert vertical lines indicating the median scores for each gender (again, pink for Female, and blue for Male). The ggtitle(), theme(), xlab(), and ylab() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title and subtitle, centre the plot title, rotate the y-axis label, and overwrite the axes’ labels.\n\nPlot 2Code\n\n\n\n\n\n\n\n\n\n\nf = stu_qqq_SG_final %&gt;%\n  filter(Gender == \"Female\")\n\nm = stu_qqq_SG_final %&gt;%\n  filter(Gender == \"Male\")\n\nstu_qqq_SG_final = stu_qqq_SG_final %&gt;%\n  mutate(Gender = fct_relevel(Gender, \n                               \"Male\", \n                               \"Female\"))\n\nggplot(stu_qqq_SG_final,\n       aes(x = Science,\n           fill = Gender)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(aes(xintercept=median(f$Science)),\n             color=\"red\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) +\n  geom_vline(aes(xintercept=median(m$Science)),\n             color=\"blue\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) +\n  ggtitle(label = \"Girls Are Not Necessarily Weaker in Science\",\n          subtitle= \"Distribution of Science Scores by Gender\") + \n  ylab(\"Density\") + xlab(\"Score\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\nObservation: Based on the plot, the distribution of Science scores for both Male and Female are broadly similar. With slightly more boys than girls on the two ends of the distribution (i.e., either poor or excellent scores), and conversely, more girls than boys in the centre of the distribution (i.e., close to median scores). The median scores for both gender are also very close to each other, with Male slightly higher than Female. This debunks the notion that girls are weaker in Science compared to boys.\nFuture research may consider why a relatively similar distribution in the Science performance of Singapore students at 15-year-old does not translate to a higher proportion of females undertaking S&T endeavours in college and beyond.\n\n\n\nParents in Singapore are often concerned about the type of schools their children enrol in as there is a strong, ingrained belief that some schools (i.e., Private) are better than others (i.e., Public). In this take-home exercise, we attempt to answer the question: “Do students in public schools perform poorer in Mathematics?” by creating a half-eye plot combined with a box plot for the Mathematics scores by type of school.\nFirstly, the filter() function in the dplyr package is used to select the subsets of stu_qqq_SG_final containing the observations for each gender. The mutate() function in the dplyr package is then used with the “fct_relevel” argument to manually arrange the two gender’s order in the plot so that the Female density plot uses the stereotypical/associated colour of pink, while the Male density plot uses the stereotypical/associated colour of blue.\nThe ggplot(), geom_boxplot() and coord_flip() functions in the ggplot2 package are used to create the plot, insert the box plots for each type of school, and change the orientation of the plot. The stat_halfeye() and stat_dots() functions from the ggdist package are used to show the distributions for each type of school. The ggtitle(), theme(), xlab(), and ylab() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title and subtitle, centre the plot title, rotate the y-axis label, and overwrite the axes’ labels.\n\nPlot 3Code\n\n\n\n\n\n\n\n\n\n\nggplot(stu_qqq_SG_final, \n       aes(x = SchoolType, \n           y = Mathematics)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.15,\n               .width = 0,\n               point_colour = NA,\n               scale= 0.55) +\n  geom_boxplot(width = .15,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = 3,\n            dotsize = 0.1) +\n  coord_flip() +\n  ggtitle(label = \"Public Schools Have Both Best and Worst Performing\\n Students in Mathematics\",\n          subtitle= \"Distribution of Mathematics Scores by School Type\") +\n  xlab(\"School\\nType\") + ylab(\"Score\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\nObservation: Based on the plot, the distribution of Mathematics scores for Singapore students in public and private schools differ in the following ways:\n\nThe range of scores in public schools is larger than that in private schools, i.e., public schools have a wider range of student capabilities for Mathematics;\nRelated to above, there is a greater density of students in private schools that perform moderately well (i.e., around the median score for private schools) compared to students from public schools.\nThe median Mathematics score for students in private schools is slightly higher than that for public schools.\n\nIt is important to note from the dot plots that there are a much larger number of students in public schools than private schools, which may mean that the results for private schools may not be representative.\nMore research would need to be conducted to uncover how and why the relationship between the type of school and Mathematics performance are as shown by the various observations. Also, there may be granular differences in Mathematics performance between different public schools.\n\n\n\nResearch has continued to show that socioeconomic status influence academic performance. In this take-home exercise, we attempt to answer the question: “Are Reading scores related to socioeconomic status?” by creating a scatter plot to show the distribution of Reading scores vis-a-vis the Index of Economic, Social, and Cultural Status values.\nThe ggplot(), geom_point() and geom_smooth() functions in the ggplot2 package are used to create the scatter plot, and add a smooth line showing the correlation between the two variables. The stat_cor() function in the ggpubr package is used to generate the correlation coefficient, R, which is 0.41. The ggtitle(), theme(), xlab(), and ylab() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title and subtitle, centre the plot title, rotate the y-axis label, and overwrite the axes’ labels.\n\nPlot 4Code\n\n\n\n\n\n\n\n\n\n\nggplot(data = stu_qqq_SG_final, \n            aes(x = Reading, \n                y = SocioeconStatus)) +   \n  geom_point(alpha = 0.1) +\n  geom_smooth(linewidth = 1, colour = \"pink\") + \n  stat_cor() +\n  ggtitle(label = \"Moderate Positive Correlation between\\nReading Scores and Socioeconomic Status\",\n          subtitle= \"Distribution of Reading Scores vis-a-vis Socioeconomic Status\") +\n  xlab(\"Score\") + ylab(\"Socioeconomic\\nStatus\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\nObservation: Based on the plot, there is no clear linear relationship between Reading scores and socioeconomic status given that the dots are generally well distributed, i.e., there are students with low socioeconomic status (below 0 value) with scores ranging from 200 to 800, as there are students with high socio economic status (above 0 value) with scores ranging from 200 to 800. Nevertheless, there is a higher concentration of students in the 400-600 score range with high socioeconomic status compared to students with low socioeconomic status in the same score range.\nFuture research may consider whether there are better variables that may explain differences in Reading performance.\nFor a start, we consider if a simpler variable such as the number of books may help to explain differences in Reading performance better than the Index of Economic, Social, and Cultural Status values. A ridgeline created to visualise the distributions of the Singapore students’ Reading performance depending on the self-reported number of books at home.\nFirstly, the select() function in the dplyr package and the melt() function in the reshape2 package are used to select the relevant columns and transform them into a long table. The mutate() function in the dplyr package is then used with the “fct_relevel” argument to manually arrange the number of books in ascending order in the plot.\nThen, the ggplot() and scale_fill_viridis_d() functions in the ggplot2 package, and the stat_density_ridges() function in the ggridges package are used to create the plot. The ggtitle() and theme() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title and subtitle, centre the plot title, rotate the y-axis label, and remove the legend.\n\nPlot 5Code\n\n\n\n\n\n\n\n\n\n\nr = stu_qqq_SG_final %&gt;% \n  select(Books, Reading) %&gt;%\n  mutate(Books = fct_relevel(Books, \n                              \"0 Book\", \n                              \"1-10 Books\",\n                              \"11-25 Books\",\n                              \"26-100 Books\",\n                              \"101-200 Books\",\n                              \"201-500 Books\",\n                              \"&gt;500 Books\"))\n                        \nggplot(r, \n       aes(x = Reading, \n           y = Books,\n           fill = factor(after_stat(quantile)))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 2,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(alpha = 0.8) +\n  ggtitle(label = \"More Books at Home Generally Correspond\\nto Higher Reading Scores\",\n          subtitle = \"Distribution of Reading Scores vis-a-vis Number of Books at Home\") +\n  xlab(\"Score\") + ylab(\"No.of\\nBooks\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1), \n        legend.position = \"none\")\n\n\n\n\n\n\n\nObservation: Based on the plot, there is a relationship between the number of books at home and the Reading scores. Focusing on the median values for each ridge (as shown by the divider line between the two colours), the values increase as the number of books at home increase. The only exception is the median value of Reading scores for students with more than 500 books at home, which is lower than that for 201-500 books.\nHence, we can see a clearer trend between Reading scores and number of books as compared to that between Reading scores and the Index of Economic, Social, and Cultural Status values. Further analysis can be conducted to determine the correlation coefficient of the relationship and whether the relationship is statistically significant.\n\n\n\n\n\nIn conclusion, the ggplot2 package is a powerful package for exploratory data analysis through visualisation. The PISA performance dataset is interesting and contains useful data for studying Singapore students’ academic performance in the three subjects at age 15. The insights gained from analysing the various questions posed in this take-home exercise provides a preview of the data analyses that can be conducted in further studies to better understand how various factors including gender, type of school, and socioeconomic status affect academic performance, so as to better inform education policy planning in Singapore.\n\n\n\n\nR for Visual Analytics.\nR for Data Science.\nFundamentals of Data Visualisation.\nPISA 2022 Database and Technical Report.\nOECD on Singapore: Rapid Improvement Followed by Strong Performance.\n\n~~~ End of Take-home Exercise 1 ~~~"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#introduction",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#introduction",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "Singapore has come a long way since gaining independence in 1965. Without natural resources, human capital development through a well-planned education system is a critical part of Singapore’s transformation from third world to first.\nDespite the success, there is still a correlation between socio-economic status and education achievement, as well as ingrained perceptions that some schools are better than others.\nHence, there is a need to use data to analyse the performance of Singapore students across different subjects, and identify any relationships between the performance in various subjects and factors such as gender, socioeconomic status, and type of school.\n\n\n\nIn this take-home exercise, the objective is to use the appropriate Exploratory Data Analysis (EDA) methods and ggplot2 functions (part of the amazing tidyverse ecosystem) to answer the following analytical questions:\n\nWhat is the distribution of Singapore students’ performance in Mathematics, Reading, and Science? What are the similarities and/or differences between the distributions for the three different subjects?\nAre there relationships between the students’ performance in the three subjects and factors such as their gender, socioeconomic status, and school? If yes, what kind of relationships are present?\n\nBased on the analysis and observations, this take-home exercise also briefly suggests the potential insights can be further studied in future research to better inform education policy planning."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#getting-started",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "The R packages used in this take-home exercise are:\n\nhaven for importing SAS files;\ntidyverse (i.e. readr, tidyr, dplyr, ggplot2) for performing data science tasks such as importing, tidying, and wrangling data, as well as creating graphics based on The Grammar of Graphics;\nreshape2 for transforming data between wide and long formats;\nggthemr for aesthetic themes created by user, Ciarán Tobin;\nggridges for creating ridgeline plots;\nggdist for visualising distributions and uncertainty; and\nggpubr for creating publication ready ggplot2 plots.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed in the computer. If yes, they are then loaded into the R environment. If no, they are installed, and then loaded into the R environment.\n\npacman::p_load(haven, tidyverse, reshape2,\n               ggthemr, ggridges, ggdist,\n               ggpubr)\n\nThe ggthemr() function in the ggthemr package is used to set the default theme of this take-home exercise as “solarized”.\n\nggthemr(\"solarized\")\n\n\n\n\nThe OECD Programme for International Student Assessment (PISA) measures how well 15-year-old students in different countries are “prepared to meet the challenges of today’s knowledge societies” by looking at “their ability to use their knowledge and skills to meet real-life challenges”. The PISA surveys take place every three years, the latest being conducted in 2022.\nThe PISA 2022 database contains the full set of responses from individual students, school principals, and parents. There are a total of five data files and their contents are as follows:\n\nStudent questionnaire data file;\nSchool questionnaire data file;\nTeacher questionnaire data file;\nCognitive item data file; and\nQuestionnaire timing data file.\n\nFor the purpose of this take-home exercise, the “Student questionnaire data file” is used."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-wrangling",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "The dataset used in this take-home exercise is the 2022 PISA student questionnaire data file, cy08msp_stu_qqq.sas7bdat, which is in the SAS file format.\nThe file is imported into the R environment using the read_sas() function in the haven package and stored as the R object, stu_qqq.\n\nstu_qqq = read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\nThe tibble data frame, stu_qqq, has 1,279 columns (variables) and 613,744 rows (observations).\n\n\n\nThere are 6,606 rows with the country code (i.e., CNT) value of “SGP”, which represents Singapore. This count is cross-verified by the information provided in the “CY08MSP_STU_QQQ” sheet in the codebook. The codebook also stated that Singapore students’ made up 1.0763% of the entire global student population who took part in the 2022 PISA.\nThe filter() function in the dplyr package is used to obtain these rows, and stored as the R object, stu_qqq_SG.\n\nstu_qqq_SG = stu_qqq %&gt;% filter(CNT == \"SGP\")\n\nThe tibble data frame, stu_qqq_SG, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(stu_qqq_SG, \"data/stu_qqq_SG.rds\")\n\n\nstu_qqq_SG = read_rds(\"data/stu_qqq_SG.rds\")\n\n\n\n\nOf the 1,279 variables (columns), the following 36 are preliminarily selected to answer the analytical questions:\n\nInternational School ID (“CNTSCHID”);\nInternational Student ID (“CNTSTUID”);\nType of School (“STRATUM”);\n\n“SGP01” is Public Secondary School.\n“SGP03” is Private Secondary School.\n\nStudent Gender (“ST004D01T”);\n\n“01” is Female.\n“02” is Male.\n\nIndex of Economic, Social, and Cultural Status (“ESCS”);\nNumber of Books at Home (“ST255Q01JA”);\n\n“01” is no books”.\n“02” is 1-10 books.\n“03” is 11-25 books.\n“04” is 26-100 books.\n“05” is 101-200 books.\n“06” is 201-500 books.\n“07” is more than 500 books.\n\nPlausible Values 1 to 10 in Mathematics (“PV1MATH” to “PV10MATH”);\nPlausible Values 1 to 10 in Reading (“PV1READ” to “PV10READ”); and\nPlausible Values 1 to 10 in Science (“PV1SCIE” to “PV10SCIE”).\n\nAgain, the select() function in the dplyr package is used to obtain these rows, and stored as the R object, stu_qqq_SG_final.\n\nstu_qqq_SG_final = stu_qqq_SG %&gt;% \n  select(\"CNTSCHID\", \"CNTSTUID\", \"STRATUM\", \"ST004D01T\", \"ESCS\", \"ST255Q01JA\",\n           \"PV1MATH\", \"PV2MATH\", \"PV3MATH\", \"PV4MATH\", \"PV5MATH\", \"PV6MATH\", \n           \"PV7MATH\", \"PV8MATH\", \"PV9MATH\", \"PV10MATH\", \"PV1READ\", \"PV2READ\", \n           \"PV3READ\", \"PV4READ\", \"PV5READ\", \"PV6READ\", \"PV7READ\", \"PV8READ\", \n           \"PV9READ\", \"PV10READ\", \"PV1SCIE\", \"PV2SCIE\", \"PV3SCIE\", \"PV4SCIE\", \n           \"PV5SCIE\", \"PV6SCIE\", \"PV7SCIE\", \"PV8SCIE\", \"PV9SCIE\", \"PV10SCIE\")\n\n\n\n\nThe dataset from PISA is expected to be relatively clean. Nevertheless, due diligence checks for duplicates and missing values are still made to confirm the assumption.\nThe duplicated() function in the base package is used to check for duplicates in stu_qqq_SG_final. There are no duplicates in the tibble data frame.\n\nstu_qqq_SG_final[duplicated(stu_qqq_SG_final), ]\n\n# A tibble: 0 × 36\n# ℹ 36 variables: CNTSCHID &lt;dbl&gt;, CNTSTUID &lt;dbl&gt;, STRATUM &lt;chr&gt;,\n#   ST004D01T &lt;dbl&gt;, ESCS &lt;dbl&gt;, ST255Q01JA &lt;dbl&gt;, PV1MATH &lt;dbl&gt;,\n#   PV2MATH &lt;dbl&gt;, PV3MATH &lt;dbl&gt;, PV4MATH &lt;dbl&gt;, PV5MATH &lt;dbl&gt;, PV6MATH &lt;dbl&gt;,\n#   PV7MATH &lt;dbl&gt;, PV8MATH &lt;dbl&gt;, PV9MATH &lt;dbl&gt;, PV10MATH &lt;dbl&gt;, PV1READ &lt;dbl&gt;,\n#   PV2READ &lt;dbl&gt;, PV3READ &lt;dbl&gt;, PV4READ &lt;dbl&gt;, PV5READ &lt;dbl&gt;, PV6READ &lt;dbl&gt;,\n#   PV7READ &lt;dbl&gt;, PV8READ &lt;dbl&gt;, PV9READ &lt;dbl&gt;, PV10READ &lt;dbl&gt;, PV1SCIE &lt;dbl&gt;,\n#   PV2SCIE &lt;dbl&gt;, PV3SCIE &lt;dbl&gt;, PV4SCIE &lt;dbl&gt;, PV5SCIE &lt;dbl&gt;, …\n\n\nThe colSums() function in the base package is used to check for missing values in stu_qqq_SG_final. There are no missing values in the tibble data frame for all most columns except for two:\n\nThe column with the Index of Economic, Social, and Cultural Status (“ESCS”) has 47 rows (observations) with NA values.\nThe column indicating the number of books at home (“ST255Q01JA” has 44 rows (observations) with NA values.\n\nIn total, there are 50 rows with one or more NA values. As this makes up only 0.757% of the 6,606 observations, we will remove them from the subsequent analysis. The na.omit() function in the stats package is used to remove them from stu_qqq_SG_final, which now has 6,556 observations and 36 variables. A confirmatory check is then made with the colSums() function in the base package.\n\ncolSums(is.na(stu_qqq_SG_final))\n\n  CNTSCHID   CNTSTUID    STRATUM  ST004D01T       ESCS ST255Q01JA    PV1MATH \n         0          0          0          0         47         44          0 \n   PV2MATH    PV3MATH    PV4MATH    PV5MATH    PV6MATH    PV7MATH    PV8MATH \n         0          0          0          0          0          0          0 \n   PV9MATH   PV10MATH    PV1READ    PV2READ    PV3READ    PV4READ    PV5READ \n         0          0          0          0          0          0          0 \n   PV6READ    PV7READ    PV8READ    PV9READ   PV10READ    PV1SCIE    PV2SCIE \n         0          0          0          0          0          0          0 \n   PV3SCIE    PV4SCIE    PV5SCIE    PV6SCIE    PV7SCIE    PV8SCIE    PV9SCIE \n         0          0          0          0          0          0          0 \n  PV10SCIE \n         0 \n\n\n\nstu_qqq_SG_final = stu_qqq_SG_final %&gt;%\n  na.omit()\n\n\ncolSums(is.na(stu_qqq_SG_final))\n\n  CNTSCHID   CNTSTUID    STRATUM  ST004D01T       ESCS ST255Q01JA    PV1MATH \n         0          0          0          0          0          0          0 \n   PV2MATH    PV3MATH    PV4MATH    PV5MATH    PV6MATH    PV7MATH    PV8MATH \n         0          0          0          0          0          0          0 \n   PV9MATH   PV10MATH    PV1READ    PV2READ    PV3READ    PV4READ    PV5READ \n         0          0          0          0          0          0          0 \n   PV6READ    PV7READ    PV8READ    PV9READ   PV10READ    PV1SCIE    PV2SCIE \n         0          0          0          0          0          0          0 \n   PV3SCIE    PV4SCIE    PV5SCIE    PV6SCIE    PV7SCIE    PV8SCIE    PV9SCIE \n         0          0          0          0          0          0          0 \n  PV10SCIE \n         0 \n\n\n\n\n\nFor ease of use, the columns are renamed accordingly using the rename() function in the dplyr package.\n\nstu_qqq_SG_final = stu_qqq_SG_final %&gt;%\n  rename(School = \"CNTSCHID\",\n         SchoolType = \"STRATUM\",\n         ID = \"CNTSTUID\",\n         Gender = \"ST004D01T\",\n         SocioeconStatus = \"ESCS\",\n         Books = \"ST255Q01JA\",\n         Math01 = \"PV1MATH\", Math02 = \"PV2MATH\", \n         Math03 = \"PV3MATH\", Math04 = \"PV4MATH\", \n         Math05 = \"PV5MATH\", Math06 = \"PV6MATH\", \n         Math07 = \"PV7MATH\", Math08 = \"PV8MATH\", \n         Math09 = \"PV9MATH\", Math10 = \"PV10MATH\", \n         Read01 = \"PV1READ\", Read02 = \"PV2READ\", \n         Read03 = \"PV3READ\", Read04 = \"PV4READ\", \n         Read05 = \"PV5READ\", Read06 = \"PV6READ\", \n         Read07 = \"PV7READ\", Read08 = \"PV8READ\", \n         Read09 = \"PV9READ\", Read10 = \"PV10READ\", \n         Sci01 = \"PV1SCIE\", Sci02 = \"PV2SCIE\", \n         Sci03 = \"PV3SCIE\", Sci04 = \"PV4SCIE\",\n         Sci05 = \"PV5SCIE\", Sci06 = \"PV6SCIE\", \n         Sci07 = \"PV7SCIE\", Sci08 = \"PV8SCIE\", \n         Sci09 = \"PV9SCIE\", Sci10 = \"PV10SCIE\")\n\nAlso, for the ease of use, the values for Gender, School Type, and Number of Books are replaced with characters using the ifelse() function in the base package.\n\nstu_qqq_SG_final$Gender = ifelse(\n  stu_qqq_SG_final$Gender == 01, \n  \"Female\", \"Male\")\nstu_qqq_SG_final$SchoolType = ifelse(\n  stu_qqq_SG_final$SchoolType == \"SGP01\", \n  \"Public\", \"Private\")\n\nstu_qqq_SG_final = stu_qqq_SG_final %&gt;%\n  mutate(Books = recode(Books, \n                        \"01\" = \"0 Book\",\n                        \"02\" = \"1-10 Books\",\n                        \"03\" = \"11-25 Books\",\n                        \"04\" = \"26-100 Books\",\n                        \"05\" = \"101-200 Books\",\n                        \"06\" = \"201-500 Books\",\n                        \"07\" = \"&gt;500 Books\"))\n\n\n\n\nThere are 10 Plausible Values (PVs) each for Mathematics, Reading, and Science. However, PISA cautions against averaging the PVs at the student level. Instead, it suggests that population statistics should be estimated using each PV separately - e.g., if one is interested in the correlation coefficient between the social index and the reading performance in PISA, 10 correlation coefficients should be computed and then averaged.\nA combination of half-density plot and box plot are plotted for the each of the 10 PVs for each subject. Firstly, the select() function in the dplyr package and the melt() function in the reshape2 package are used to select the relevant columns and transform them into a long table. The variables are then renamed using the rename() function in the dplyr package.\nThe ggplot(), geom_boxplot(), coord_flip() functions in the ggplot2 package, and the stat_halfeye() function in the ggdist package are used to create the plots. The ggtitle() and theme() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title, centre the plot title, and rotate the y-axis label.\n\nMathReadingScience\n\n\n\n\nCode\nmath = stu_qqq_SG_final %&gt;% \n  select(Math01, Math02, Math03, Math04, Math05,\n         Math06, Math07, Math08, Math09, Math10) %&gt;%\n  melt() %&gt;%\n  rename(\"PV\" = \"variable\", \"Score\" = \"value\")\n\nggplot(math, \n       aes(x = PV, \n           y = Score)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .15,\n               outlier.shape = NA) +\n  coord_flip() +\n  ggtitle(\"Distribution of Math Scores for Different PVs\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\n\n\n\n\n\nCode\nread = stu_qqq_SG_final %&gt;% \n  select(Read01, Read02, Read03, Read04, Read05,\n         Read06, Read07, Read08, Read09, Read10) %&gt;%\n  melt() %&gt;%\n  rename(\"PV\" = \"variable\", \"Score\" = \"value\")\n\nggplot(read, \n       aes(x = PV, \n           y = Score)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .15,\n               outlier.shape = NA) +\n  coord_flip() +\n  ggtitle(\"Distribution of Reading Scores for Different PVs\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\n\n\n\n\n\nCode\nsci = stu_qqq_SG_final %&gt;% \n  select(Sci01, Sci02, Sci03, Sci04, Sci05,\n         Sci06, Sci07, Sci08, Sci09, Sci10) %&gt;%\n  melt() %&gt;%\n  rename(\"PV\" = \"variable\", \"Score\" = \"value\")\n\nggplot(sci, \n       aes(x = PV, \n           y = Score)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .15,\n               outlier.shape = NA) +\n  coord_flip() +\n  ggtitle(\"Distribution of Science Scores for Different PVs\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\n\n\n\n\nBased on the plots, the 10 sets of PVs for each subject are broadly similar to one another (similar distributions and similar median values). Hence, for the purposes of this take-home exercise, the PV 1 values for each subject are used.\nThe select() and rename() functions in the dplyr package are used to further narrow down the number of variables chosen to 9 out of the preliminary 36 variables chosen previously, and rename some of the columns for easier identification.\n\nstu_qqq_SG_final = stu_qqq_SG_final %&gt;% \n  select(\"ID\", \"School\", \"SchoolType\", \n            \"Gender\", \"SocioeconStatus\", \n            \"Books\", \n            \"Math01\", \"Read01\", \"Sci01\") %&gt;%\n  rename(\"Mathematics\" = \"Math01\",\n         \"Reading\" = \"Read01\",\n         \"Science\" = \"Sci01\")\n\nThe finalised tibble data frame, stu_qqq_SG_final, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(stu_qqq_SG_final, \"data/stu_qqq_SG_final.rds\")\n\n\nstu_qqq_SG_final = read_rds(\"data/stu_qqq_SG_final.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#exploratory-data-analysis---computing-visualising-and-deriving-insights-on-singapores-pisa-performance",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#exploratory-data-analysis---computing-visualising-and-deriving-insights-on-singapores-pisa-performance",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "A ridgeline plot is created to visualise the distributions of the Singapore students’ performance for all three subjects within one plot.\nFirstly, the select() function in the dplyr package and the melt() function in the reshape2 package are used to select the relevant columns and transform them into a long table. The variables are then renamed using the rename() function in the dplyr package. The mutate() function in the dplyr package is then used with the “fct_relevel” argument to manually arrange the subjects’ order in the plot.\nThen, the ggplot() and scale_fill_viridis_d() functions in the ggplot2 package, and the stat_density_ridges() function in the ggridges package are used to create the plot. The ggtitle() and theme() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title and subtitle, centre the plot title, and rotate the y-axis label.\n\nPlot 1Code\n\n\n\n\n\n\n\n\n\n\ncomb = stu_qqq_SG_final %&gt;% \n  select(Mathematics, Reading, Science) %&gt;%\n  melt() %&gt;%\n  rename(\"Subject\" = \"variable\", \"Score\" = \"value\") %&gt;%\n  mutate(Subject = fct_relevel(Subject, \n                               \"Mathematics\", \n                               \"Science\", \n                               \"Reading\"))\n\nggplot(comb, \n       aes(x = Score, \n           y = Subject,\n           fill = factor(after_stat(quantile)))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\", alpha = 0.8) +\n  ggtitle(label = \"SG Students Generally Perform Better in\\nMathematics than Science & Reading\",\n          subtitle = \"Distribution of Mathematics, Science and Reading Scores\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\nObservation: Based on the plot, we can see that the scores for all three subjects resemble normal distribution. Also, based on the quartile lines, the scores for Mathematics tend to be higher than that for Science and Reading; while the scores of Science tend to be higher than that for Reading. This corresponds to general perceptions that Singapore students tend to be better at technical subjects (such as Mathematics and Science) as compared to subjects related to language skills (such as Reading).\nFuture research in this area may consider the historical trends in the differences between Singapore students’ performance in the three subjects and whether there are societal and economic forces behind such trends.\n\n\n\n\n\nThere is often a stereotype that men are better than women in the areas of science and technology (S&T). In this take-home exercise, we attempt to answer the question: “Are girls weaker than boys at Science in Singapore?” by creating a density plot of the Singapore students’ Science scores by gender.\nFirstly, the filter() function in the dplyr package is used to select the subsets of stu_qqq_SG_final containing the observations for each gender. The mutate() function in the dplyr package is then used with the “fct_relevel” argument to manually arrange the two gender’s order in the plot so that the Female density plot uses the stereotypical/associated colour of pink, while the Male density plot uses the stereotypical/associated colour of blue.\nThen, the ggplot(), geom_density() and geom_vline() functions in the ggplot2 package are used to create the plot and insert vertical lines indicating the median scores for each gender (again, pink for Female, and blue for Male). The ggtitle(), theme(), xlab(), and ylab() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title and subtitle, centre the plot title, rotate the y-axis label, and overwrite the axes’ labels.\n\nPlot 2Code\n\n\n\n\n\n\n\n\n\n\nf = stu_qqq_SG_final %&gt;%\n  filter(Gender == \"Female\")\n\nm = stu_qqq_SG_final %&gt;%\n  filter(Gender == \"Male\")\n\nstu_qqq_SG_final = stu_qqq_SG_final %&gt;%\n  mutate(Gender = fct_relevel(Gender, \n                               \"Male\", \n                               \"Female\"))\n\nggplot(stu_qqq_SG_final,\n       aes(x = Science,\n           fill = Gender)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(aes(xintercept=median(f$Science)),\n             color=\"red\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) +\n  geom_vline(aes(xintercept=median(m$Science)),\n             color=\"blue\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) +\n  ggtitle(label = \"Girls Are Not Necessarily Weaker in Science\",\n          subtitle= \"Distribution of Science Scores by Gender\") + \n  ylab(\"Density\") + xlab(\"Score\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\nObservation: Based on the plot, the distribution of Science scores for both Male and Female are broadly similar. With slightly more boys than girls on the two ends of the distribution (i.e., either poor or excellent scores), and conversely, more girls than boys in the centre of the distribution (i.e., close to median scores). The median scores for both gender are also very close to each other, with Male slightly higher than Female. This debunks the notion that girls are weaker in Science compared to boys.\nFuture research may consider why a relatively similar distribution in the Science performance of Singapore students at 15-year-old does not translate to a higher proportion of females undertaking S&T endeavours in college and beyond.\n\n\n\nParents in Singapore are often concerned about the type of schools their children enrol in as there is a strong, ingrained belief that some schools (i.e., Private) are better than others (i.e., Public). In this take-home exercise, we attempt to answer the question: “Do students in public schools perform poorer in Mathematics?” by creating a half-eye plot combined with a box plot for the Mathematics scores by type of school.\nFirstly, the filter() function in the dplyr package is used to select the subsets of stu_qqq_SG_final containing the observations for each gender. The mutate() function in the dplyr package is then used with the “fct_relevel” argument to manually arrange the two gender’s order in the plot so that the Female density plot uses the stereotypical/associated colour of pink, while the Male density plot uses the stereotypical/associated colour of blue.\nThe ggplot(), geom_boxplot() and coord_flip() functions in the ggplot2 package are used to create the plot, insert the box plots for each type of school, and change the orientation of the plot. The stat_halfeye() and stat_dots() functions from the ggdist package are used to show the distributions for each type of school. The ggtitle(), theme(), xlab(), and ylab() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title and subtitle, centre the plot title, rotate the y-axis label, and overwrite the axes’ labels.\n\nPlot 3Code\n\n\n\n\n\n\n\n\n\n\nggplot(stu_qqq_SG_final, \n       aes(x = SchoolType, \n           y = Mathematics)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.15,\n               .width = 0,\n               point_colour = NA,\n               scale= 0.55) +\n  geom_boxplot(width = .15,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = 3,\n            dotsize = 0.1) +\n  coord_flip() +\n  ggtitle(label = \"Public Schools Have Both Best and Worst Performing\\n Students in Mathematics\",\n          subtitle= \"Distribution of Mathematics Scores by School Type\") +\n  xlab(\"School\\nType\") + ylab(\"Score\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\nObservation: Based on the plot, the distribution of Mathematics scores for Singapore students in public and private schools differ in the following ways:\n\nThe range of scores in public schools is larger than that in private schools, i.e., public schools have a wider range of student capabilities for Mathematics;\nRelated to above, there is a greater density of students in private schools that perform moderately well (i.e., around the median score for private schools) compared to students from public schools.\nThe median Mathematics score for students in private schools is slightly higher than that for public schools.\n\nIt is important to note from the dot plots that there are a much larger number of students in public schools than private schools, which may mean that the results for private schools may not be representative.\nMore research would need to be conducted to uncover how and why the relationship between the type of school and Mathematics performance are as shown by the various observations. Also, there may be granular differences in Mathematics performance between different public schools.\n\n\n\nResearch has continued to show that socioeconomic status influence academic performance. In this take-home exercise, we attempt to answer the question: “Are Reading scores related to socioeconomic status?” by creating a scatter plot to show the distribution of Reading scores vis-a-vis the Index of Economic, Social, and Cultural Status values.\nThe ggplot(), geom_point() and geom_smooth() functions in the ggplot2 package are used to create the scatter plot, and add a smooth line showing the correlation between the two variables. The stat_cor() function in the ggpubr package is used to generate the correlation coefficient, R, which is 0.41. The ggtitle(), theme(), xlab(), and ylab() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title and subtitle, centre the plot title, rotate the y-axis label, and overwrite the axes’ labels.\n\nPlot 4Code\n\n\n\n\n\n\n\n\n\n\nggplot(data = stu_qqq_SG_final, \n            aes(x = Reading, \n                y = SocioeconStatus)) +   \n  geom_point(alpha = 0.1) +\n  geom_smooth(linewidth = 1, colour = \"pink\") + \n  stat_cor() +\n  ggtitle(label = \"Moderate Positive Correlation between\\nReading Scores and Socioeconomic Status\",\n          subtitle= \"Distribution of Reading Scores vis-a-vis Socioeconomic Status\") +\n  xlab(\"Score\") + ylab(\"Socioeconomic\\nStatus\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\nObservation: Based on the plot, there is no clear linear relationship between Reading scores and socioeconomic status given that the dots are generally well distributed, i.e., there are students with low socioeconomic status (below 0 value) with scores ranging from 200 to 800, as there are students with high socio economic status (above 0 value) with scores ranging from 200 to 800. Nevertheless, there is a higher concentration of students in the 400-600 score range with high socioeconomic status compared to students with low socioeconomic status in the same score range.\nFuture research may consider whether there are better variables that may explain differences in Reading performance.\nFor a start, we consider if a simpler variable such as the number of books may help to explain differences in Reading performance better than the Index of Economic, Social, and Cultural Status values. A ridgeline created to visualise the distributions of the Singapore students’ Reading performance depending on the self-reported number of books at home.\nFirstly, the select() function in the dplyr package and the melt() function in the reshape2 package are used to select the relevant columns and transform them into a long table. The mutate() function in the dplyr package is then used with the “fct_relevel” argument to manually arrange the number of books in ascending order in the plot.\nThen, the ggplot() and scale_fill_viridis_d() functions in the ggplot2 package, and the stat_density_ridges() function in the ggridges package are used to create the plot. The ggtitle() and theme() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title and subtitle, centre the plot title, rotate the y-axis label, and remove the legend.\n\nPlot 5Code\n\n\n\n\n\n\n\n\n\n\nr = stu_qqq_SG_final %&gt;% \n  select(Books, Reading) %&gt;%\n  mutate(Books = fct_relevel(Books, \n                              \"0 Book\", \n                              \"1-10 Books\",\n                              \"11-25 Books\",\n                              \"26-100 Books\",\n                              \"101-200 Books\",\n                              \"201-500 Books\",\n                              \"&gt;500 Books\"))\n                        \nggplot(r, \n       aes(x = Reading, \n           y = Books,\n           fill = factor(after_stat(quantile)))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 2,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(alpha = 0.8) +\n  ggtitle(label = \"More Books at Home Generally Correspond\\nto Higher Reading Scores\",\n          subtitle = \"Distribution of Reading Scores vis-a-vis Number of Books at Home\") +\n  xlab(\"Score\") + ylab(\"No.of\\nBooks\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1), \n        legend.position = \"none\")\n\n\n\n\n\n\n\nObservation: Based on the plot, there is a relationship between the number of books at home and the Reading scores. Focusing on the median values for each ridge (as shown by the divider line between the two colours), the values increase as the number of books at home increase. The only exception is the median value of Reading scores for students with more than 500 books at home, which is lower than that for 201-500 books.\nHence, we can see a clearer trend between Reading scores and number of books as compared to that between Reading scores and the Index of Economic, Social, and Cultural Status values. Further analysis can be conducted to determine the correlation coefficient of the relationship and whether the relationship is statistically significant."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#conclusion",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "In conclusion, the ggplot2 package is a powerful package for exploratory data analysis through visualisation. The PISA performance dataset is interesting and contains useful data for studying Singapore students’ academic performance in the three subjects at age 15. The insights gained from analysing the various questions posed in this take-home exercise provides a preview of the data analyses that can be conducted in further studies to better understand how various factors including gender, type of school, and socioeconomic status affect academic performance, so as to better inform education policy planning in Singapore."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#key-references",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#key-references",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "R for Visual Analytics.\nR for Data Science.\nFundamentals of Data Visualisation.\nPISA 2022 Database and Technical Report.\nOECD on Singapore: Rapid Improvement Followed by Strong Performance.\n\n~~~ End of Take-home Exercise 1 ~~~"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "title": "In-class Exercise 7",
    "section": "",
    "text": "&lt;placeholder&gt;\n~~~ End of In-class Exercise 7 ~~~"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "title": "In-class Exercise 1: Now You See It!",
    "section": "",
    "text": "Note: Updated on 18 January 2024 to make editorial edits."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#overview-and-learning-outcomes",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#overview-and-learning-outcomes",
    "title": "In-class Exercise 1: Now You See It!",
    "section": "1 Overview and Learning Outcomes",
    "text": "1 Overview and Learning Outcomes\nThis is the in-class exercise 1.\nThe learning outcomes are to load the relevant R packages and import the required dataset into the environment."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#loading-r-packages",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#loading-r-packages",
    "title": "In-class Exercise 1: Now You See It!",
    "section": "2 Loading R Packages",
    "text": "2 Loading R Packages\nIn this in-class exercise, two R packages will be used. They are:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data; and\nhaven for importing SAS files.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed in the computer. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, haven)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#importing-data",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#importing-data",
    "title": "In-class Exercise 1: Now You See It!",
    "section": "3 Importing Data",
    "text": "3 Importing Data\nThe dataset used in this in-class exercise is the PISA 2022 student questionnaire data file, which is in the SAS file format. It is imported into the R environment using the read_sas() function in the haven package and stored as the R object, stu_qqq.\n\nstu_qqq = read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\nThe tibble data frame, stu_qqq, has 1,279 columns (variables) and 613,744 rows (observations).\nThere are 6,606 rows with the country code “SGP”, which represents Singapore. The filter() function in the dplyr package is used to obtain these rows, and stored as the R object, stu_qqq_SG.\n\nstu_qqq_SG = stu_qqq %&gt;% filter(CNT == \"SGP\")\n\nThe tibble data frame, stu_qqq_SG, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(stu_qqq_SG, \"data/stu_qqq_SG.rds\")\n\n\nstu_qqq_SG = read_rds(\"data/stu_qqq_SG.rds\")\n\n~~~ End of In-class Exercise 1 ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html",
    "title": "Hands-on Exercise 9",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 9 of the R for Visual Analytics book.\nVisualising distribution is not new in statistical analysis. Chapter 1 covered some of the popular statistical graphic methods for visualising distribution. They are histogram, probability density curve (pdf), boxplot, notch plot, and violin plot, using the ggplot2 package. Chapter 9 will cover two relatively new statistical graphic methods for visualising distribution. They are ridgeline plot and raincloud plot using the ggplot2 package and its extensions.\n\n\n\n\n\nIn this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nggthemes for extra themes, geoms, and scales for ggplot2;\nggridges for creating ridgeline plots; and\nggdist for visualising distributions and uncertainty.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, ggthemes,\n               ggridges, ggdist)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, exam.\n\nexam = read_csv(\"data/Exam_data.csv\")\n\nThe tibble data frame, exam_data, has 7 columns and 322 rows.\n\nIt consists of the year-end examination grades of a cohort of 322 Primary 3 students from a local school.\nThe 7 variables/attributes are:\n\nCategorical: ID, CLASS, GENDER, and RACE.\nContinuous: MATHS, ENGLISH, and SCIENCE.\n\n\n\n\n\n\nxxx\n\n\nxxx\n\n\n\nxxx\n\n\n\nxxx\n\n\n\nxxx\n\n\n\nxxx\n\n\n\n\nxxx\n\n\nxxx\n\n\n\nxxx\n\n\n\nxxx\n\n\n\nxxx\n\n\n\nxxx\n\n\n\nxxx\n\n\n\nxxx\n\n\n\n\nxxx\n~~~ End of Hands-on Exercise 9 ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#overview-and-learning-outcomes",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#overview-and-learning-outcomes",
    "title": "Hands-on Exercise 9",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 9 of the R for Visual Analytics book.\nVisualising distribution is not new in statistical analysis. Chapter 1 covered some of the popular statistical graphic methods for visualising distribution. They are histogram, probability density curve (pdf), boxplot, notch plot, and violin plot, using the ggplot2 package. Chapter 9 will cover two relatively new statistical graphic methods for visualising distribution. They are ridgeline plot and raincloud plot using the ggplot2 package and its extensions."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#getting-started",
    "title": "Hands-on Exercise 9",
    "section": "",
    "text": "In this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nggthemes for extra themes, geoms, and scales for ggplot2;\nggridges for creating ridgeline plots; and\nggdist for visualising distributions and uncertainty.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, ggthemes,\n               ggridges, ggdist)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, exam.\n\nexam = read_csv(\"data/Exam_data.csv\")\n\nThe tibble data frame, exam_data, has 7 columns and 322 rows.\n\nIt consists of the year-end examination grades of a cohort of 322 Primary 3 students from a local school.\nThe 7 variables/attributes are:\n\nCategorical: ID, CLASS, GENDER, and RACE.\nContinuous: MATHS, ENGLISH, and SCIENCE."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#introducing-tidytext",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#introducing-tidytext",
    "title": "Hands-on Exercise 9",
    "section": "",
    "text": "xxx\n\n\nxxx\n\n\n\nxxx\n\n\n\nxxx\n\n\n\nxxx\n\n\n\nxxx"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#basic-concept-of-term-frequency-inverse-document-frequency-tf-idf",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#basic-concept-of-term-frequency-inverse-document-frequency-tf-idf",
    "title": "Hands-on Exercise 9",
    "section": "",
    "text": "xxx\n\n\nxxx\n\n\n\nxxx\n\n\n\nxxx\n\n\n\nxxx\n\n\n\nxxx\n\n\n\nxxx\n\n\n\nxxx"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#references",
    "title": "Hands-on Exercise 9",
    "section": "",
    "text": "xxx\n~~~ End of Hands-on Exercise 9 ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07C.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07C.html",
    "title": "Hands-on Exercise 7C",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 23 of the R for Visual Analytics book.\nThe learning outcomes are to use the appropriate functions of the tmap and tidyverse packages to plot analytical maps through the following steps:\n\nImport geospatial data in rds format into the R environment.\nCreate cartographic quality choropleth maps using functions from the tmap package.\nCreate a rate map.\nCreate a percentile map.\nCreate a boxmap.\n\n\n\n\n\n\nIn this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nsf for handling geospatial data; and\ntmap for thematic mapping.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, sf, tmap)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_rds() function in the readr package and stored as the R object, NGA_wp. The data is from a prepared dataset, NGA_wp.rds, which is a polygon feature data frame providing information on water points in Nigeria at the LGA level.\n\nNGA_wp = read_rds(\"data/rds/NGA_wp.rds\")\n\n\n\n\n\n\n\nThe non-functional water points, and total functional water points are plotted side by side.\n\np1 = tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional\\nwater point by LGAs\",\n            legend.outside = FALSE)\n\np2 = tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total\\nwater point by LGAs\",\n            legend.outside = FALSE)\n\ntmap_arrange(p2, p1, nrow = 1)\n\n\n\n\n\n\n\n\nIt is important to consider rates on top of simply counts of things, given that water points are not equally distributed in space.\n\n\nThe proportion of functional water points and the proportion of non-functional water points in each LGA are derived. The mutate() function in the dplyr package is used to derive two fields, “pct_functional”, and “pct_nonfunctional”.\n\nNGA_wp = NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\n\nThe map of rates (for functional water points) is then plotted below.\n\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\",\n          legend.hist = TRUE) +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water point by LGAs\",\n            legend.outside = TRUE)\n\n\n\n\n\n\n\n\nExtreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin, 1994).\n\n\nThe percentile map is a special type of a quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\nFirst, NA records are removed. Then, the customised classification method is created and the relevant values are extracted.\nWhen variables are extracted from an sf data frame, the geometry is extracted as well. For mapping and spatial manipulation, this is the expected behavior, but many base R functions cannot deal with the geometry. Specifically, the quantile() function gives an error. As a result st_set_geomtry(NULL) function is used to drop geometry field.\n\nNGA_wp = NGA_wp %&gt;%\n  drop_na()\n\npercent = c(0,.01,.1,.5,.9,.99,1)\n\nvar = NGA_wp[\"pct_functional\"] %&gt;%\n  st_set_geometry(NULL)\n\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\n\n\nWriting a function has three big advantages over using copy-and-paste:\n\nCan give a function an evocative name that makes your code easier to understand.\nAs requirements change, you only need to update code in one place, instead of many.\nEliminate the chance of making incidental mistakes when copying and pasting (i.e. updating a variable name in one place, but not in another).\n\nSource: Chapter 19: Functions of R for Data Science.\n\n\nFirst, an R function is written to extract a variable (i.e. “wp_nonfunctional”) as a vector out of an sf data frame.\n\nNote:\n\nArguments:\n\n“vname”: variable name (as character, in quotes).\n“df”: name of sf data frame.\n\nReturn:\n\nv: vector with values (without a column name).\n\n\n\n\nget.var = function(vname,df) {\n  v = df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v = unname(v[,1])\n  return(v)\n}\n\n\n\n\nThen, a percentile mapping function is written.\n\npercentmap = function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent = c(0,.01,.1,.5,.9,.99,1)\n  var = get.var(vnam, df)\n  bperc = quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n\nA bare bones implementation is done below. Additional arguments involving the title, legend positioning, etc. could be passed to the function to customise various features of the map.\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\n\n\n\n\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category.\n\nWhen there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence.\nIn contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\n\nNote: To create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\n\nAn R function that creates break points for a box map is written.\n\nArguments:\n\n“v”: vector with observations.\n“mult”: multiplier for IQR (default 1.5).\n\nReturn:\n\n“bb”: vector with 7 break points compute quartile and fences.\n\n\n\nboxbreaks = function(v,mult=1.5) {\n  qv = unname(quantile(v))\n  iqr = qv[4] - qv[2]\n  upfence = qv[4] + mult * iqr\n  lofence = qv[2] - mult * iqr\n  # initialize break points vector\n  bb = vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence &lt; qv[1]) {  # no lower outliers\n    bb[1] = lofence\n    bb[2] = floor(qv[1])\n  } else {\n    bb[2] = lofence\n    bb[1] = qv[1]\n  }\n  if (upfence &gt; qv[5]) { # no upper outliers\n    bb[7] = upfence\n    bb[6] = ceiling(qv[5])\n  } else {\n    bb[6] = upfence\n    bb[7] = qv[5]\n  }\n  bb[3:5] = qv[2:4]\n  return(bb)\n}\n\n\n\n\nThe same get.var function written at sub-section 23.5.2.1 is reused. The boxbreaks function is tested.\n\nvar = get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\n\nAn R function to create a box map.\n\nArguments:\n\n“vnam”: variable name (as character, in quotes).\n“df”: simple features polygon layer.\n“legtitle”: legend title.\n“mtitle”: map title.\n“multi”: mutliplier for IQR.\n\nReturn: a tmap elements (i.e., plots a map).\n\n\nboxmap = function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var = get.var(vnam,df)\n  bb = boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"&lt; 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"&gt; 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)\n\n\n\n\n~~~ End of Hands-on Exercise 7C ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07C.html#overview-and-learning-outcomes",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07C.html#overview-and-learning-outcomes",
    "title": "Hands-on Exercise 7C",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 23 of the R for Visual Analytics book.\nThe learning outcomes are to use the appropriate functions of the tmap and tidyverse packages to plot analytical maps through the following steps:\n\nImport geospatial data in rds format into the R environment.\nCreate cartographic quality choropleth maps using functions from the tmap package.\nCreate a rate map.\nCreate a percentile map.\nCreate a boxmap."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07C.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07C.html#getting-started",
    "title": "Hands-on Exercise 7C",
    "section": "",
    "text": "In this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nsf for handling geospatial data; and\ntmap for thematic mapping.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, sf, tmap)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_rds() function in the readr package and stored as the R object, NGA_wp. The data is from a prepared dataset, NGA_wp.rds, which is a polygon feature data frame providing information on water points in Nigeria at the LGA level.\n\nNGA_wp = read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07C.html#basic-choropleth-mapping",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07C.html#basic-choropleth-mapping",
    "title": "Hands-on Exercise 7C",
    "section": "",
    "text": "The non-functional water points, and total functional water points are plotted side by side.\n\np1 = tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional\\nwater point by LGAs\",\n            legend.outside = FALSE)\n\np2 = tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total\\nwater point by LGAs\",\n            legend.outside = FALSE)\n\ntmap_arrange(p2, p1, nrow = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07C.html#choropleth-map-for-rates",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07C.html#choropleth-map-for-rates",
    "title": "Hands-on Exercise 7C",
    "section": "",
    "text": "It is important to consider rates on top of simply counts of things, given that water points are not equally distributed in space.\n\n\nThe proportion of functional water points and the proportion of non-functional water points in each LGA are derived. The mutate() function in the dplyr package is used to derive two fields, “pct_functional”, and “pct_nonfunctional”.\n\nNGA_wp = NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\n\nThe map of rates (for functional water points) is then plotted below.\n\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\",\n          legend.hist = TRUE) +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water point by LGAs\",\n            legend.outside = TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07C.html#extreme-value-maps",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07C.html#extreme-value-maps",
    "title": "Hands-on Exercise 7C",
    "section": "",
    "text": "Extreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin, 1994).\n\n\nThe percentile map is a special type of a quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\nFirst, NA records are removed. Then, the customised classification method is created and the relevant values are extracted.\nWhen variables are extracted from an sf data frame, the geometry is extracted as well. For mapping and spatial manipulation, this is the expected behavior, but many base R functions cannot deal with the geometry. Specifically, the quantile() function gives an error. As a result st_set_geomtry(NULL) function is used to drop geometry field.\n\nNGA_wp = NGA_wp %&gt;%\n  drop_na()\n\npercent = c(0,.01,.1,.5,.9,.99,1)\n\nvar = NGA_wp[\"pct_functional\"] %&gt;%\n  st_set_geometry(NULL)\n\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\n\n\nWriting a function has three big advantages over using copy-and-paste:\n\nCan give a function an evocative name that makes your code easier to understand.\nAs requirements change, you only need to update code in one place, instead of many.\nEliminate the chance of making incidental mistakes when copying and pasting (i.e. updating a variable name in one place, but not in another).\n\nSource: Chapter 19: Functions of R for Data Science.\n\n\nFirst, an R function is written to extract a variable (i.e. “wp_nonfunctional”) as a vector out of an sf data frame.\n\nNote:\n\nArguments:\n\n“vname”: variable name (as character, in quotes).\n“df”: name of sf data frame.\n\nReturn:\n\nv: vector with values (without a column name).\n\n\n\n\nget.var = function(vname,df) {\n  v = df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v = unname(v[,1])\n  return(v)\n}\n\n\n\n\nThen, a percentile mapping function is written.\n\npercentmap = function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent = c(0,.01,.1,.5,.9,.99,1)\n  var = get.var(vnam, df)\n  bperc = quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n\nA bare bones implementation is done below. Additional arguments involving the title, legend positioning, etc. could be passed to the function to customise various features of the map.\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\n\n\n\n\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category.\n\nWhen there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence.\nIn contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\n\nNote: To create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\n\nAn R function that creates break points for a box map is written.\n\nArguments:\n\n“v”: vector with observations.\n“mult”: multiplier for IQR (default 1.5).\n\nReturn:\n\n“bb”: vector with 7 break points compute quartile and fences.\n\n\n\nboxbreaks = function(v,mult=1.5) {\n  qv = unname(quantile(v))\n  iqr = qv[4] - qv[2]\n  upfence = qv[4] + mult * iqr\n  lofence = qv[2] - mult * iqr\n  # initialize break points vector\n  bb = vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence &lt; qv[1]) {  # no lower outliers\n    bb[1] = lofence\n    bb[2] = floor(qv[1])\n  } else {\n    bb[2] = lofence\n    bb[1] = qv[1]\n  }\n  if (upfence &gt; qv[5]) { # no upper outliers\n    bb[7] = upfence\n    bb[6] = ceiling(qv[5])\n  } else {\n    bb[6] = upfence\n    bb[7] = qv[5]\n  }\n  bb[3:5] = qv[2:4]\n  return(bb)\n}\n\n\n\n\nThe same get.var function written at sub-section 23.5.2.1 is reused. The boxbreaks function is tested.\n\nvar = get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\n\nAn R function to create a box map.\n\nArguments:\n\n“vnam”: variable name (as character, in quotes).\n“df”: simple features polygon layer.\n“legtitle”: legend title.\n“mtitle”: map title.\n“multi”: mutliplier for IQR.\n\nReturn: a tmap elements (i.e., plots a map).\n\n\nboxmap = function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var = get.var(vnam,df)\n  bb = boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"&lt; 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"&gt; 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)\n\n\n\n\n~~~ End of Hands-on Exercise 7C ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07A.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07A.html",
    "title": "Hands-on Exercise 7A",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 21 of the R for Visual Analytics book.\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore based on the Master Plan 2014 subzone boundaries.\nThe learning outcome is to plot functional and truthful choropleth maps using the tmap package.\n\n\n\n\n\nIn this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nsf for handling geospatial data; and\ntmap for thematic mapping.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, sf, tmap)\n\n\n\n\nThe aspatial dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, popdata.\nThe data contains information from the Department of Statistics regarding Singapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in a csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. Although it does not contain any coordinates values, but its PA and SZ fields can be used as unique identifiers to geocode to the geospatial shapefile.\n\npopdata = read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\nThe tibble data frame, popdata, has 7 columns and 984,656 rows.\nUsing popdata, a data table consisting of 2020 values would need to be prepared to include the variables “PA”, “SZ”, “YOUNG”, “ECONOMY ACTIVE”, “AGED”, and “TOTAL, DEPENDENCY”.\n\n“YOUNG”: age group 0 to 4 until age groyup 20 to 24;\n“ECONOMY ACTIVE”: age group 25-29 until age group 60-64;\n“AGED”: age group 65 and above;\n“TOTAL”: all age group; and\n“DEPENDENCY”: the ratio between young and aged against economy active group.\n\nThe pivot_wider() function in the tidyr package, and the mutate(), filter(), group_by(), and select() functions in the dplyr package are used.\n\npopdata2020 = popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\nThe values in the “PA” and “SZ” fields are converted to uppercase to facilitate joining later on. This is because the values of the “PA” and “SZ” fields are in uppercase and lowercase , while the “SUBZONE_N” and “PLN_AREA_N” are in uppercase.\n\npopdata2020 = popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\n\n\nThe geospatial dataset for this hands-on exercise is imported into the R environment using the st_read() function in the sf package and stored as the R object, mpsz.\nThe data contains information regarding the Master Plan 2014 Subzone Boundary (i.e. MP14_SUBZONE_WEB_PL) in the ESRI shapefile format. This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\n\nmpsz = st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jmphosis\\ISSS608\\Hands-on_Ex\\Hands-on_Ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe geospatial objects are multipolygon features. There are a total of 323 features and 15 fields in mpsz simple feature data frame. mpsz is in svy21 projected coordinate system. The bounding box provides the x extend and y extend of the data.\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n\nThe left_join() function in the dplyr package is used to join the geographical data and attribute (aspatial) table using planning subzone names, i.e., “SUBZONE_N” and “SZ” as the common identifier.\nThe function is used with the mpsz simple feature data frame as the left data table is to ensure that the output will be a simple feature data frame.\n\nmpsz_pop2020 = left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\n\n\n\nTwo approaches can be used to prepare the thematic map using the tmap package:\n\nPlott a thematic map quickly using the qtm() function; and\nPlot a highly customisable thematic map using tmap elements.\n\n\n\nThe easiest and quickest way to draw a choropleth map using the tmap package is by using the qtm() function. It is concise and provides a good default visualisation in many cases.\nA cartographic standard choropleth map is plotted below.\n\nThe tmap_mode() function with the value “plot” is used to produce a static map. For an interactive mode, the “view” value should be used.\nThe “fille”argument is used to map the attribute (i.e. DEPENDENCY).\n\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\nDespite its usefulness in drawing a choropleth map quickly and easily, the disadvantge of the qtm() function is that the aesthetics of individual layers are harder to control. To draw a high quality cartographic choropleth map, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio\\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\nThe basic building block of tmap is the tm_shape() function, followed by one or more layer elements such as tm_fill() and tm_polygons() functions.\nThe tm_shape() function is used to define the input data (i.e mpsz_pop2020) and the tm_polygons() function is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as “Dependency” to the tm_polygons() function.\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\nNote:\n\nThe default interval binning used is called “pretty”.\nThe default colour scheme used is YlOrRd of ColorBrewer.\nBy default, missing values will be shaded in grey.\n\n\n\n\n\nThe tm_polygons() function is a wraper of the tm_fill() and tm_border() functions. The tm_fill() function shades the polygons by using the default colour scheme and the tm_borders() function adds the borders of the shapefile onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\nThe “alpha” argument is used in the tm_borders() function to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBesides the “alpha” argument, there are three other arguments for the tm_borders() function:\n\n“col” for border colour;\n“lwd” for border line width (default is 1); and\n“lty” for border line type (default is “solid”).\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\nMost choropleth maps employ some methods of data classification. The aim is to take a large number of observations and group them into data ranges or classes.\nThe tmap package provides a total ten data classification methods: “fixed”, “sd”, “equal”, “pretty (default)”, “quantile”, “kmeans”, “hclust”, “bclust”, “fisher”, and “jenks”. The “style” argument in the tm_fill() or tm_polygons() functions is used to indicate the data classification method.\n\n\nA “jenks” data classification that used 5 classes is plotted.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nAn “equal” data classification that used 5 classes is plotted.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nNote: The distribution by the quantile data classification method is more even than the equal data classification method. Warning: Maps Lie!\n\n\n\n\nUsing different styles:\n\nQuantilesdPrettyKmeansHClustBClustLOG10_PrettyFisherDPIHHeadtails\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"sd\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"pretty\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"hclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"bclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"log10_pretty\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"fisher\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"dpih\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"headtails\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nUsing different number of bins:\n\nn=2n=5n=6n=10n=20\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 20,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nUsing custom breaks:\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\n\ntm_shape(mpsz_pop2020)+   \n  tm_fill(\"DEPENDENCY\",           \n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +   \n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nThe tmap package supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n\nThe preferred colour is assigned to the “palette“ argument of the tm_fill() function.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nTo reverse the colour shading, a “-” prefix is added.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nThe map layout refers to the combination of all map elements into a cohesive map. Map elements include, among others, the objects to be mapped, title, scale bar, compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n\nIn the tmap package, several “legend” argument options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nThe tmap package allows a wide variety of layout settings to be changed using the tmap_style() function.\nThe “classic” style is used below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\nBesides map style, the tmap package also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nThe tm_compass(), tm_scale_bar(), and tm_grid() functions are used to add compass, scale bar and grid lines respectively.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nTo reset the default style, the tmap_style” function is set back to the “white” value.\n\ntmap_style(\"white\")\n\n\n\n\n\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn the tmap package, small multiple maps can be plotted in three ways:\n\nBy assigning multiple values to at least one of the asthetic arguments;\nBy defining a group-by variable in the tm_facets() function; and\nBy creating multiple stand-alone maps with the tmap_arrange() function.\n\n\n\nSmall multiple choropleth maps are created below by defining “ncols” argument in the tm_fill() function.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\nSmall multiple choropleth maps are created below by assigning multiple values to at least one of the aesthetic arguments.\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\nSmall multiple choropleth maps are created using the tm_facets() function to show the different regions.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=FALSE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nSmall multiple choropleth maps are created as multiple stand-alone maps with the tmap_arrange() function.\n\nyoungmap = tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap = tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nInstead of creating small multiple choropleth maps, the selection function to map spatial objects meeting the selection criterion can also be used. The map below is for the Central Region only.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions\n\n~~~ End of Hands-on Exercise 7A ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07A.html#overview-and-learning-outcomes",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07A.html#overview-and-learning-outcomes",
    "title": "Hands-on Exercise 7A",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 21 of the R for Visual Analytics book.\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore based on the Master Plan 2014 subzone boundaries.\nThe learning outcome is to plot functional and truthful choropleth maps using the tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07A.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07A.html#getting-started",
    "title": "Hands-on Exercise 7A",
    "section": "",
    "text": "In this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nsf for handling geospatial data; and\ntmap for thematic mapping.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, sf, tmap)\n\n\n\n\nThe aspatial dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, popdata.\nThe data contains information from the Department of Statistics regarding Singapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in a csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. Although it does not contain any coordinates values, but its PA and SZ fields can be used as unique identifiers to geocode to the geospatial shapefile.\n\npopdata = read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\nThe tibble data frame, popdata, has 7 columns and 984,656 rows.\nUsing popdata, a data table consisting of 2020 values would need to be prepared to include the variables “PA”, “SZ”, “YOUNG”, “ECONOMY ACTIVE”, “AGED”, and “TOTAL, DEPENDENCY”.\n\n“YOUNG”: age group 0 to 4 until age groyup 20 to 24;\n“ECONOMY ACTIVE”: age group 25-29 until age group 60-64;\n“AGED”: age group 65 and above;\n“TOTAL”: all age group; and\n“DEPENDENCY”: the ratio between young and aged against economy active group.\n\nThe pivot_wider() function in the tidyr package, and the mutate(), filter(), group_by(), and select() functions in the dplyr package are used.\n\npopdata2020 = popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\nThe values in the “PA” and “SZ” fields are converted to uppercase to facilitate joining later on. This is because the values of the “PA” and “SZ” fields are in uppercase and lowercase , while the “SUBZONE_N” and “PLN_AREA_N” are in uppercase.\n\npopdata2020 = popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\n\n\nThe geospatial dataset for this hands-on exercise is imported into the R environment using the st_read() function in the sf package and stored as the R object, mpsz.\nThe data contains information regarding the Master Plan 2014 Subzone Boundary (i.e. MP14_SUBZONE_WEB_PL) in the ESRI shapefile format. This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\n\nmpsz = st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jmphosis\\ISSS608\\Hands-on_Ex\\Hands-on_Ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe geospatial objects are multipolygon features. There are a total of 323 features and 15 fields in mpsz simple feature data frame. mpsz is in svy21 projected coordinate system. The bounding box provides the x extend and y extend of the data.\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n\nThe left_join() function in the dplyr package is used to join the geographical data and attribute (aspatial) table using planning subzone names, i.e., “SUBZONE_N” and “SZ” as the common identifier.\nThe function is used with the mpsz simple feature data frame as the left data table is to ensure that the output will be a simple feature data frame.\n\nmpsz_pop2020 = left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07A.html#plotting-choropleth-maps-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07A.html#plotting-choropleth-maps-using-tmap",
    "title": "Hands-on Exercise 7A",
    "section": "",
    "text": "Two approaches can be used to prepare the thematic map using the tmap package:\n\nPlott a thematic map quickly using the qtm() function; and\nPlot a highly customisable thematic map using tmap elements.\n\n\n\nThe easiest and quickest way to draw a choropleth map using the tmap package is by using the qtm() function. It is concise and provides a good default visualisation in many cases.\nA cartographic standard choropleth map is plotted below.\n\nThe tmap_mode() function with the value “plot” is used to produce a static map. For an interactive mode, the “view” value should be used.\nThe “fille”argument is used to map the attribute (i.e. DEPENDENCY).\n\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\nDespite its usefulness in drawing a choropleth map quickly and easily, the disadvantge of the qtm() function is that the aesthetics of individual layers are harder to control. To draw a high quality cartographic choropleth map, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio\\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\nThe basic building block of tmap is the tm_shape() function, followed by one or more layer elements such as tm_fill() and tm_polygons() functions.\nThe tm_shape() function is used to define the input data (i.e mpsz_pop2020) and the tm_polygons() function is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as “Dependency” to the tm_polygons() function.\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\nNote:\n\nThe default interval binning used is called “pretty”.\nThe default colour scheme used is YlOrRd of ColorBrewer.\nBy default, missing values will be shaded in grey.\n\n\n\n\n\nThe tm_polygons() function is a wraper of the tm_fill() and tm_border() functions. The tm_fill() function shades the polygons by using the default colour scheme and the tm_borders() function adds the borders of the shapefile onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\nThe “alpha” argument is used in the tm_borders() function to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBesides the “alpha” argument, there are three other arguments for the tm_borders() function:\n\n“col” for border colour;\n“lwd” for border line width (default is 1); and\n“lty” for border line type (default is “solid”).\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\nMost choropleth maps employ some methods of data classification. The aim is to take a large number of observations and group them into data ranges or classes.\nThe tmap package provides a total ten data classification methods: “fixed”, “sd”, “equal”, “pretty (default)”, “quantile”, “kmeans”, “hclust”, “bclust”, “fisher”, and “jenks”. The “style” argument in the tm_fill() or tm_polygons() functions is used to indicate the data classification method.\n\n\nA “jenks” data classification that used 5 classes is plotted.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nAn “equal” data classification that used 5 classes is plotted.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nNote: The distribution by the quantile data classification method is more even than the equal data classification method. Warning: Maps Lie!\n\n\n\n\nUsing different styles:\n\nQuantilesdPrettyKmeansHClustBClustLOG10_PrettyFisherDPIHHeadtails\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"sd\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"pretty\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"hclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"bclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"log10_pretty\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"fisher\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"dpih\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"headtails\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nUsing different number of bins:\n\nn=2n=5n=6n=10n=20\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 20,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nUsing custom breaks:\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\n\ntm_shape(mpsz_pop2020)+   \n  tm_fill(\"DEPENDENCY\",           \n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +   \n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nThe tmap package supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n\nThe preferred colour is assigned to the “palette“ argument of the tm_fill() function.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nTo reverse the colour shading, a “-” prefix is added.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nThe map layout refers to the combination of all map elements into a cohesive map. Map elements include, among others, the objects to be mapped, title, scale bar, compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n\nIn the tmap package, several “legend” argument options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nThe tmap package allows a wide variety of layout settings to be changed using the tmap_style() function.\nThe “classic” style is used below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\nBesides map style, the tmap package also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nThe tm_compass(), tm_scale_bar(), and tm_grid() functions are used to add compass, scale bar and grid lines respectively.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nTo reset the default style, the tmap_style” function is set back to the “white” value.\n\ntmap_style(\"white\")\n\n\n\n\n\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn the tmap package, small multiple maps can be plotted in three ways:\n\nBy assigning multiple values to at least one of the asthetic arguments;\nBy defining a group-by variable in the tm_facets() function; and\nBy creating multiple stand-alone maps with the tmap_arrange() function.\n\n\n\nSmall multiple choropleth maps are created below by defining “ncols” argument in the tm_fill() function.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\nSmall multiple choropleth maps are created below by assigning multiple values to at least one of the aesthetic arguments.\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\nSmall multiple choropleth maps are created using the tm_facets() function to show the different regions.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=FALSE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nSmall multiple choropleth maps are created as multiple stand-alone maps with the tmap_arrange() function.\n\nyoungmap = tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap = tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nInstead of creating small multiple choropleth maps, the selection function to map spatial objects meeting the selection criterion can also be used. The map below is for the Central Region only.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07A.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07A.html#references",
    "title": "Hands-on Exercise 7A",
    "section": "",
    "text": "tmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions\n\n~~~ End of Hands-on Exercise 7A ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05E.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05E.html",
    "title": "Hands-on Exercise 5E",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 16 of the R for Visual Analytics book.\nThe learning outcomes are:\n\nManipulate transaction data into a treemap stccuture using selected functions provided in the dplyr package.\nPlot static treemaps using the treemap package.\nDesign interactive treemaps using the d3treeR package.\n\n\n\n\n\n\nIn this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\ntreemap for plotting treemaps; and\nd3treeR for plotting interactive treemaps.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, treemap,\n               treemapify, d3treeR)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, realis2018. The data contains information regarding private property transaction records in 2018 from the Urban Redevelopment Authority.\n\nrealis2018 = read_csv(\"data/realis2018.csv\")\n\nThe tibble data frame, realis2018, has 20 columns and 23,205 rows.\n\n\n\nThe data frame, realis2018, is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap.\nHence, the raw data frame should be manipulated to prepre a suitable data frame by:\n\nGrouping transaction records by “Project Name”, “Planning Region”, “Planning Area”, “Property Type”, and “Type of Sale”, and\nComputing “Total Unit Sold”, “Total Area”, “Median Unit Price”, and “Median Transacted Price” by applying the appropriate summary statistics on “No. of Units”, “Area (sqm)”, “Unit Price ($ psm)”, and “Transacted Price ($)” respectively.\n\nThe following functions in the dplyr package would be used:\n\ngroup_by() breaks down a data frame into specified groups of rows; and\nsummarise() computes the summary for each group.\n\nGrouping affects the verbs as follows:\n\nGrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\nGrouped arrange() is the same as ungrouped; unless “.by_group = TRUE” is set, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette (“window-functions”).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\n\n\nrealis2018_summarised = realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\nNote: Aggregation functions such as sum() and median() obey the usual rule of missing values: if there is any missing value in the input, the output will be a missing value. The “na.rm” argument set as “TRUE” removes the missing values prior to computation.\n\n\n\n\n\nThe treemap package is specially designed to offer great flexibility in drawing treemaps. The core function, treemap(), offers at least 43 arguments.\n\n\nThe treemap() function in the treemap package is used to plot a treemap to show the distribution of median unit prices and total unit sold of resale condominium by geographical hierarchy in 2017.\nFirst, the records of resale condominium are selected using the filter() function in the dplyr package.\n\nrealis2018_selected = realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n\nA basic treempa is plotted using the treemap() function, with the three core arguments of “index”, “vSize”, and “vColor”.\n\nThe “index” vector must consist of at least two column names or else no hierarchy treemap will be plotted. If multiple column names are provided, the first name is the highest aggregation level, the second name the second highest aggregation level, etc.\nThe “vSize” argument must be a column that does not contain negative values. This is because its values will be used to map the sizes of the rectangles of the treemaps.\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\")\n\n\n\n\n\nNote: The treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectangles should be in different intensity showing, in the case above, median unit prices. Hence, the “vColor” argument is used in combination with the “type” argument to determine the colours of the rectangles. Without defining the “type” argument, it is assumed that “type = index”, in the case above, the hierarchy of planning areas.\n\n\n\n\nHence, the “type” argument set as “value” is added.\nThe rectangles are then coloured with different intensities of green, reflecting their respective median unit prices. The legend reveals that the values are binned into ten bins (i.e. 0-5000, 5000-10000, etc.) with an equal interval of 5000.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"        )\n\n\n\n\n\n\n\nThere are two arguments that determine the mapping to colour palettes: “mapping”, and “palette”. The only difference between “value” and “manual” is the default value for mapping.\n\nThe “value” treemap considers palette to be a diverging color palette (e.g., ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color.\nThe “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n\n\n\nA “value” type treemap is plotted below.\nAlthough the colour palette used is RdYlBu but there are no red rectangles in the treemap because all the median unit prices are positive. The reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"        )\n\n\n\n\n\n\n\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe colour scheme used is very confusing because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\")\n\n\n\n\nA single colour palette such as Blues is used instead.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\")\n\n\n\n\n\n\n\nThe treemap() function supports two popular treemap layouts: “squarified”, and “pivotSize”. The default is “pivotSize”.\n\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID).\nThe ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\n\n\nA squarified treemap is plotted by changing the “algorithm” argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\")\n\n\n\n\n\n\n\nWhen the “pivotSize” algorithm is used, the “sortID” argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\")\n\n\n\n\n\n\n\n\nThe treemapify package is specially developed to draw treemaps in ggplot2.\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n\nThe treemap is plotted by grouping by “Planning Region”.\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\nThe treemap is plotted by further grouping by “Planning Area”.\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\nBoundary lines are then added.\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")\n\n\n\n\n\n\n\n\nAn interactive treemap is created using two steps:\nFirst, the treemap() function is used to build a treemap, tm, using the selected variables.\n\ntm = treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\")\n\n\n\n\nThen, the d3tree() function is used to build an interactive treemap.\n\nd3tree(tm,rootname = \"Singapore\" )\n\n\n\n\n\n~~~ End of Hands-on Exercise 5E ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05E.html#overview-and-learning-outcomes",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05E.html#overview-and-learning-outcomes",
    "title": "Hands-on Exercise 5E",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 16 of the R for Visual Analytics book.\nThe learning outcomes are:\n\nManipulate transaction data into a treemap stccuture using selected functions provided in the dplyr package.\nPlot static treemaps using the treemap package.\nDesign interactive treemaps using the d3treeR package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05E.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05E.html#getting-started",
    "title": "Hands-on Exercise 5E",
    "section": "",
    "text": "In this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\ntreemap for plotting treemaps; and\nd3treeR for plotting interactive treemaps.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, treemap,\n               treemapify, d3treeR)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, realis2018. The data contains information regarding private property transaction records in 2018 from the Urban Redevelopment Authority.\n\nrealis2018 = read_csv(\"data/realis2018.csv\")\n\nThe tibble data frame, realis2018, has 20 columns and 23,205 rows.\n\n\n\nThe data frame, realis2018, is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap.\nHence, the raw data frame should be manipulated to prepre a suitable data frame by:\n\nGrouping transaction records by “Project Name”, “Planning Region”, “Planning Area”, “Property Type”, and “Type of Sale”, and\nComputing “Total Unit Sold”, “Total Area”, “Median Unit Price”, and “Median Transacted Price” by applying the appropriate summary statistics on “No. of Units”, “Area (sqm)”, “Unit Price ($ psm)”, and “Transacted Price ($)” respectively.\n\nThe following functions in the dplyr package would be used:\n\ngroup_by() breaks down a data frame into specified groups of rows; and\nsummarise() computes the summary for each group.\n\nGrouping affects the verbs as follows:\n\nGrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\nGrouped arrange() is the same as ungrouped; unless “.by_group = TRUE” is set, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette (“window-functions”).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\n\n\nrealis2018_summarised = realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\nNote: Aggregation functions such as sum() and median() obey the usual rule of missing values: if there is any missing value in the input, the output will be a missing value. The “na.rm” argument set as “TRUE” removes the missing values prior to computation."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05E.html#designing-treemap-treemap-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05E.html#designing-treemap-treemap-package",
    "title": "Hands-on Exercise 5E",
    "section": "",
    "text": "The treemap package is specially designed to offer great flexibility in drawing treemaps. The core function, treemap(), offers at least 43 arguments.\n\n\nThe treemap() function in the treemap package is used to plot a treemap to show the distribution of median unit prices and total unit sold of resale condominium by geographical hierarchy in 2017.\nFirst, the records of resale condominium are selected using the filter() function in the dplyr package.\n\nrealis2018_selected = realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n\nA basic treempa is plotted using the treemap() function, with the three core arguments of “index”, “vSize”, and “vColor”.\n\nThe “index” vector must consist of at least two column names or else no hierarchy treemap will be plotted. If multiple column names are provided, the first name is the highest aggregation level, the second name the second highest aggregation level, etc.\nThe “vSize” argument must be a column that does not contain negative values. This is because its values will be used to map the sizes of the rectangles of the treemaps.\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\")\n\n\n\n\n\nNote: The treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectangles should be in different intensity showing, in the case above, median unit prices. Hence, the “vColor” argument is used in combination with the “type” argument to determine the colours of the rectangles. Without defining the “type” argument, it is assumed that “type = index”, in the case above, the hierarchy of planning areas.\n\n\n\n\nHence, the “type” argument set as “value” is added.\nThe rectangles are then coloured with different intensities of green, reflecting their respective median unit prices. The legend reveals that the values are binned into ten bins (i.e. 0-5000, 5000-10000, etc.) with an equal interval of 5000.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"        )\n\n\n\n\n\n\n\nThere are two arguments that determine the mapping to colour palettes: “mapping”, and “palette”. The only difference between “value” and “manual” is the default value for mapping.\n\nThe “value” treemap considers palette to be a diverging color palette (e.g., ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color.\nThe “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n\n\n\nA “value” type treemap is plotted below.\nAlthough the colour palette used is RdYlBu but there are no red rectangles in the treemap because all the median unit prices are positive. The reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"        )\n\n\n\n\n\n\n\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe colour scheme used is very confusing because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\")\n\n\n\n\nA single colour palette such as Blues is used instead.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\")\n\n\n\n\n\n\n\nThe treemap() function supports two popular treemap layouts: “squarified”, and “pivotSize”. The default is “pivotSize”.\n\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID).\nThe ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\n\n\nA squarified treemap is plotted by changing the “algorithm” argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\")\n\n\n\n\n\n\n\nWhen the “pivotSize” algorithm is used, the “sortID” argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05E.html#designing-treemap-treemapify-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05E.html#designing-treemap-treemapify-package",
    "title": "Hands-on Exercise 5E",
    "section": "",
    "text": "The treemapify package is specially developed to draw treemaps in ggplot2.\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n\nThe treemap is plotted by grouping by “Planning Region”.\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\nThe treemap is plotted by further grouping by “Planning Area”.\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\nBoundary lines are then added.\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05E.html#designing-interactive-treemap-d3treer-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05E.html#designing-interactive-treemap-d3treer-package",
    "title": "Hands-on Exercise 5E",
    "section": "",
    "text": "An interactive treemap is created using two steps:\nFirst, the treemap() function is used to build a treemap, tm, using the selected variables.\n\ntm = treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\")\n\n\n\n\nThen, the d3tree() function is used to build an interactive treemap.\n\nd3tree(tm,rootname = \"Singapore\" )\n\n\n\n\n\n~~~ End of Hands-on Exercise 5E ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05C.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05C.html",
    "title": "Hands-on Exercise 5C",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 14 of the R for Visual Analytics book.\nHeatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observations in the rows and colouring the cells within the table.\nHeatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting any correlations that exist between them.\nThe learning outcome is to plot static and interactive heatmaps for visualising and analysing multivariate data.\n\n\n\n\n\nIn this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nseriation for ordering objects;\ndendextend for extending dendrogram objects to visualise and compare trees of hierarchical clusterings; and\nheatmaply for plotting interactive heatmaps.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, seriation,\n               dendextend, heatmaply)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, wh. The data is from the World Happiness 2018 report.\n\nwh = read_csv(\"data/WHData-2018.csv\")\n\nThe tibble data frame, wh, has 12 columns and 156 rows. Other than the “Country” and “Region” variables, the remaining variables are continuous numerical data.\n\n\n\nThe row names are changed to the country names instead of numbers.\n\nrow.names(wh) = wh$Country\n\nThe relevant variables are selected - “Happiness score”, “GDP per capita”, “social support”, “Health life expectancy”, “Freedom to make life choices”, “Generosity”, and “Perceptions of corruption”.\n\nwh1 = select(wh, c(3, 7:12))\n\nThe data frame is then converted to the data matrix.\n\nwh_matrix = data.matrix(wh)\n\n\n\n\n\nThere are many R packages and functions that can be used to drawing static heatmaps, they are:\n\nheatmap() function in the stats package. It draws a simple heatmap.\nheatmap.2() function in the gplots package. It draws an enhanced heatmap.\npheatmap() function in the pheatmap package. The package (also known as Pretty Heatmap) provides functions to draws pretty heatmaps and allows more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here.\n\nIn this sub-section, the most basic, heatmap() function is used.\n\n\nA heatmap is plotted using the heatmap() function in the stats package.\n\nwh_heatmap = heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\nNote: By default, the heatmap() function plots a cluster heatmap. The arguments “Rowv=NA” and “Colv=NA” are used to switch off the option of plotting the row and column dendrograms.\n\nIf the “Rowv” and “Colv” arguments are excluded, the plotted cluster heatmap would show the row and column dendrograms.\n\nwh_heatmap = heatmap(wh_matrix)\n\n\n\n\n\nNote: The order of both rows and columns is different compare to the native wh_matrix. This is because the heatmap would do a reordering using clusterisation, i.e., it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\nBased on the plot above, the red cells denote small values, and yellow cells denote large ones. This heatmap is not really informative. Indeed, the “Happiness score” variable has relatively higher values, what means that the other variables with small values all look the same. Thus, we need to normalise this matrix. This is done using the “scale” argument. It can be applied to rows or to columns depending on needs.\nA heatmap with normalisation column-wise is plotted below.\n\nwh_heatmap = heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\nNote: The values are now scaled. Also, the “margins” argument is used to ensure that the entire x-axis labels are displayed completely, and the “cexRow” and “cexCol” arguments are used to define the font size used for the y-axis and x-axis labels respectively.\n\n\n\n\n\nThe heatmaply package is for building interactive cluster heatmaps that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili. The Introduction to Heatmaply provides more information on the features and functions of the package.\nIn this sub-section, the heatmaply package is used to design an interactive cluster heatmap.\n\n\nThe heatmaply() function is used to plot the interactive heatmap for the wh_matrix.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\n\nNote:\n\nDifferent from the heatmap() function, the heatmaply() function places the default horizontal dendrogram on the right side. On the other hand, the text label of each row is placed on the left side.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\n\n\nWhen analysing a multivariate dataset, it is very common that the variables include values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation is commonly used before clustering. The three main data transformation methods are supported by the heatmaply() function are: scale, normalise, and percentilse.\n\n\nWhen all variables are (or assumed to be) from some normal distribution, then scaling (i.e., subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution. In such a case, each value would reflect the distance from the mean in units of standard deviation. The “scale” argument in the heatmaply() function supports column and row scaling.\nThe plot below uses the “scale” argument to scale column-wise.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\nThe plot below uses the “scale” argument to scale row-wise.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"row\")\n\n\n\n\n\n\n\n\nWhen variables in the dataset comes from possibly different (and non-normal) distributions, the normalise function can be used to bring the values to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations. This preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”. Different from scaling, the normalise method is performed on the input dataset, i.e., wh_matrix.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\nThis is similar to ranking the variables, but instead of keeping the rank values, they are divided by the maximal rank. This is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile. The benefit of the percentise method is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it. Similar to the normalise method, the percentise method is also performed on the input dataset, i.e., wh_matrix.\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nThe heatmaply package supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\n“distfun” is used to compute the distance (dissimilarity) between both rows and columns. Defaults to “dist”. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses “as.dist(1 - cor(t(x)))” as the distance metric (using the specified correlation method).\n“hclustfun” is used to compute the hierarchical clustering when “Rowv” or “Colv” are not dendrograms. Defaults to “hclust”.\n“dist_method” default is NULL, which results in “euclidean” being used. It can accept alternative character strings indicating the method to be passed to distfun. By default, “distfun” is “dist””, and hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\n“hclust_method” default is NULL, which results in “complete” method being used. It can accept alternative character strings indicating the method to be passed to “hclustfun”. By default hclustfun is hclust, and hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n\n\nA heatmap is plotted using a hierarchical clustering algorithm with “dist_method =”euclidean” and “hclust_method =”ward.D”.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n\nIn order to determine the best clustering method and number of clusters, the dend_expend() and find_k() functions in the dendextend package is used.\nFirst, the dend_expend() function is used to determine the recommended clustering method to be used. The output table shows that the “average” method should be used because it gave the highest optimum value.\n\nwh_d = dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), \n            method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nNext, the find_k() function is used to determine the optimal number of clusters. The plot shows that k=3 would be good.\n\nwh_clust = hclust(wh_d, method = \"average\")\nnum_k = find_k(wh_clust)\nplot(num_k)\n\n\n\n\nAfter obtaining the statistical analysis results, the heatmap is plotted.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n\nOne of the problems with hierarchical clustering is that it does not actually place the rows in a definite order; it merely constrains the space of possible orderings. For example, take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it does not tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nThe heatmaply() function uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimise the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nIn the plot below, the seriation algorithm of Optimal Leaf Ordering (OLO) is used. This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimise the sum of dissimilarities between adjacent leaves.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\nThe default option is “OLO” (Optimal leaf ordering) which optimises the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\nThe option “none” gives us dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n\nThe default colour palette uses by the heatmaply package is “viridis“. However, users can use other colour palettes to improve the aesthetics and visual friendliness of the heatmap.\nIn the plot below, the “Blues” colour palette of rColorBrewer is used.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n\nBesides providing a wide collection of arguments for meeting the statistical analysis needs, the heatmaply package also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the plot below, the following arguments are used:\n\n“k_row” is used to produce 5 groups.\n“margins” is used to change the top margin to 60 and row margin to 200.\n“fontsizw_row” and “fontsize_col” are used to change the font size for row and column labels to 4.\n“main” argument is used to write the main title of the plot.\n“xlab” and “ylab” are used to given the x-axis and y-axis labels respectively.\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )\n\n\n\n\n\n~~~ End of Hands-on Exercise 5C ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05C.html#overview-and-learning-outcomes",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05C.html#overview-and-learning-outcomes",
    "title": "Hands-on Exercise 5C",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 14 of the R for Visual Analytics book.\nHeatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observations in the rows and colouring the cells within the table.\nHeatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting any correlations that exist between them.\nThe learning outcome is to plot static and interactive heatmaps for visualising and analysing multivariate data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05C.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05C.html#getting-started",
    "title": "Hands-on Exercise 5C",
    "section": "",
    "text": "In this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nseriation for ordering objects;\ndendextend for extending dendrogram objects to visualise and compare trees of hierarchical clusterings; and\nheatmaply for plotting interactive heatmaps.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, seriation,\n               dendextend, heatmaply)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, wh. The data is from the World Happiness 2018 report.\n\nwh = read_csv(\"data/WHData-2018.csv\")\n\nThe tibble data frame, wh, has 12 columns and 156 rows. Other than the “Country” and “Region” variables, the remaining variables are continuous numerical data.\n\n\n\nThe row names are changed to the country names instead of numbers.\n\nrow.names(wh) = wh$Country\n\nThe relevant variables are selected - “Happiness score”, “GDP per capita”, “social support”, “Health life expectancy”, “Freedom to make life choices”, “Generosity”, and “Perceptions of corruption”.\n\nwh1 = select(wh, c(3, 7:12))\n\nThe data frame is then converted to the data matrix.\n\nwh_matrix = data.matrix(wh)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05C.html#creating-static-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05C.html#creating-static-heatmap",
    "title": "Hands-on Exercise 5C",
    "section": "",
    "text": "There are many R packages and functions that can be used to drawing static heatmaps, they are:\n\nheatmap() function in the stats package. It draws a simple heatmap.\nheatmap.2() function in the gplots package. It draws an enhanced heatmap.\npheatmap() function in the pheatmap package. The package (also known as Pretty Heatmap) provides functions to draws pretty heatmaps and allows more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here.\n\nIn this sub-section, the most basic, heatmap() function is used.\n\n\nA heatmap is plotted using the heatmap() function in the stats package.\n\nwh_heatmap = heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\nNote: By default, the heatmap() function plots a cluster heatmap. The arguments “Rowv=NA” and “Colv=NA” are used to switch off the option of plotting the row and column dendrograms.\n\nIf the “Rowv” and “Colv” arguments are excluded, the plotted cluster heatmap would show the row and column dendrograms.\n\nwh_heatmap = heatmap(wh_matrix)\n\n\n\n\n\nNote: The order of both rows and columns is different compare to the native wh_matrix. This is because the heatmap would do a reordering using clusterisation, i.e., it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\nBased on the plot above, the red cells denote small values, and yellow cells denote large ones. This heatmap is not really informative. Indeed, the “Happiness score” variable has relatively higher values, what means that the other variables with small values all look the same. Thus, we need to normalise this matrix. This is done using the “scale” argument. It can be applied to rows or to columns depending on needs.\nA heatmap with normalisation column-wise is plotted below.\n\nwh_heatmap = heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\nNote: The values are now scaled. Also, the “margins” argument is used to ensure that the entire x-axis labels are displayed completely, and the “cexRow” and “cexCol” arguments are used to define the font size used for the y-axis and x-axis labels respectively."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05C.html#creating-interactive-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05C.html#creating-interactive-heatmap",
    "title": "Hands-on Exercise 5C",
    "section": "",
    "text": "The heatmaply package is for building interactive cluster heatmaps that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili. The Introduction to Heatmaply provides more information on the features and functions of the package.\nIn this sub-section, the heatmaply package is used to design an interactive cluster heatmap.\n\n\nThe heatmaply() function is used to plot the interactive heatmap for the wh_matrix.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\n\nNote:\n\nDifferent from the heatmap() function, the heatmaply() function places the default horizontal dendrogram on the right side. On the other hand, the text label of each row is placed on the left side.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\n\n\nWhen analysing a multivariate dataset, it is very common that the variables include values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation is commonly used before clustering. The three main data transformation methods are supported by the heatmaply() function are: scale, normalise, and percentilse.\n\n\nWhen all variables are (or assumed to be) from some normal distribution, then scaling (i.e., subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution. In such a case, each value would reflect the distance from the mean in units of standard deviation. The “scale” argument in the heatmaply() function supports column and row scaling.\nThe plot below uses the “scale” argument to scale column-wise.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\nThe plot below uses the “scale” argument to scale row-wise.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"row\")\n\n\n\n\n\n\n\n\nWhen variables in the dataset comes from possibly different (and non-normal) distributions, the normalise function can be used to bring the values to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations. This preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”. Different from scaling, the normalise method is performed on the input dataset, i.e., wh_matrix.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\nThis is similar to ranking the variables, but instead of keeping the rank values, they are divided by the maximal rank. This is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile. The benefit of the percentise method is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it. Similar to the normalise method, the percentise method is also performed on the input dataset, i.e., wh_matrix.\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nThe heatmaply package supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\n“distfun” is used to compute the distance (dissimilarity) between both rows and columns. Defaults to “dist”. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses “as.dist(1 - cor(t(x)))” as the distance metric (using the specified correlation method).\n“hclustfun” is used to compute the hierarchical clustering when “Rowv” or “Colv” are not dendrograms. Defaults to “hclust”.\n“dist_method” default is NULL, which results in “euclidean” being used. It can accept alternative character strings indicating the method to be passed to distfun. By default, “distfun” is “dist””, and hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\n“hclust_method” default is NULL, which results in “complete” method being used. It can accept alternative character strings indicating the method to be passed to “hclustfun”. By default hclustfun is hclust, and hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n\n\nA heatmap is plotted using a hierarchical clustering algorithm with “dist_method =”euclidean” and “hclust_method =”ward.D”.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n\nIn order to determine the best clustering method and number of clusters, the dend_expend() and find_k() functions in the dendextend package is used.\nFirst, the dend_expend() function is used to determine the recommended clustering method to be used. The output table shows that the “average” method should be used because it gave the highest optimum value.\n\nwh_d = dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), \n            method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nNext, the find_k() function is used to determine the optimal number of clusters. The plot shows that k=3 would be good.\n\nwh_clust = hclust(wh_d, method = \"average\")\nnum_k = find_k(wh_clust)\nplot(num_k)\n\n\n\n\nAfter obtaining the statistical analysis results, the heatmap is plotted.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n\nOne of the problems with hierarchical clustering is that it does not actually place the rows in a definite order; it merely constrains the space of possible orderings. For example, take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it does not tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nThe heatmaply() function uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimise the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nIn the plot below, the seriation algorithm of Optimal Leaf Ordering (OLO) is used. This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimise the sum of dissimilarities between adjacent leaves.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\nThe default option is “OLO” (Optimal leaf ordering) which optimises the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\nThe option “none” gives us dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n\nThe default colour palette uses by the heatmaply package is “viridis“. However, users can use other colour palettes to improve the aesthetics and visual friendliness of the heatmap.\nIn the plot below, the “Blues” colour palette of rColorBrewer is used.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n\nBesides providing a wide collection of arguments for meeting the statistical analysis needs, the heatmaply package also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the plot below, the following arguments are used:\n\n“k_row” is used to produce 5 groups.\n“margins” is used to change the top margin to 60 and row margin to 200.\n“fontsizw_row” and “fontsize_col” are used to change the font size for row and column labels to 4.\n“main” argument is used to write the main title of the plot.\n“xlab” and “ylab” are used to given the x-axis and y-axis labels respectively.\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )\n\n\n\n\n\n~~~ End of Hands-on Exercise 5C ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05A.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05A.html",
    "title": "Hands-on Exercise 5A",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 13 of the R for Visual Analytics book.\nTernary plots are a way of displaying the distribution and variability of three-part compositional data. Its display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersects at the component values of the point.\nIn this hands-on exercise, a ternary plot is created to visualise and analyse the population structure of Singapore. The learning outcomes are:\n\nInstall and launch tidyverse and ggtern packages.\nDerive three new measures using the mutate() function in the dplyr package.\nBuild a static ternary plot using the ggtern() function in the ggtern package.\nBuild an interactive ternary plot using the plot_ly() function in the Plotly R package.\n\n\n\n\n\n\nIn this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nggtern (ggplot extension) for plotting ternary diagrams; and\nplotly for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, plotly)\n\n\nrequire(devtools)\ninstall_version(\"ggtern\", version = \"3.4.1\", repos = \"http://cran.us.r-project.org\")\n\nRcppArmad... (0.12.6.6.1 -&gt; 0.12.8.0.0) [CRAN]\n\nlibrary(ggtern)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, pop_data. It contains data regarding Singapore Residents by Planning Area Subzone, Age Group, Sex and Type of Dwelling, June 2000-2018.\n\npop_data = read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\nThe tibble data frame, pop_data, has 5 columns and 108,126 rows.\n\n\n\nThe mutate() function in the dplyr package is then used to derive three new measures - young, active, and old.\n\nagpop_mutated = pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)\n\n\n\n\n\n\n\nThe ggtern() function in the ggtern package is used to create a simple ternary plot.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggtern(data=agpop_mutated,\n       aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\nThe labels and a colour theme are then added to enhance the plot.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggtern(data=agpop_mutated, \n       aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population Structure, 2018\") +\n  theme_rgbw()\n\n\n\n\n\n\n\nThe plot_ly() function in the plotly package is then used to create an interactive ternary plot.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n# reusable function for creating annotation object\nlabel = function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2)}\n\n# reusable function for axis formatting\naxis = function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10))}\n\nternaryAxes = list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\"))\n\n# Initiating a plotly visualisation \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\") %&gt;%\n  layout(annotations = label(\"Ternary Markers\"), \n         ternary = ternaryAxes)\n\n\n\n\n~~~ End of Hands-on Exercise 5A ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05A.html#overview-and-learning-outcomes",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05A.html#overview-and-learning-outcomes",
    "title": "Hands-on Exercise 5A",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 13 of the R for Visual Analytics book.\nTernary plots are a way of displaying the distribution and variability of three-part compositional data. Its display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersects at the component values of the point.\nIn this hands-on exercise, a ternary plot is created to visualise and analyse the population structure of Singapore. The learning outcomes are:\n\nInstall and launch tidyverse and ggtern packages.\nDerive three new measures using the mutate() function in the dplyr package.\nBuild a static ternary plot using the ggtern() function in the ggtern package.\nBuild an interactive ternary plot using the plot_ly() function in the Plotly R package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05A.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05A.html#getting-started",
    "title": "Hands-on Exercise 5A",
    "section": "",
    "text": "In this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nggtern (ggplot extension) for plotting ternary diagrams; and\nplotly for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, plotly)\n\n\nrequire(devtools)\ninstall_version(\"ggtern\", version = \"3.4.1\", repos = \"http://cran.us.r-project.org\")\n\nRcppArmad... (0.12.6.6.1 -&gt; 0.12.8.0.0) [CRAN]\n\nlibrary(ggtern)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, pop_data. It contains data regarding Singapore Residents by Planning Area Subzone, Age Group, Sex and Type of Dwelling, June 2000-2018.\n\npop_data = read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\nThe tibble data frame, pop_data, has 5 columns and 108,126 rows.\n\n\n\nThe mutate() function in the dplyr package is then used to derive three new measures - young, active, and old.\n\nagpop_mutated = pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05A.html#plotting-ternary-diagram",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05A.html#plotting-ternary-diagram",
    "title": "Hands-on Exercise 5A",
    "section": "",
    "text": "The ggtern() function in the ggtern package is used to create a simple ternary plot.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggtern(data=agpop_mutated,\n       aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\nThe labels and a colour theme are then added to enhance the plot.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggtern(data=agpop_mutated, \n       aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population Structure, 2018\") +\n  theme_rgbw()\n\n\n\n\n\n\n\nThe plot_ly() function in the plotly package is then used to create an interactive ternary plot.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n# reusable function for creating annotation object\nlabel = function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2)}\n\n# reusable function for axis formatting\naxis = function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10))}\n\nternaryAxes = list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\"))\n\n# Initiating a plotly visualisation \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\") %&gt;%\n  layout(annotations = label(\"Ternary Markers\"), \n         ternary = ternaryAxes)\n\n\n\n\n~~~ End of Hands-on Exercise 5A ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04C.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04C.html",
    "title": "Hands-on Exercise 4C",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 11 of the R for Visual Analytics book.\nVisualising uncertainty is relatively new in statistical graphics. In this hands-on exercise, the learning outcomes are:\n\nPlot statistics error bars using ggplot2;\nPlot interactive error bars by combining ggplot2, plotly and DT packages;\nCreate advanced uncertainty visualisations using ggdist; and\nCreate hypothetical outcome plots (HOPs) using the ungeviz package.\n\n\n\n\n\n\nIn this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nDT for interfacing with JavaScript library DataTables that create interactive tables on html pages;\nplotly for plotting interactive statistical graphs;\ngganimate (ggplot extension) for creating animated statistical graphs;\ncrosstalk for inter-widget interactivity for html widgets; and\nggdist for visualising distributions and uncertainty.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\npacman::p_load(tidyverse, DT, plotly,\n               gganimate, crosstalk, ggdist,\n               ungeviz)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, exam.\n\nexam = read_csv(\"data/Exam_data.csv\")\n\nThe tibble data frame, exam_data, has 7 columns and 322 rows.\n\nIt consists of the year-end examination grades of a cohort of 322 Primary 3 students from a local school.\nThe 7 variables/attributes are:\n\nCategorical: ID, CLASS, GENDER, and RACE.\nContinuous: MATHS, ENGLISH, and SCIENCE.\n\n\n\n\n\n\nA point estimate is a single number, e.g., a mean. Uncertainty, on the other hand, is expressed as standard error, confidence interval, or credible interval. Do not confuse the uncertainty of a point estimate with the variation in the sample.\nFirst, the necessary summary statistics, my_sum, regarding Maths scores by race are derived.\n\nmy_sum = exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\nNext, the the tibble data frame, my_sum, is displayed in an html table format.\n\nTableCode\n\n\n\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\n\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\n\n\nThe error bars of mean Maths scores by race are plotted using the geom_errorbar() function in the ggplot2 package.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard Error of Mean Maths Scores by Race\")\n\n\n\n\n\n\n\nThe confidence intervals of mean Maths scores by race are plotted using the geom_errorbar() function in the ggplot2 package.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% Confidence Interval of Mean Maths Scores by Race\")\n\n\n\n\n\n\n\nAn interactive plot of error bars for the 99% confidence interval of mean Maths scores by race is plotted using the ggplotly() function in the plotly package, and the datatable() function in the DT package.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence Interval of Average /&lt;br&gt;Maths scores by Race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of Pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))\n\n\n\n\n\n\n\n\nThe ggdist package that provides a flexible set of ggplot2 geoms and stats functions designed for visualising distributions and uncertainty. It is designed for both frequentist and Bayesian uncertainty visualisation, taking the view that uncertainty visualisation can be unified through the perspective of distribution visualisation:\n\nFor frequentist models, one visualises confidence distributions or bootstrap distributions; and\nFor Bayesian models, one visualises probability distributions.\n\n\n\nThe stat_pointinterval() is used to build a plot for displaying distribution of mean Maths scores by race.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising Confidence Intervals of Mean Maths Scores\",\n    subtitle = \"Mean Point + Multiple-interval Plot\")\n\n\n\n\nThe plot can be changed if the arguments are adjusted:\n\n“.width” = 0.95;\n“.point” = median; and\n“.interval” = qi\n\nThe plot below is for median Maths scores by race.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising Confidence Intervals of Median Maths Scores\",\n    subtitle = \"Median Point + Multiple-interval Plot\")\n\n\n\n\nFurthermore, the first plot (mean Maths scores by race) can be adjusted to show 95% and 99% confidence intervals.\n\nPlot (95% CI)Plot (99% CI)Codes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval(.width = 0.95) +\n  labs(\n    title = \"Visualising Confidence Intervals of Mean Maths Scores\",\n    subtitle = \"Mean Point + Multiple-interval Plot\")\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval(.width = 0.99) +\n  labs(\n    title = \"Visualising Confidence Intervals of Mean Maths Scores\",\n    subtitle = \"Mean Point + Multiple-interval Plot\")\n\n\n\n\n\n\n\nThe stat_gradientinterval() is used to build a plot for displaying distribution of Maths scores by race using gradient colours.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising Confidence Intervals of Mean Maths Scores\",\n    subtitle = \"Gradient + Interval Plot\")\n\n\n\n\n\n\n\n\nAn animated plot the the hypothetical outcome plots is created below.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    size = 0.4, color = \"#0072B2\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), height = 0.6, color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)\n\n\n\n\n~~~ End of Hands-on Exercise 4C ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04C.html#overview-and-learning-outcomes",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04C.html#overview-and-learning-outcomes",
    "title": "Hands-on Exercise 4C",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 11 of the R for Visual Analytics book.\nVisualising uncertainty is relatively new in statistical graphics. In this hands-on exercise, the learning outcomes are:\n\nPlot statistics error bars using ggplot2;\nPlot interactive error bars by combining ggplot2, plotly and DT packages;\nCreate advanced uncertainty visualisations using ggdist; and\nCreate hypothetical outcome plots (HOPs) using the ungeviz package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04C.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04C.html#getting-started",
    "title": "Hands-on Exercise 4C",
    "section": "",
    "text": "In this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nDT for interfacing with JavaScript library DataTables that create interactive tables on html pages;\nplotly for plotting interactive statistical graphs;\ngganimate (ggplot extension) for creating animated statistical graphs;\ncrosstalk for inter-widget interactivity for html widgets; and\nggdist for visualising distributions and uncertainty.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\npacman::p_load(tidyverse, DT, plotly,\n               gganimate, crosstalk, ggdist,\n               ungeviz)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, exam.\n\nexam = read_csv(\"data/Exam_data.csv\")\n\nThe tibble data frame, exam_data, has 7 columns and 322 rows.\n\nIt consists of the year-end examination grades of a cohort of 322 Primary 3 students from a local school.\nThe 7 variables/attributes are:\n\nCategorical: ID, CLASS, GENDER, and RACE.\nContinuous: MATHS, ENGLISH, and SCIENCE."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04C.html#visualising-the-uncertainty-of-point-estimates-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04C.html#visualising-the-uncertainty-of-point-estimates-ggplot2-methods",
    "title": "Hands-on Exercise 4C",
    "section": "",
    "text": "A point estimate is a single number, e.g., a mean. Uncertainty, on the other hand, is expressed as standard error, confidence interval, or credible interval. Do not confuse the uncertainty of a point estimate with the variation in the sample.\nFirst, the necessary summary statistics, my_sum, regarding Maths scores by race are derived.\n\nmy_sum = exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\nNext, the the tibble data frame, my_sum, is displayed in an html table format.\n\nTableCode\n\n\n\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\n\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\n\n\nThe error bars of mean Maths scores by race are plotted using the geom_errorbar() function in the ggplot2 package.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard Error of Mean Maths Scores by Race\")\n\n\n\n\n\n\n\nThe confidence intervals of mean Maths scores by race are plotted using the geom_errorbar() function in the ggplot2 package.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% Confidence Interval of Mean Maths Scores by Race\")\n\n\n\n\n\n\n\nAn interactive plot of error bars for the 99% confidence interval of mean Maths scores by race is plotted using the ggplotly() function in the plotly package, and the datatable() function in the DT package.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence Interval of Average /&lt;br&gt;Maths scores by Race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of Pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04C.html#visualising-uncertainty-ggdist-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04C.html#visualising-uncertainty-ggdist-methods",
    "title": "Hands-on Exercise 4C",
    "section": "",
    "text": "The ggdist package that provides a flexible set of ggplot2 geoms and stats functions designed for visualising distributions and uncertainty. It is designed for both frequentist and Bayesian uncertainty visualisation, taking the view that uncertainty visualisation can be unified through the perspective of distribution visualisation:\n\nFor frequentist models, one visualises confidence distributions or bootstrap distributions; and\nFor Bayesian models, one visualises probability distributions.\n\n\n\nThe stat_pointinterval() is used to build a plot for displaying distribution of mean Maths scores by race.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising Confidence Intervals of Mean Maths Scores\",\n    subtitle = \"Mean Point + Multiple-interval Plot\")\n\n\n\n\nThe plot can be changed if the arguments are adjusted:\n\n“.width” = 0.95;\n“.point” = median; and\n“.interval” = qi\n\nThe plot below is for median Maths scores by race.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising Confidence Intervals of Median Maths Scores\",\n    subtitle = \"Median Point + Multiple-interval Plot\")\n\n\n\n\nFurthermore, the first plot (mean Maths scores by race) can be adjusted to show 95% and 99% confidence intervals.\n\nPlot (95% CI)Plot (99% CI)Codes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval(.width = 0.95) +\n  labs(\n    title = \"Visualising Confidence Intervals of Mean Maths Scores\",\n    subtitle = \"Mean Point + Multiple-interval Plot\")\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval(.width = 0.99) +\n  labs(\n    title = \"Visualising Confidence Intervals of Mean Maths Scores\",\n    subtitle = \"Mean Point + Multiple-interval Plot\")\n\n\n\n\n\n\n\nThe stat_gradientinterval() is used to build a plot for displaying distribution of Maths scores by race using gradient colours.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising Confidence Intervals of Mean Maths Scores\",\n    subtitle = \"Gradient + Interval Plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04C.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04C.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "title": "Hands-on Exercise 4C",
    "section": "",
    "text": "An animated plot the the hypothetical outcome plots is created below.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    size = 0.4, color = \"#0072B2\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), height = 0.6, color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)\n\n\n\n\n~~~ End of Hands-on Exercise 4C ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04A.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04A.html",
    "title": "Hands-on Exercise 4A",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 9 of the R for Visual Analytics book.\nVisualising distribution is not new in statistical analysis. Chapter 1 covered some of the popular statistical graphic methods for visualising distribution. They are histogram, probability density curve (pdf), boxplot, notch plot, and violin plot, using the ggplot2 package. Chapter 9 will cover two relatively new statistical graphic methods for visualising distribution. They are ridgeline plot and raincloud plot using the ggplot2 package and its extensions.\n\n\n\n\n\nIn this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nggthemes for extra themes, geoms, and scales for ggplot2;\nggridges for creating ridgeline plots; and\nggdist for visualising distributions and uncertainty.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, ggthemes,\n               ggridges, ggdist)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, exam.\n\nexam = read_csv(\"data/Exam_data.csv\")\n\nThe tibble data frame, exam_data, has 7 columns and 322 rows.\n\nIt consists of the year-end examination grades of a cohort of 322 Primary 3 students from a local school.\nThe 7 variables/attributes are:\n\nCategorical: ID, CLASS, GENDER, and RACE.\nContinuous: MATHS, ENGLISH, and SCIENCE.\n\n\n\n\n\n\nA ridgeline plot (also called joyplot) is a data visualisation technique for revealing the distribution of a numeric value for several groups. Distribution can be represented using histograms or density plots, all aligned to the same horizontal scale and presented with a slight overlap.\nSuch as plot makes sense when the number of groups to represent is medium to high, and thus, a classic window separation would take up too much space. The overlap of the groups allows for a more efficient use of space.\nAlso, it works well when there is a clear pattern in the result, like if there is an obvious ranking in groups. Otherwise, the groups will tend to overlap each other, leading to a messy plot that does not provide any insight.\n\n\nThere are several ways to plot a ridgeline plot with R. The ggridges package has two main geom functions to plot a ridgeline plot: geom_ridgeline() and geom_density_ridges(). The former takes height values directly to draw the ridgelines, and the latter first estimates data densities and then draws those using ridgelines.\nIn the plot below, a ridgeline plot below is plotted using the geom_density_ridges() function.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = \"#7097BB\",\n    alpha = 0.8,\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\nThe area under a ridgeline plot can be filled with colours that vary along the x axis. This effect can be achieved by using either the geom_ridgeline_gradient() or geom_density_ridges_gradient() functions. Both geom functions work just like geom_ridgeline() and geom_density_ridges(), except that they allow for varying fill colors. However, they do not allow for the alpha argument for transparency in the fill.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Score\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\nBeside providing additional geom objects to support the need to plot a ridgeline plot, the ggridges package also provides a statistic function, stat_density_ridges(), that replaces the stat_density() function in the ggplot2 package.\nThe plot below uses the stat(ecdf) function to calculate the probabilities representing the empirical cumulative function for the distribution of scores.\n\nNote: It is important to set the argument “calc_ecdf” as TRUE in the stat_density_ridges() function.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\nA ridgeline plot that assigns different colours to different quantiles can be plotted using the geom_density_ridges_gradient() function, and stating the the stat(quantile) as an aesthetic argument.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\nInstead of using a number to define the number of quantiles, cut-off points (e.g., 2.5% and 97.5% tails) can also be used to colour a ridgeline plot.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()\n\n\n\n\n\n\n\n\nA raincloud plot is a data visualisation technique that produces a half-density of a distribution plot. It gets the name because the density plot is in the shape of a “raincloud”. The raincloud (half-density) plot enhances the traditional box-plot by highlighting the presence of multiple modalities (an indicator that different groups may exist). A boxplot does not show where densities are clustered, but a raincloud plot does.\nThe functions from ggdist and ggplot2 packages are used to create a raincloud plot.\n\n\nFirst, a half-eye plot is plotted using the stat_halfeye() function of the ggdist package.\n\nNote: The slab interval can be removed by setting the “.width” argument to 0 and “point_colour” argument to NA.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\nSecond, the geom_boxplot() function in the ggplot2 package is used to add a narrow boxplot. The width and opacity are adjusted accordingly.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)\n\n\n\n\n\n\n\nThird, the stat_dots() function in the ggdist package is used to produce a half-dotplot, which is similar to a histogram that indicates the number of samples (number of dots) in each bin.\nThe “side” argument is set to “left” to place it on the left-hand side.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\n\nLast, the coord_flip() function in the ggplot2 package is used to flip the raincloud plot horizontally to give it the raincloud appearance.\nThe theme_economist() function in the ggthemes package is added to give the raincloud chart a standard professional publishing appearance.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()\n\n\n\n\n~~~ End of Hands-on Exercise 4A ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04A.html#overview-and-learning-outcomes",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04A.html#overview-and-learning-outcomes",
    "title": "Hands-on Exercise 4A",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 9 of the R for Visual Analytics book.\nVisualising distribution is not new in statistical analysis. Chapter 1 covered some of the popular statistical graphic methods for visualising distribution. They are histogram, probability density curve (pdf), boxplot, notch plot, and violin plot, using the ggplot2 package. Chapter 9 will cover two relatively new statistical graphic methods for visualising distribution. They are ridgeline plot and raincloud plot using the ggplot2 package and its extensions."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04A.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04A.html#getting-started",
    "title": "Hands-on Exercise 4A",
    "section": "",
    "text": "In this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nggthemes for extra themes, geoms, and scales for ggplot2;\nggridges for creating ridgeline plots; and\nggdist for visualising distributions and uncertainty.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, ggthemes,\n               ggridges, ggdist)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, exam.\n\nexam = read_csv(\"data/Exam_data.csv\")\n\nThe tibble data frame, exam_data, has 7 columns and 322 rows.\n\nIt consists of the year-end examination grades of a cohort of 322 Primary 3 students from a local school.\nThe 7 variables/attributes are:\n\nCategorical: ID, CLASS, GENDER, and RACE.\nContinuous: MATHS, ENGLISH, and SCIENCE."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04A.html#visualising-distribution-with-ridgeline-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04A.html#visualising-distribution-with-ridgeline-plot",
    "title": "Hands-on Exercise 4A",
    "section": "",
    "text": "A ridgeline plot (also called joyplot) is a data visualisation technique for revealing the distribution of a numeric value for several groups. Distribution can be represented using histograms or density plots, all aligned to the same horizontal scale and presented with a slight overlap.\nSuch as plot makes sense when the number of groups to represent is medium to high, and thus, a classic window separation would take up too much space. The overlap of the groups allows for a more efficient use of space.\nAlso, it works well when there is a clear pattern in the result, like if there is an obvious ranking in groups. Otherwise, the groups will tend to overlap each other, leading to a messy plot that does not provide any insight.\n\n\nThere are several ways to plot a ridgeline plot with R. The ggridges package has two main geom functions to plot a ridgeline plot: geom_ridgeline() and geom_density_ridges(). The former takes height values directly to draw the ridgelines, and the latter first estimates data densities and then draws those using ridgelines.\nIn the plot below, a ridgeline plot below is plotted using the geom_density_ridges() function.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = \"#7097BB\",\n    alpha = 0.8,\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\nThe area under a ridgeline plot can be filled with colours that vary along the x axis. This effect can be achieved by using either the geom_ridgeline_gradient() or geom_density_ridges_gradient() functions. Both geom functions work just like geom_ridgeline() and geom_density_ridges(), except that they allow for varying fill colors. However, they do not allow for the alpha argument for transparency in the fill.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Score\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\nBeside providing additional geom objects to support the need to plot a ridgeline plot, the ggridges package also provides a statistic function, stat_density_ridges(), that replaces the stat_density() function in the ggplot2 package.\nThe plot below uses the stat(ecdf) function to calculate the probabilities representing the empirical cumulative function for the distribution of scores.\n\nNote: It is important to set the argument “calc_ecdf” as TRUE in the stat_density_ridges() function.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\nA ridgeline plot that assigns different colours to different quantiles can be plotted using the geom_density_ridges_gradient() function, and stating the the stat(quantile) as an aesthetic argument.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\nInstead of using a number to define the number of quantiles, cut-off points (e.g., 2.5% and 97.5% tails) can also be used to colour a ridgeline plot.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04A.html#visualising-distribution-with-raincloud-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04A.html#visualising-distribution-with-raincloud-plot",
    "title": "Hands-on Exercise 4A",
    "section": "",
    "text": "A raincloud plot is a data visualisation technique that produces a half-density of a distribution plot. It gets the name because the density plot is in the shape of a “raincloud”. The raincloud (half-density) plot enhances the traditional box-plot by highlighting the presence of multiple modalities (an indicator that different groups may exist). A boxplot does not show where densities are clustered, but a raincloud plot does.\nThe functions from ggdist and ggplot2 packages are used to create a raincloud plot.\n\n\nFirst, a half-eye plot is plotted using the stat_halfeye() function of the ggdist package.\n\nNote: The slab interval can be removed by setting the “.width” argument to 0 and “point_colour” argument to NA.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\nSecond, the geom_boxplot() function in the ggplot2 package is used to add a narrow boxplot. The width and opacity are adjusted accordingly.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)\n\n\n\n\n\n\n\nThird, the stat_dots() function in the ggdist package is used to produce a half-dotplot, which is similar to a histogram that indicates the number of samples (number of dots) in each bin.\nThe “side” argument is set to “left” to place it on the left-hand side.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\n\nLast, the coord_flip() function in the ggplot2 package is used to flip the raincloud plot horizontally to give it the raincloud appearance.\nThe theme_economist() function in the ggthemes package is added to give the raincloud chart a standard professional publishing appearance.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()\n\n\n\n\n~~~ End of Hands-on Exercise 4A ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03A.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03A.html",
    "title": "Hands-on Exercise 3A",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 3 of the R for Visual Analytics book.\nThe learning outcome is to create interactive data visualisation using functions in the ggiraph and plotlyr package.\n\n\n\n\n\nIn this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\npatchwork for preparing composite figures created using ggplot2;\nDT for interfacing with JavaScript library DataTables that create interactive tables on html pages;\nggiraph for making ggplot graphics interactive;\nplotly for plotting interactive statistical graphs; and\ncrosstalk for inter-widget interactivity for html widgets.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, patchwork,\n               DT, ggiraph, plotly,\n               crosstalk)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, exam_data.\n\nexam_data = read_csv(\"data/Exam_data.csv\")\n\nThe tibble data frame, exam_data, has 7 columns and 322 rows.\n\nIt consists of the year-end examination grades of a cohort of 322 Primary 3 students from a local school.\nThe 7 variables/attributes are:\n\nCategorical: ID, CLASS, GENDER, and RACE.\nContinuous: MATHS, ENGLISH, and SCIENCE.\n\n\n\n\n\n\nThe ggiraph package is an html widget and a ggplot2 extension that allows ggplot graphics to be interactive. This is achieved using the interactive geometries that can understand three arguments:\n\ntooltip: a column of datasets that contain tooltips to be displayed when the mouse is pointing to the elements.;\nonclick: a column of datasets that contain a JavaScript function to be executed when the elements are clicked on; and\ndata_id: a column of datasets that contain an ID to be associated with the elements.\n\nIf used within a Shiny application, the elements associated with an ID (data_id) can be selected and manipulated on the client and server sides.\n\n\nA typical code chunk to plot an interactive statistical graph using functions in the ggiraph package consists of two parts:\n\nAn interactive version of a ggplot object is created using the geom_dotplot_interactive() function; and\nThe girafe() function is then used to generate an interactive svg object to be displayed on the html page.\n\nThe “tooltip” aesthetic argument of the geom_dotplot_interactive() function is used to specify the field that will be displayed in the tooltip.\nIn the plot below, when the mouse pointer hovers over a data point of interest, the student’s ID is displayed.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\np1 = ggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\n\ngirafe(ggobj = p1,\n       width_svg = 6,\n       height_svg = 6*0.618)\n\n\n\n\n\n\n\nThe content of the tooltip can be customised by including a list object. A new field, tooltip, was created in the tibble data frame, exam_data. It is populated with information from the ID and CLASS fields. This tooltip is then used in place of ID in the “tooltip” aesthetic argument of the geom_dotplot_interactive() function.\nWhen the mouse pointer hovers over a data point of interest, the student’s ID and class are displayed.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nexam_data$tooltip = c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np2 = ggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(ggobj = p2,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\nThe opts_tooltip() function in the ggiraph package is used to customise the tooltip rendering by adding css declarations.\n\nNote: The background for the tooltip has been changed from black to white colour, and the text colour has been changed from white to black.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\ntooltip_css = \"background-color:white; #&lt;&lt;\nfont-style:bold; color:black;\" #&lt;&lt;\n\ngirafe(ggobj = p2,\n       width_svg = 6,\n       height_svg = 6*0.618,\n       options = list(    #&lt;&lt;\n         opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n      )\n\n\n\n\n\n\n\nDerived statistics can also be displayed in a tooltip. In the example below, a function is used to compute the 90% confidence interval of the mean of Maths scores by RACE are plotted in a bar chart.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\ntooltip = function(y, ymax, accuracy = .01) {\n  mean = scales::number(y, accuracy = accuracy)\n  sem = scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean Maths Scores:\", mean, \"+/-\", sem)\n}\n\ngg_point = ggplot(data = exam_data, \n                   aes(x = RACE)) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\") +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2)\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\nThe “data_id” aesthetic argument of the geom_dotplot_interactive() function is used to show associated elements of the same designated field.\nIn the plot below, elements of the same CLASS are highlighted when the mouse hovers over any one of them.\n\nNote: [In-class Exercise (Week 4)] The inclusion of “tooltip = CLASS” in the “aes” argument of the geom_dotplot_interactive() function allows the student’s class to be displayed at the tooltip when the mouse hovers over it.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\np3 = ggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(tooltip = CLASS, \n        data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(ggobj = p3,\n       width_svg = 6,\n       height_svg = 6*0.618)\n\n\n\n\n\n\n\nThe highlighting effect can be changed using css codes to show the selected associated elements and fade the non-selected elements.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\ngirafe(ggobj = p3,\n       width_svg = 6,\n       height_svg = 6*0.618,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")))\n\n\n\n\n\n\n\nThe tooltip and hover effect can be combined in an interactive statistical graph.\nThe associated elements are highlighted when the mouse hovers over one of them. At the same time, the tooltip will show which CLASS the highlighted elements belong to.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\np4 = ggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(ggobj = p4,\n       width_svg = 6,\n       height_svg = 6*0.618,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")))\n\n\n\n\n\n\n\nFinally, the “onclick” aesthetic argument of the geom_dotplot_interactive() function is used to provide hotlink interactivity on the web.\nUpon clicking one of the elements, the web document link will open.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nexam_data$onclick = sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np5 = ggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(ggobj = p5,\n       width_svg = 6,\n       height_svg = 6*0.618)\n\n\n\n\n\n\n\nThe coordinated multiple views methods can be used to show corresponding data points based on the same ID, with the following steps:\n\nAppropriate interactive functions of the ggiraph package is used to create the multiple views.\nThe patchwork package is used inside the girafe() function to create the interactive coordinated multiple views.\n\n\nNote: The “data_id” aesthetic argument is critical to link observations between plots and the”tooltip” aesthetic argument is optional but nice to have when the mouse hovers over a point.\n\n\nNote: [In-class Exercise (Week 4)] The same approach used at sub-section 3.3.2 for displaying multiple information on the tooltip is applied here. The inclusion of “tooltip = exam_data$tooltip2” in the “aes” argument of the geom_dotplot_interactive() function allows the student’s ID, class, Maths and English scores to be displayed at the tooltip when the mouse hovers over it.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nexam_data$tooltip2 = c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS,\n  \"\\n Maths Score = \", exam_data$MATHS,\n  \"\\n English Score = \", exam_data$ENGLISH)) \n\np6 = ggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID,\n        tooltip = exam_data$tooltip2),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim = c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np7 = ggplot(data = exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID,\n        tooltip = exam_data$tooltip2),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim = c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p6 + p7), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")))\n\n\n\n\n\n\n\n\nThe plotly package can be used to create interactive web graphics from ggplot2 graphs and/or a custom interface to the (MIT-licensed) JavaScript library plotly.js inspired by the Grammar of Graphics. Unlike other plotly platforms, plot.R is free and open-source.\nThere are two ways to create an interactive graph using the plotly package:\n\nUsing the plot_ly() function; and\nUsing the ggplotly() function.\n\n\n\nA basic interactive plot is created using the plot_ly() function.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\n\n\n\n\n\n\nThe “colour” argument is used to map a qualitative visual variable (e.g. RACE).\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE)\n\n\n\n\n\n\n\nA basic interactive plot is created using the gglotly() function.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\np8 = ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size = 1) +\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100))\n\nggplotly(p8)\n\n\n\n\n\n\n\nA coordinated linked plot can be created using the plotly package function in three steps:\n\nThe highlight_key() function in the plotly package is used as shared data.\nThe two scatterplots are created by using functions in the ggplot2 package.\nThe subplot() function in the plotly package is used to place the two scatterplots side-by-side.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nd = highlight_key(exam_data)\np9 = ggplot(data = d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size = 1) +\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100))\n\np10 = ggplot(data = d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size = 1) +\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100))\n\nsubplot(ggplotly(p9),\n        ggplotly(p10))\n\n\n\n\n\n\n\n\nThe DT package provides interfacing with JavaScript library DataTables that create interactive tables on html page. Data objects in R can be rendered as HTML tables using the JavaScript library DataTables (typically via R Markdown or Shiny).\n\ndatatable(exam_data, class= \"compact\")\n\n\n\n\n\n\n\n\n\n\n\nCrosstalk is an add-on to the htmlwidgets package. It extends the package with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\nCoordinated brushing is implemented using:\n\nThe highlight() function in the plotly package sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nThe bscols() function in the crosstalk package makes it easy to put html elements side by side. It is especially designed to work in an R Markdown document.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np11 = ggplot(d,\n             aes(ENGLISH, MATHS)) + \n  geom_point(size = 1) +\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100))\n\ngg = highlight(ggplotly(p11),\n               \"plotly_selected\")  \n\nbscols(gg, \n       datatable(d), \n       widths = 5)\n\n\n\n\n\n\n\n\n\n\n\nggiraph and example.\nHow to Plot With ggiraph.\nInteractive map of France with ggiraph.\nCustom Interactive Sunbursts with ggplot in R.\n\n\n\n\n\nGetting Started with plotly in R\nA collection of plotly R graphs.\nCarson Sievert (2020) Interactive Web-based Data Visualization with R, plotly, and Shiny, Chapman and Hall/CRC. Online version.\nPlotly R Figure Reference provides a comprehensive discussion of each visual representations.\nPlotly R Library Fundamentals is a good place to learn the fundamental features of plotly’s R API.\ngganimate and example.\nBuilding An Animation Step-by-Step with gganimate.\nCreating A Composite gif with Multiple gganimate Panels.\n\n~~~ End of Hands-on Exercise 3A ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03A.html#overview-and-learning-outcomes",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03A.html#overview-and-learning-outcomes",
    "title": "Hands-on Exercise 3A",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 3 of the R for Visual Analytics book.\nThe learning outcome is to create interactive data visualisation using functions in the ggiraph and plotlyr package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03A.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03A.html#getting-started",
    "title": "Hands-on Exercise 3A",
    "section": "",
    "text": "In this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\npatchwork for preparing composite figures created using ggplot2;\nDT for interfacing with JavaScript library DataTables that create interactive tables on html pages;\nggiraph for making ggplot graphics interactive;\nplotly for plotting interactive statistical graphs; and\ncrosstalk for inter-widget interactivity for html widgets.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, patchwork,\n               DT, ggiraph, plotly,\n               crosstalk)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, exam_data.\n\nexam_data = read_csv(\"data/Exam_data.csv\")\n\nThe tibble data frame, exam_data, has 7 columns and 322 rows.\n\nIt consists of the year-end examination grades of a cohort of 322 Primary 3 students from a local school.\nThe 7 variables/attributes are:\n\nCategorical: ID, CLASS, GENDER, and RACE.\nContinuous: MATHS, ENGLISH, and SCIENCE."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03A.html#interactive-data-visualisation-ggiraph-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03A.html#interactive-data-visualisation-ggiraph-methods",
    "title": "Hands-on Exercise 3A",
    "section": "",
    "text": "The ggiraph package is an html widget and a ggplot2 extension that allows ggplot graphics to be interactive. This is achieved using the interactive geometries that can understand three arguments:\n\ntooltip: a column of datasets that contain tooltips to be displayed when the mouse is pointing to the elements.;\nonclick: a column of datasets that contain a JavaScript function to be executed when the elements are clicked on; and\ndata_id: a column of datasets that contain an ID to be associated with the elements.\n\nIf used within a Shiny application, the elements associated with an ID (data_id) can be selected and manipulated on the client and server sides.\n\n\nA typical code chunk to plot an interactive statistical graph using functions in the ggiraph package consists of two parts:\n\nAn interactive version of a ggplot object is created using the geom_dotplot_interactive() function; and\nThe girafe() function is then used to generate an interactive svg object to be displayed on the html page.\n\nThe “tooltip” aesthetic argument of the geom_dotplot_interactive() function is used to specify the field that will be displayed in the tooltip.\nIn the plot below, when the mouse pointer hovers over a data point of interest, the student’s ID is displayed.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\np1 = ggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\n\ngirafe(ggobj = p1,\n       width_svg = 6,\n       height_svg = 6*0.618)\n\n\n\n\n\n\n\nThe content of the tooltip can be customised by including a list object. A new field, tooltip, was created in the tibble data frame, exam_data. It is populated with information from the ID and CLASS fields. This tooltip is then used in place of ID in the “tooltip” aesthetic argument of the geom_dotplot_interactive() function.\nWhen the mouse pointer hovers over a data point of interest, the student’s ID and class are displayed.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nexam_data$tooltip = c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np2 = ggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(ggobj = p2,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\nThe opts_tooltip() function in the ggiraph package is used to customise the tooltip rendering by adding css declarations.\n\nNote: The background for the tooltip has been changed from black to white colour, and the text colour has been changed from white to black.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\ntooltip_css = \"background-color:white; #&lt;&lt;\nfont-style:bold; color:black;\" #&lt;&lt;\n\ngirafe(ggobj = p2,\n       width_svg = 6,\n       height_svg = 6*0.618,\n       options = list(    #&lt;&lt;\n         opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n      )\n\n\n\n\n\n\n\nDerived statistics can also be displayed in a tooltip. In the example below, a function is used to compute the 90% confidence interval of the mean of Maths scores by RACE are plotted in a bar chart.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\ntooltip = function(y, ymax, accuracy = .01) {\n  mean = scales::number(y, accuracy = accuracy)\n  sem = scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean Maths Scores:\", mean, \"+/-\", sem)\n}\n\ngg_point = ggplot(data = exam_data, \n                   aes(x = RACE)) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\") +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2)\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\nThe “data_id” aesthetic argument of the geom_dotplot_interactive() function is used to show associated elements of the same designated field.\nIn the plot below, elements of the same CLASS are highlighted when the mouse hovers over any one of them.\n\nNote: [In-class Exercise (Week 4)] The inclusion of “tooltip = CLASS” in the “aes” argument of the geom_dotplot_interactive() function allows the student’s class to be displayed at the tooltip when the mouse hovers over it.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\np3 = ggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(tooltip = CLASS, \n        data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(ggobj = p3,\n       width_svg = 6,\n       height_svg = 6*0.618)\n\n\n\n\n\n\n\nThe highlighting effect can be changed using css codes to show the selected associated elements and fade the non-selected elements.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\ngirafe(ggobj = p3,\n       width_svg = 6,\n       height_svg = 6*0.618,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")))\n\n\n\n\n\n\n\nThe tooltip and hover effect can be combined in an interactive statistical graph.\nThe associated elements are highlighted when the mouse hovers over one of them. At the same time, the tooltip will show which CLASS the highlighted elements belong to.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\np4 = ggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(ggobj = p4,\n       width_svg = 6,\n       height_svg = 6*0.618,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")))\n\n\n\n\n\n\n\nFinally, the “onclick” aesthetic argument of the geom_dotplot_interactive() function is used to provide hotlink interactivity on the web.\nUpon clicking one of the elements, the web document link will open.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nexam_data$onclick = sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np5 = ggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(ggobj = p5,\n       width_svg = 6,\n       height_svg = 6*0.618)\n\n\n\n\n\n\n\nThe coordinated multiple views methods can be used to show corresponding data points based on the same ID, with the following steps:\n\nAppropriate interactive functions of the ggiraph package is used to create the multiple views.\nThe patchwork package is used inside the girafe() function to create the interactive coordinated multiple views.\n\n\nNote: The “data_id” aesthetic argument is critical to link observations between plots and the”tooltip” aesthetic argument is optional but nice to have when the mouse hovers over a point.\n\n\nNote: [In-class Exercise (Week 4)] The same approach used at sub-section 3.3.2 for displaying multiple information on the tooltip is applied here. The inclusion of “tooltip = exam_data$tooltip2” in the “aes” argument of the geom_dotplot_interactive() function allows the student’s ID, class, Maths and English scores to be displayed at the tooltip when the mouse hovers over it.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nexam_data$tooltip2 = c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS,\n  \"\\n Maths Score = \", exam_data$MATHS,\n  \"\\n English Score = \", exam_data$ENGLISH)) \n\np6 = ggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID,\n        tooltip = exam_data$tooltip2),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim = c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np7 = ggplot(data = exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID,\n        tooltip = exam_data$tooltip2),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim = c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p6 + p7), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03A.html#interactive-data-visualisation-plotly-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03A.html#interactive-data-visualisation-plotly-methods",
    "title": "Hands-on Exercise 3A",
    "section": "",
    "text": "The plotly package can be used to create interactive web graphics from ggplot2 graphs and/or a custom interface to the (MIT-licensed) JavaScript library plotly.js inspired by the Grammar of Graphics. Unlike other plotly platforms, plot.R is free and open-source.\nThere are two ways to create an interactive graph using the plotly package:\n\nUsing the plot_ly() function; and\nUsing the ggplotly() function.\n\n\n\nA basic interactive plot is created using the plot_ly() function.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\n\n\n\n\n\n\nThe “colour” argument is used to map a qualitative visual variable (e.g. RACE).\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE)\n\n\n\n\n\n\n\nA basic interactive plot is created using the gglotly() function.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\np8 = ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size = 1) +\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100))\n\nggplotly(p8)\n\n\n\n\n\n\n\nA coordinated linked plot can be created using the plotly package function in three steps:\n\nThe highlight_key() function in the plotly package is used as shared data.\nThe two scatterplots are created by using functions in the ggplot2 package.\nThe subplot() function in the plotly package is used to place the two scatterplots side-by-side.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nd = highlight_key(exam_data)\np9 = ggplot(data = d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size = 1) +\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100))\n\np10 = ggplot(data = d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size = 1) +\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100))\n\nsubplot(ggplotly(p9),\n        ggplotly(p10))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03A.html#interactive-data-visualisation-interactive-data-table-using-dt-package",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03A.html#interactive-data-visualisation-interactive-data-table-using-dt-package",
    "title": "Hands-on Exercise 3A",
    "section": "",
    "text": "The DT package provides interfacing with JavaScript library DataTables that create interactive tables on html page. Data objects in R can be rendered as HTML tables using the JavaScript library DataTables (typically via R Markdown or Shiny).\n\ndatatable(exam_data, class= \"compact\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03A.html#interactive-data-visualisation-crosstalk-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03A.html#interactive-data-visualisation-crosstalk-methods",
    "title": "Hands-on Exercise 3A",
    "section": "",
    "text": "Crosstalk is an add-on to the htmlwidgets package. It extends the package with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\nCoordinated brushing is implemented using:\n\nThe highlight() function in the plotly package sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nThe bscols() function in the crosstalk package makes it easy to put html elements side by side. It is especially designed to work in an R Markdown document.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np11 = ggplot(d,\n             aes(ENGLISH, MATHS)) + \n  geom_point(size = 1) +\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100))\n\ngg = highlight(ggplotly(p11),\n               \"plotly_selected\")  \n\nbscols(gg, \n       datatable(d), \n       widths = 5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03A.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03A.html#references",
    "title": "Hands-on Exercise 3A",
    "section": "",
    "text": "ggiraph and example.\nHow to Plot With ggiraph.\nInteractive map of France with ggiraph.\nCustom Interactive Sunbursts with ggplot in R.\n\n\n\n\n\nGetting Started with plotly in R\nA collection of plotly R graphs.\nCarson Sievert (2020) Interactive Web-based Data Visualization with R, plotly, and Shiny, Chapman and Hall/CRC. Online version.\nPlotly R Figure Reference provides a comprehensive discussion of each visual representations.\nPlotly R Library Fundamentals is a good place to learn the fundamental features of plotly’s R API.\ngganimate and example.\nBuilding An Animation Step-by-Step with gganimate.\nCreating A Composite gif with Multiple gganimate Panels.\n\n~~~ End of Hands-on Exercise 3A ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 1 of the R for Visual Analytics book.\nThe learning outcomes are:\n\nLearn the basic principles and essential components of ggplot2;\nGain hands-on experience on using ggplot2 components to plot statistical graphics based on the principle of Layered Grammar of Graphics; and\nApply the essential graphical elements provided by ggplot2 to create elegant yet functional statistical graphics.\n\n\n\n\n\n\nThe code chunk below uses the p_load() function in the pacman package to check if the tidyverse packages are installed in the computer. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, exam_data.\n\nexam_data = read_csv(\"data/Exam_data.csv\")\n\nThe tibble data frame, exam_data, has 7 columns and 322 rows.\n\nIt consists of the year-end examination grades of a cohort of 322 Primary 3 students from a local school.\nThe 7 variables/attributes are:\n\nCategorical: ID, CLASS, GENDER, and RACE.\nContinuous: MATHS, ENGLISH, and SCIENCE.\n\n\n\n\n\n\nThe ggplot2 package is an R package for declaratively creating data-driven graphics based on The Grammar of Graphics.\nIt is part of the tidyverse family of packages, and is specially designed for visual exploration and communication.\n\n\nR Graphics is the core graphical functions in Base R. A comparison is made between R Graphics and ggplot by plotting a simple histogram.\n\nR Graphicsggplot2\n\n\n\nhist(exam_data$MATHS)\n\n\n\n\n\n\n\nggplot(data=exam_data, aes(x = MATHS)) +\n  geom_histogram(bins=10, \n                 boundary = 100,\n                 color=\"black\", \n                 fill=\"grey\") +\n  ggtitle(\"Distribution of Maths scores\")\n\n\n\n\n\n\n\nIn the above example, a relatively simple code chunk is used for R Graphics, while more details are added via ggplot2.\nSo why use ggplot2? According to the creator of tidyverse, Hadley Wickham:\n\n“The transferable skills from ggplot2 are not the idiosyncrasies of plotting syntax, but a powerful way of thinking about visualisation, as a way of mapping between variables and the visual properties of geometric objects that you can perceive.”\n\n\n\n\n\nIt is important to understand the principles of the Grammar of Graphics.\nThe Grammar of Graphics is a general scheme for data visualisation which breaks up graphs into semantic components such as scales and layers. It was introduced by Leland Wilkinson (1999) through the book - The Grammar of Graphics, published by Springer. It is an answer to a question:\n\nWhat is a statistical graphic?\n\nIn the nutshell, the Grammar of Graphics defines the rules of structuring mathematical and aesthetic elements into a meaningful graph.\nThere are two principles in the Grammar of Graphics:\n\nGraphics is made up of distinct layers of grammatical elements.\nMeaningful plots is achieved through aesthetic mapping.\n\nSo why is it important to have a good grasp of the grammar of graphics?\n\nAllows us to gain insights into the composition of complicated graphics, and reveal unexpected connections between seemingly different graphics (Cox, 1978).\nProvides a strong foundation for understanding a diverse range of graphics.\nImpart better understanding of what well-formed or correct graphics could look like (although there will still be many grammatically correct but nonsensical graphics).\n\n\n\nThe seven grammars of ggplot2 are:\n\nData: The dataset being plotted.\nAesthetics take attributes of the data and use them to influence visual characteristics, e.g., position, colours, size, shape, transparency.\nGeometrics: The visual elements used for the data, e.g., point, bar, line.\nFacets split the data into subsets to create multiple variations of the same graph, e.g., paneling, multiple plots.\nStatistics refer to statistical transformations that summarise data e.g., mean, confidence intervals.\nCoordinates refer to coordinate systems that define the plane on which data are mapped on the graphic.\nThemes modify all non-data components of a plot, e.g., main title, sub-title, y-aixs title, legend, background.\n\n\n\n\n\nThe ggplot() function in the ggplot2 package is used to initiate a ggplot object. The “data” argument defines the dataset to be used for plotting the graphic.\nIf the dataset is not already a data frame, it will be converted to one via the fortify() function in the ggplot2 package.\n\nggplot(data=exam_data)\n\n\n\n\n\n\n\nAesthetic mappings take attributes of the data and and use them to influence visual characteristics, e.g., position, colour, size, shape, or transparency. Each visual characteristic can thus encode an aspect of the data and be used to convey information.\nAll aesthetics of a plot are specified in the aes() function. Each geom layer can have its own aesthetic specification.\nThe code chunk below adds the aesthetic element into the plot. The x-axis and its label are added to the plot.\n\nggplot(data = exam_data, \n       aes(x = MATHS))\n\n\n\n\n\n\n\nThe geometric objects are the actual marks put on the plot. A plot must have at least one geom, but may have more.\nSome examples of geom:\n\ngeom_point() for drawing individual points (e.g., a scatter plot);\ngeom_line() for drawing lines (e.g., for a line chart);\ngeom_smooth() for drawing smoothed lines (e.g., for simple trends or approximations);\ngeom_bar() for drawing bars (e.g., for bar chart);\ngeom_histogram() for drawing binned values (e.g. a histogram);\ngeom_polygon() for drawing arbitrary shapes; and\ngeom_map() for drawing polygons in the shape of a map. Can use the map_data() function to access the data used for these maps.\n\n\n\nA bar chat is plotted using the geom_bar() function.\n\nggplot(data = exam_data, \n       aes(x = RACE)) +\n  geom_bar()\n\n\n\n\n\nObservation: From the bar chart, we can see that most of the students are Chinese, followed by Malay. A minority of students are Indian or other races.\n\n\n\n\nIn a dot plot, the width of a dot corresponds to the bin width (or maximum width, depending on the binning algorithm), and the dots are stacked, with each dot representing one observation.\nA dot plot is plotted using the geom_dotplot() function.\n\nggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(dotsize = 0.5)\n\n\n\n\nThe y-axis for the plot above is misleading. Hence, the scale_y_continuous() function in the ggplot2 package is used to turn off the y-axis. Also, the “binwidth” argument is used to change the bin width to 2.5.\n\nggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(binwidth=2.5,\n               dotsize = 0.5) +\n  scale_y_continuous(NULL,\n                     breaks = NULL)\n\n\n\n\n\nObservation: From the dot plot, we can see that the maths scores are left-skewed, with more students scoring above 50 than below 50.\n\n\n\n\nThe geom_histogram() function is used to create a simple histogram using the values in the “MATHS” attribute of the data set.\n\nggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_histogram()    \n\n\n\n\n\nObservation: From the histogram, we can see that the maths scores are left-skewed, with more students scoring above 50 than below 50.\n\n\n\n\nSome customisations are done to the histogram:\n\nThe “bins” argument is used to change the number of bins from the default 30 to 20;\nThe “fill” argument is used to fill the bars with light blue; and\nThe “colour” argument is used to change the outline of the bars to black.\n\n\nggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_histogram(bins = 20,            \n                 colour = \"black\",      \n                 fill = \"light blue\")\n\n\n\n\n\nObservation: From the histogram, we can see that the maths scores are left-skewed, with more students scoring above 50 than below 50. Also, we can also see more clearly that the score range with the most students is around 75.\n\n\n\n\nThe fill of the histogram is changed using the sub-group of the aesthetic() function, which divides the bars by the “GENDER” attribute.\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           fill = GENDER)) +\n  geom_histogram(bins = 20, \n                 color = \"grey30\")\n\n\n\n\n\nObservation: From the histogram, we can see the distributions of math scores for female and male students. There appears to be less female students who score below 50 compared to male students who score below 50.\n\n\n\n\nThe geom_density() function is used to compute and plot the kernel density estimate, which is a smoothed version of a histogram. It is a useful alternative to the histogram for continuous data that comes from an underlying smooth distribution.\n\nggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_density()           \n\n\n\n\n\nObservation: From the kernel density estimate plot, we can see the most ‘dense’/common score is actually slightly above 75.\n\nThe kernel density estimate is then plotted by the “GENDER” attribute using the “colour” argument.\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           colour = GENDER)) +\n  geom_density()\n\n\n\n\n\nObservation: From the kernel density estimate plot, we can see more clearly that there are more female students than male students who score above 50. Conversely, there are more male students than female students who score below 50.\n\nA similar plot can be produced using the “fill” argument.\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           fill = GENDER,\n           alpha = 0.1)) +\n  geom_density()\n\n\n\n\n\n\n\nThe geom_boxplot() function is used to display a continuous value list. It visualises five summary statistics (minimum, first quartile, median, third quartile, and maximum) and any outliers.\n\nggplot(data = exam_data, \n       aes(y = MATHS,       \n           x= GENDER)) +    \n  geom_boxplot()\n\n\n\n\n\nObservation: From the boxplot, we can see that the median scores for female students and male students are rather similar. There are also outliers for both genders in the lower score range.\n\nThe “notch” argument is used to help visually assess whether the median of distributions differ. If the notches do not overlap, it means that the medians are different.\n\nggplot(data = exam_data, \n       aes(y = MATHS, \n           x = GENDER)) +\n  geom_boxplot(notch = TRUE)\n\n\n\n\n\nObservation: From the notched plot, we can see clearly that the median scores for female students and male students are different.\n\n\n\n\nThe geom_violin() function is designed for creating a violin plot. Violin plots are a way of comparing multiple data distributions. With ordinary density curves, it is difficult to compare more than just a few distributions because the lines visually interfere with each other. With a violin plot, it’s easier to compare several distributions since they are placed side by side.\n\nggplot(data = exam_data, \n       aes(y = MATHS, \n           x = GENDER)) +\n  geom_violin()\n\n\n\n\n\nObservation: From the violin plot, we can see clearly that the distribution of the scores by gender is relatively similarly. A key difference is that the lower scores in the distribution for male students are lower than that for female students.\n\n\n\n\nThe geom_point() function is used to create a scatterplot. The “MATHS” and “ENGLISH” attributes of the dataset are plotted.\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           y = ENGLISH)) +\n  geom_point()            \n\n\n\n\n\nObservation: From the scatterplot, we can see that there appears to be a positive correlation between English and Maths scores, i.e., students with higher English scores also tend to have higher Maths scores.\n\n\n\n\nThe code chunk below plots the data points on boxplots and scatterplots using both the geom_boxplot() and geom_point() functions.\n\nggplot(data = exam_data, \n       aes(y = MATHS, \n           x = GENDER)) +\n  geom_boxplot() +                    \n  geom_point(position = \"jitter\",\n             size = 0.5)\n\n\n\n\n\nNote: The “position” argument is used to randomly vary the location of the dots, which otherwise would have lined up in the middle of the boxplot.\n\n\n\n\n\nThe statistics functions statistically transform data, usually into some form of summary - e.g., mean, confidence limit.\nThere are two ways to use these functions:\n\nAdd a stat() function and override the default geom, or\nAdd a geom() function and override the default stat.\n\n\n\nThe boxplots below are incomplete because the positions of the means were not shown.\n\nggplot(data = exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nThe mean values are added using the stat_summary() function in the ggplot2 package. This overrides the default geom.\n\nggplot(data = exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() +\n  stat_summary(geom = \"point\",       \n               fun = \"mean\",         \n               colour = \"red\",        \n               size = 4)               \n\n\n\n\n\nNote: The “fun” argument is used to insert the mean value as a dot instead of the “fun.y” argument which has been deprecated.\n\n\nObservation: We can see that the mean scores for both gender are below their respective median scores, indicating left-skewed distributions of scores.\n\n\n\n\nThe mean values are added using the geom_point() function. This overrides the default stat.\n\nggplot(data = exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() +\n  geom_point(stat = \"summary\",        \n             fun = \"mean\",           \n             colour = \"red\",          \n             size = 4)          \n\n\n\n\n\nNote: The “fun” argument is used to insert the mean value as a dot instead of the “fun.y” argument which has been deprecated.\n\n\nObservation: Similarly, we can see that the mean scores for both gender are below their respective median scores, indicating left-skewed distributions of scores.\n\n\n\n\nReverting to the scatterplot on the distribution of the “MATHS” and “ENGLISH” attributes (see section 1.7.9), the interpretability of the plot can be improved with the addition of a best fit curve. The geom_smooth() function is used.\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           y = ENGLISH)) +\n  geom_point() +\n  geom_smooth(linewidth = 0.5)\n\n\n\n\n\nNote: The “linewidth” argument is used to indicate the width of the grey shaded region of the best fit curve instead of the “size” argument which has been deprecated.\n\n\nObservation: From the best fit curve, we can see that there appears to be a positive correlation between English and Maths scores, i.e., students with higher English scores also tend to have higher Maths scores. Also, there also appears to be an inflection point around Maths score 75 and English score 70.\n\nThe default method is loess, which draws a smooth curve. If the “method” argument is set to “lm”, the plot produces a linear line based on a linear model.\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           y = ENGLISH)) +\n  geom_point() +\n  geom_smooth(method = lm, \n              linewidth = 0.5)\n\n\n\n\n\nNote: The “linewidth” argument is used to indicate the width of the grey shaded region of the best fit curve instead of the “size” argument which has been deprecated.\n\n\n\n\n\nFacetting generates small multiples (sometimes also called trellis plot), each displaying a different subset of the dataset. They are an alternative to aesthetics for displaying additional discrete variables. The ggplot2 package supports two types of facets, namely: facet_grid() and facet_wrap().\n\n\nThe facet_wrap() function wraps a 1d sequence of panels into 2d. This is generally a better use of screen space than the facet_grid() function because most displays are roughly rectangular.\n\nggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_histogram(bins = 20) +\n    facet_wrap(~ CLASS)\n\n\n\n\n\nObservation: Based on the trellis plot, we can see that there are differences in the Maths scores between different classes. Students from classes 3A-F do better than students from classes 3G-I.\n\n\n\n\nThe facet_grid() function forms a matrix of panels defined by row and column facetting variables. It is most useful when there are two discrete variables, and all combinations of the variables exist in the dataset.\n\nggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_histogram(bins = 20) +\n    facet_grid(~ CLASS)\n\n\n\n\n\nObservation: Based on the trellis plot, we can also see that there are differences in the Maths scores between different classes. Students from classes 3A-F do better than students from classes 3G-I. However, this plot is more difficult to interpret than the one using the facet_wrap() function.\n\n\n\n\n\nThe coordinates functions map the position of objects onto the plane of the plot. There are a number of different possible coordinate systems to use, they are:\n\ncoord_cartesian() - the default cartesian coordinate systems, where x and y values can be specified (e.g., allows zoom in or out).\ncoord_flip() - a cartesian system with the x and y axes flipped.\ncoord_fixed() - a cartesian system with a “fixed” aspect ratio (e.g., 1.78 for a widescreen” plot).\ncoord_quickmap() - a coordinate system that approximates a good aspect ratio for maps.\n\n\n\nBy default, the geom_bar() function produces a bar chart in vertical form. This can be switched using the coord_flip() function.\n\nggplot(data = exam_data, \n       aes(x = RACE)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\nObservation: A plot using horizontal bars may be useful when the labels for the vertical axis are long words.\n\n\n\n\nThe scatterplot in section 1.7.9 is slightly misleading because the y-axis and x-axis range are not equal. This can be corrected using the coord_caretesian() function.\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           y = ENGLISH)) +\n  geom_point() +\n  geom_smooth(method = lm, \n              linewidth = 0.5) +  \n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100))\n\n\n\n\n\nObservation: From the best fit curve, we can see that there appears to be a positive correlation between English and Maths scores, i.e., students with higher English scores also tend to have higher Maths scores. Also, the line is gentler than a y = x line, which means that every one unit increment in Maths scores leads to a lesser than one unit increment in English scores.\n\n\n\n\n\nThemes control elements of the graph that are not related to the data, such as:\n\nBackground colour;\nSize of fonts;\nGrid lines; and\nColour of labels.\n\nThe built-in themes include:\n\ntheme_grey() (default);\ntheme_bw(); and\ntheme_classic().\n\nA list of theme can be found at this link. Each theme element can be conceived of as either a line (e.g. x-axis), a rectangle (e.g. graph background), or text (e.g. axis title).\n\n\nThe default (theme_grey()), theme_bw(),theme_classic(), andtheme_minimal()` functions are used for the horizontal bar charts below.\n\nDefault (theme_grey)theme_bwtheme_classictheme_minimal\n\n\n\nggplot(data = exam_data, \n       aes(x = RACE)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\n\nggplot(data = exam_data, \n       aes(x = RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_bw()\n\n\n\n\n\n\n\nggplot(data = exam_data, \n       aes(x = RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_classic()\n\n\n\n\n\n\n\nggplot(data = exam_data, \n       aes(x = RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nHadley Wickham (2023) ggplot2: Elegant Graphics for Data Analysis. Online 3rd edition.\nWinston Chang (2013) R Graphics Cookbook 2nd edition. Online version.\nHealy, Kieran (2019) Data Visualization: A practical introduction. Online version.\nLearning ggplot2 on Paper – Components\nLearning ggplot2 on Paper – Layer\nLearning ggplot2 on Paper – Scale\n\n~~~ End of Hands-on Exercise 1 ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#overview-and-learning-outcomes",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#overview-and-learning-outcomes",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 1 of the R for Visual Analytics book.\nThe learning outcomes are:\n\nLearn the basic principles and essential components of ggplot2;\nGain hands-on experience on using ggplot2 components to plot statistical graphics based on the principle of Layered Grammar of Graphics; and\nApply the essential graphical elements provided by ggplot2 to create elegant yet functional statistical graphics."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#getting-started",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "The code chunk below uses the p_load() function in the pacman package to check if the tidyverse packages are installed in the computer. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, exam_data.\n\nexam_data = read_csv(\"data/Exam_data.csv\")\n\nThe tibble data frame, exam_data, has 7 columns and 322 rows.\n\nIt consists of the year-end examination grades of a cohort of 322 Primary 3 students from a local school.\nThe 7 variables/attributes are:\n\nCategorical: ID, CLASS, GENDER, and RACE.\nContinuous: MATHS, ENGLISH, and SCIENCE."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#introducing-ggplot",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#introducing-ggplot",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "The ggplot2 package is an R package for declaratively creating data-driven graphics based on The Grammar of Graphics.\nIt is part of the tidyverse family of packages, and is specially designed for visual exploration and communication.\n\n\nR Graphics is the core graphical functions in Base R. A comparison is made between R Graphics and ggplot by plotting a simple histogram.\n\nR Graphicsggplot2\n\n\n\nhist(exam_data$MATHS)\n\n\n\n\n\n\n\nggplot(data=exam_data, aes(x = MATHS)) +\n  geom_histogram(bins=10, \n                 boundary = 100,\n                 color=\"black\", \n                 fill=\"grey\") +\n  ggtitle(\"Distribution of Maths scores\")\n\n\n\n\n\n\n\nIn the above example, a relatively simple code chunk is used for R Graphics, while more details are added via ggplot2.\nSo why use ggplot2? According to the creator of tidyverse, Hadley Wickham:\n\n“The transferable skills from ggplot2 are not the idiosyncrasies of plotting syntax, but a powerful way of thinking about visualisation, as a way of mapping between variables and the visual properties of geometric objects that you can perceive.”"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#grammar-of-graphics",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#grammar-of-graphics",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "It is important to understand the principles of the Grammar of Graphics.\nThe Grammar of Graphics is a general scheme for data visualisation which breaks up graphs into semantic components such as scales and layers. It was introduced by Leland Wilkinson (1999) through the book - The Grammar of Graphics, published by Springer. It is an answer to a question:\n\nWhat is a statistical graphic?\n\nIn the nutshell, the Grammar of Graphics defines the rules of structuring mathematical and aesthetic elements into a meaningful graph.\nThere are two principles in the Grammar of Graphics:\n\nGraphics is made up of distinct layers of grammatical elements.\nMeaningful plots is achieved through aesthetic mapping.\n\nSo why is it important to have a good grasp of the grammar of graphics?\n\nAllows us to gain insights into the composition of complicated graphics, and reveal unexpected connections between seemingly different graphics (Cox, 1978).\nProvides a strong foundation for understanding a diverse range of graphics.\nImpart better understanding of what well-formed or correct graphics could look like (although there will still be many grammatically correct but nonsensical graphics).\n\n\n\nThe seven grammars of ggplot2 are:\n\nData: The dataset being plotted.\nAesthetics take attributes of the data and use them to influence visual characteristics, e.g., position, colours, size, shape, transparency.\nGeometrics: The visual elements used for the data, e.g., point, bar, line.\nFacets split the data into subsets to create multiple variations of the same graph, e.g., paneling, multiple plots.\nStatistics refer to statistical transformations that summarise data e.g., mean, confidence intervals.\nCoordinates refer to coordinate systems that define the plane on which data are mapped on the graphic.\nThemes modify all non-data components of a plot, e.g., main title, sub-title, y-aixs title, legend, background."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-data",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "The ggplot() function in the ggplot2 package is used to initiate a ggplot object. The “data” argument defines the dataset to be used for plotting the graphic.\nIf the dataset is not already a data frame, it will be converted to one via the fortify() function in the ggplot2 package.\n\nggplot(data=exam_data)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-aesthetic-mappings",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-aesthetic-mappings",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "Aesthetic mappings take attributes of the data and and use them to influence visual characteristics, e.g., position, colour, size, shape, or transparency. Each visual characteristic can thus encode an aspect of the data and be used to convey information.\nAll aesthetics of a plot are specified in the aes() function. Each geom layer can have its own aesthetic specification.\nThe code chunk below adds the aesthetic element into the plot. The x-axis and its label are added to the plot.\n\nggplot(data = exam_data, \n       aes(x = MATHS))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-geom",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-geom",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "The geometric objects are the actual marks put on the plot. A plot must have at least one geom, but may have more.\nSome examples of geom:\n\ngeom_point() for drawing individual points (e.g., a scatter plot);\ngeom_line() for drawing lines (e.g., for a line chart);\ngeom_smooth() for drawing smoothed lines (e.g., for simple trends or approximations);\ngeom_bar() for drawing bars (e.g., for bar chart);\ngeom_histogram() for drawing binned values (e.g. a histogram);\ngeom_polygon() for drawing arbitrary shapes; and\ngeom_map() for drawing polygons in the shape of a map. Can use the map_data() function to access the data used for these maps.\n\n\n\nA bar chat is plotted using the geom_bar() function.\n\nggplot(data = exam_data, \n       aes(x = RACE)) +\n  geom_bar()\n\n\n\n\n\nObservation: From the bar chart, we can see that most of the students are Chinese, followed by Malay. A minority of students are Indian or other races.\n\n\n\n\nIn a dot plot, the width of a dot corresponds to the bin width (or maximum width, depending on the binning algorithm), and the dots are stacked, with each dot representing one observation.\nA dot plot is plotted using the geom_dotplot() function.\n\nggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(dotsize = 0.5)\n\n\n\n\nThe y-axis for the plot above is misleading. Hence, the scale_y_continuous() function in the ggplot2 package is used to turn off the y-axis. Also, the “binwidth” argument is used to change the bin width to 2.5.\n\nggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(binwidth=2.5,\n               dotsize = 0.5) +\n  scale_y_continuous(NULL,\n                     breaks = NULL)\n\n\n\n\n\nObservation: From the dot plot, we can see that the maths scores are left-skewed, with more students scoring above 50 than below 50.\n\n\n\n\nThe geom_histogram() function is used to create a simple histogram using the values in the “MATHS” attribute of the data set.\n\nggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_histogram()    \n\n\n\n\n\nObservation: From the histogram, we can see that the maths scores are left-skewed, with more students scoring above 50 than below 50.\n\n\n\n\nSome customisations are done to the histogram:\n\nThe “bins” argument is used to change the number of bins from the default 30 to 20;\nThe “fill” argument is used to fill the bars with light blue; and\nThe “colour” argument is used to change the outline of the bars to black.\n\n\nggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_histogram(bins = 20,            \n                 colour = \"black\",      \n                 fill = \"light blue\")\n\n\n\n\n\nObservation: From the histogram, we can see that the maths scores are left-skewed, with more students scoring above 50 than below 50. Also, we can also see more clearly that the score range with the most students is around 75.\n\n\n\n\nThe fill of the histogram is changed using the sub-group of the aesthetic() function, which divides the bars by the “GENDER” attribute.\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           fill = GENDER)) +\n  geom_histogram(bins = 20, \n                 color = \"grey30\")\n\n\n\n\n\nObservation: From the histogram, we can see the distributions of math scores for female and male students. There appears to be less female students who score below 50 compared to male students who score below 50.\n\n\n\n\nThe geom_density() function is used to compute and plot the kernel density estimate, which is a smoothed version of a histogram. It is a useful alternative to the histogram for continuous data that comes from an underlying smooth distribution.\n\nggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_density()           \n\n\n\n\n\nObservation: From the kernel density estimate plot, we can see the most ‘dense’/common score is actually slightly above 75.\n\nThe kernel density estimate is then plotted by the “GENDER” attribute using the “colour” argument.\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           colour = GENDER)) +\n  geom_density()\n\n\n\n\n\nObservation: From the kernel density estimate plot, we can see more clearly that there are more female students than male students who score above 50. Conversely, there are more male students than female students who score below 50.\n\nA similar plot can be produced using the “fill” argument.\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           fill = GENDER,\n           alpha = 0.1)) +\n  geom_density()\n\n\n\n\n\n\n\nThe geom_boxplot() function is used to display a continuous value list. It visualises five summary statistics (minimum, first quartile, median, third quartile, and maximum) and any outliers.\n\nggplot(data = exam_data, \n       aes(y = MATHS,       \n           x= GENDER)) +    \n  geom_boxplot()\n\n\n\n\n\nObservation: From the boxplot, we can see that the median scores for female students and male students are rather similar. There are also outliers for both genders in the lower score range.\n\nThe “notch” argument is used to help visually assess whether the median of distributions differ. If the notches do not overlap, it means that the medians are different.\n\nggplot(data = exam_data, \n       aes(y = MATHS, \n           x = GENDER)) +\n  geom_boxplot(notch = TRUE)\n\n\n\n\n\nObservation: From the notched plot, we can see clearly that the median scores for female students and male students are different.\n\n\n\n\nThe geom_violin() function is designed for creating a violin plot. Violin plots are a way of comparing multiple data distributions. With ordinary density curves, it is difficult to compare more than just a few distributions because the lines visually interfere with each other. With a violin plot, it’s easier to compare several distributions since they are placed side by side.\n\nggplot(data = exam_data, \n       aes(y = MATHS, \n           x = GENDER)) +\n  geom_violin()\n\n\n\n\n\nObservation: From the violin plot, we can see clearly that the distribution of the scores by gender is relatively similarly. A key difference is that the lower scores in the distribution for male students are lower than that for female students.\n\n\n\n\nThe geom_point() function is used to create a scatterplot. The “MATHS” and “ENGLISH” attributes of the dataset are plotted.\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           y = ENGLISH)) +\n  geom_point()            \n\n\n\n\n\nObservation: From the scatterplot, we can see that there appears to be a positive correlation between English and Maths scores, i.e., students with higher English scores also tend to have higher Maths scores.\n\n\n\n\nThe code chunk below plots the data points on boxplots and scatterplots using both the geom_boxplot() and geom_point() functions.\n\nggplot(data = exam_data, \n       aes(y = MATHS, \n           x = GENDER)) +\n  geom_boxplot() +                    \n  geom_point(position = \"jitter\",\n             size = 0.5)\n\n\n\n\n\nNote: The “position” argument is used to randomly vary the location of the dots, which otherwise would have lined up in the middle of the boxplot."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-stat",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-stat",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "The statistics functions statistically transform data, usually into some form of summary - e.g., mean, confidence limit.\nThere are two ways to use these functions:\n\nAdd a stat() function and override the default geom, or\nAdd a geom() function and override the default stat.\n\n\n\nThe boxplots below are incomplete because the positions of the means were not shown.\n\nggplot(data = exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nThe mean values are added using the stat_summary() function in the ggplot2 package. This overrides the default geom.\n\nggplot(data = exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() +\n  stat_summary(geom = \"point\",       \n               fun = \"mean\",         \n               colour = \"red\",        \n               size = 4)               \n\n\n\n\n\nNote: The “fun” argument is used to insert the mean value as a dot instead of the “fun.y” argument which has been deprecated.\n\n\nObservation: We can see that the mean scores for both gender are below their respective median scores, indicating left-skewed distributions of scores.\n\n\n\n\nThe mean values are added using the geom_point() function. This overrides the default stat.\n\nggplot(data = exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() +\n  geom_point(stat = \"summary\",        \n             fun = \"mean\",           \n             colour = \"red\",          \n             size = 4)          \n\n\n\n\n\nNote: The “fun” argument is used to insert the mean value as a dot instead of the “fun.y” argument which has been deprecated.\n\n\nObservation: Similarly, we can see that the mean scores for both gender are below their respective median scores, indicating left-skewed distributions of scores.\n\n\n\n\nReverting to the scatterplot on the distribution of the “MATHS” and “ENGLISH” attributes (see section 1.7.9), the interpretability of the plot can be improved with the addition of a best fit curve. The geom_smooth() function is used.\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           y = ENGLISH)) +\n  geom_point() +\n  geom_smooth(linewidth = 0.5)\n\n\n\n\n\nNote: The “linewidth” argument is used to indicate the width of the grey shaded region of the best fit curve instead of the “size” argument which has been deprecated.\n\n\nObservation: From the best fit curve, we can see that there appears to be a positive correlation between English and Maths scores, i.e., students with higher English scores also tend to have higher Maths scores. Also, there also appears to be an inflection point around Maths score 75 and English score 70.\n\nThe default method is loess, which draws a smooth curve. If the “method” argument is set to “lm”, the plot produces a linear line based on a linear model.\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           y = ENGLISH)) +\n  geom_point() +\n  geom_smooth(method = lm, \n              linewidth = 0.5)\n\n\n\n\n\nNote: The “linewidth” argument is used to indicate the width of the grey shaded region of the best fit curve instead of the “size” argument which has been deprecated."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-facets",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-facets",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "Facetting generates small multiples (sometimes also called trellis plot), each displaying a different subset of the dataset. They are an alternative to aesthetics for displaying additional discrete variables. The ggplot2 package supports two types of facets, namely: facet_grid() and facet_wrap().\n\n\nThe facet_wrap() function wraps a 1d sequence of panels into 2d. This is generally a better use of screen space than the facet_grid() function because most displays are roughly rectangular.\n\nggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_histogram(bins = 20) +\n    facet_wrap(~ CLASS)\n\n\n\n\n\nObservation: Based on the trellis plot, we can see that there are differences in the Maths scores between different classes. Students from classes 3A-F do better than students from classes 3G-I.\n\n\n\n\nThe facet_grid() function forms a matrix of panels defined by row and column facetting variables. It is most useful when there are two discrete variables, and all combinations of the variables exist in the dataset.\n\nggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_histogram(bins = 20) +\n    facet_grid(~ CLASS)\n\n\n\n\n\nObservation: Based on the trellis plot, we can also see that there are differences in the Maths scores between different classes. Students from classes 3A-F do better than students from classes 3G-I. However, this plot is more difficult to interpret than the one using the facet_wrap() function."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-coordinates",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-coordinates",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "The coordinates functions map the position of objects onto the plane of the plot. There are a number of different possible coordinate systems to use, they are:\n\ncoord_cartesian() - the default cartesian coordinate systems, where x and y values can be specified (e.g., allows zoom in or out).\ncoord_flip() - a cartesian system with the x and y axes flipped.\ncoord_fixed() - a cartesian system with a “fixed” aspect ratio (e.g., 1.78 for a widescreen” plot).\ncoord_quickmap() - a coordinate system that approximates a good aspect ratio for maps.\n\n\n\nBy default, the geom_bar() function produces a bar chart in vertical form. This can be switched using the coord_flip() function.\n\nggplot(data = exam_data, \n       aes(x = RACE)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\nObservation: A plot using horizontal bars may be useful when the labels for the vertical axis are long words.\n\n\n\n\nThe scatterplot in section 1.7.9 is slightly misleading because the y-axis and x-axis range are not equal. This can be corrected using the coord_caretesian() function.\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           y = ENGLISH)) +\n  geom_point() +\n  geom_smooth(method = lm, \n              linewidth = 0.5) +  \n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100))\n\n\n\n\n\nObservation: From the best fit curve, we can see that there appears to be a positive correlation between English and Maths scores, i.e., students with higher English scores also tend to have higher Maths scores. Also, the line is gentler than a y = x line, which means that every one unit increment in Maths scores leads to a lesser than one unit increment in English scores."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-themes",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#essential-grammatical-elements-in-ggplot2-themes",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "Themes control elements of the graph that are not related to the data, such as:\n\nBackground colour;\nSize of fonts;\nGrid lines; and\nColour of labels.\n\nThe built-in themes include:\n\ntheme_grey() (default);\ntheme_bw(); and\ntheme_classic().\n\nA list of theme can be found at this link. Each theme element can be conceived of as either a line (e.g. x-axis), a rectangle (e.g. graph background), or text (e.g. axis title).\n\n\nThe default (theme_grey()), theme_bw(),theme_classic(), andtheme_minimal()` functions are used for the horizontal bar charts below.\n\nDefault (theme_grey)theme_bwtheme_classictheme_minimal\n\n\n\nggplot(data = exam_data, \n       aes(x = RACE)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\n\nggplot(data = exam_data, \n       aes(x = RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_bw()\n\n\n\n\n\n\n\nggplot(data = exam_data, \n       aes(x = RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_classic()\n\n\n\n\n\n\n\nggplot(data = exam_data, \n       aes(x = RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#references",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "Hadley Wickham (2023) ggplot2: Elegant Graphics for Data Analysis. Online 3rd edition.\nWinston Chang (2013) R Graphics Cookbook 2nd edition. Online version.\nHealy, Kieran (2019) Data Visualization: A practical introduction. Online version.\nLearning ggplot2 on Paper – Components\nLearning ggplot2 on Paper – Layer\nLearning ggplot2 on Paper – Scale\n\n~~~ End of Hands-on Exercise 1 ~~~"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 2 of the R for Visual Analytics book.\nThe learning outcomes are:\n\nControl the placement of annotation on a graph using functions provided in the ggrepel package;\nCreate professional publication quality figures using functions provided in the ggthemes and hrbrthemes packages; and\nPlot composite figures by combining ggplot2 graphs using the patchwork package.\n\n\n\n\n\n\nIn this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nggrepel for geoms to repel overlapping text labels for ggplot2;\nggthemes for extra themes, geoms, and scales for ggplot2;\nhrbrthemes for typography-centric themes and theme components for ggplot2; and\npatchwork for preparing composite figures created using ggplot2.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, ggrepel, \n               ggthemes, hrbrthemes,\n               patchwork)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, exam_data.\n\nexam_data = read_csv(\"data/Exam_data.csv\")\n\nThe tibble data frame, exam_data, has 7 columns and 322 rows.\n\nIt consists of the year-end examination grades of a cohort of 322 Primary 3 students from a local school.\nThe 7 variables/attributes are:\n\nCategorical: ID, CLASS, GENDER, and RACE.\nContinuous: MATHS, ENGLISH, and SCIENCE.\n\n\n\n\n\n\nA challenge in plotting a statistical graph is annotation, especially with a large number of data points.\nThe scatterplot below shows many overlapping annotations because of the sheer number of dots on it.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           y = ENGLISH)) +\n  geom_point() +\n  geom_smooth(method = lm, \n              size = 0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\nThe ggrepel package is an extension of the ggplot2 package, which provides the geoms for ggplot2 to repel overlapping text as shown in the plot above. This is done by replacing the geom_text() and geom_label() functions in the ggplot2 package with the geom_text_repel() and geom_label_repel() functions respectively in the ggrepel package.\n\n\nThe previous scatterplot is re-plotted by replacing the geom_label() function in the ggplot2 package with the geom_label_repel() function in the ggrepel package.\n\nNote: The annotations avoid the main clusters and labels potential outlier dots.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           y = ENGLISH)) +\n  geom_point() +\n  geom_smooth(method = lm, \n              size = 0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\") +\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\n\n\nThe ggplot2 package comes with eight built-in themes, they are: theme_grey(), theme_bw(), theme_classic(), theme_dark(), theme_light(), theme_linedraw(), theme_minimal(), and theme_void().\nThe following plot uses the default grey theme for the histogram showing the distribution of Maths scores.\n\nNote: There is no need to explicitly include the theme_grey() function as it is the default theme.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins = 20, \n                 boundary = 100,\n                 color = \"grey25\", \n                 fill = \"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\nThe ggthemes package provides ggplot2 themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, ‘Stata’, ‘Excel’, and The Wall Street Journal, etc. It also provides some extra geoms and scales for ggplot2.\nThe previous plot is re-plotted with The Economist theme below, with the classic light blue background.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins = 20, \n                 boundary = 100,\n                 color = \"grey25\", \n                 fill = \"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_economist()\n\n\n\n\n\n\n\nThe hrbrthemes package has two goals. The first is that it provides a base theme that focuses on typographic elements, such as where various labels are placed and the types of fonts used.\nThe previous plot is re-plotted with the theme_ipsum function below.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins = 20, \n                 boundary = 100,\n                 color = \"grey25\", \n                 fill = \"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\n\n\n\nThe second goal of the hrbrthemes package is productivity for a production workflow. A “production workflow” is when the output of the work is to be put into a publication of some kind. While working through the analysis, the visual elements do not need to be perfect but are there to validate/support the work and are more of a starting point for the finished product.\nThe previous plot is re-plotted with the theme_ipsum function below, with the addition of some arguments to customise the elements.\n\nNote: The various arguments are:\n\n“axis_title_size” argument is used to increase the font size of the axis title to 18;\n“base_size” argument is used to increase the default axis label to 15; and\n“grid” argument is used to remove the x-axis grid lines and show only the y-axis grid lines.\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins = 20, \n                 boundary = 100,\n                 color = \"grey25\", \n                 fill = \"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")\n\n\n\n\n\n\n\n\nOften, multiple graphs are required to tell a compelling visual story. There are several ggplot2 extensions that provide functions to compose a figure with multiple graphs. A composite plot is created by combining multiple graphs.\nTo start, three separate statistical graphics are created below, showing the distribution of Maths scores, the distribution of English scores, and the English scores versus Maths scores.\n\nMathsEnglishMaths vs. EnglishCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 = ggplot(data = exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins = 20, \n                 boundary = 100,\n                 color = \"grey25\", \n                 fill = \"grey90\") + \n  coord_cartesian(xlim = c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\np2 = ggplot(data = exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins = 20, \n                 boundary = 100,\n                 color = \"grey25\", \n                 fill = \"grey90\") +\n  coord_cartesian(xlim = c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\np3 = ggplot(data = exam_data, \n             aes(x = MATHS, \n                 y = ENGLISH)) +\n  geom_point() +\n  geom_smooth(method = lm, \n              size = 0.5) +  \n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\nThere are several ggplot2 extension functions that support the preparation of a composite figure by combining several graphs. These functions include: grid.arrange() in the gridExtra package and plot_grid() in the cowplot package.\nThe ggplot2 extension patchwork package is specially designed for combining separate ggplot2 graphs into a single figure. It has a simple syntax to create layouts easily:\n\nTwo-column layout using “+”;\n“()” to create a subplot group; and\nTwo-row layout using “/”.\n\n\n\n\nA composite figure of two histograms showing the distributions of Maths scores and English scores respectively is created using the syntax of the patchwork package.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\np1 + p2\n\n\n\n\n\n\n\nA more complex composite figure can be plotted using other appropriate operators.\nFor example, the composite figure below is plotted by using:\n\n“/” to stack two ggplot2 graphs;\n“|” to place the plots beside each other; and\n“()” to define the sequence of the plotting.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n(p1 / p2) | p3\n\n\n\n\n\n\n\nThe patchwork package also allow subplots to be identified in text using its auto-tagging capabilities.\n\nRoman NumeralsArabic NumeralsAlphabetsCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'I')\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = '1')\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\n\nBesides providing functions to place plots next to each other based on the provided layout, the inset_element() function in the patchwork package can place one or several plots or graphic elements freely on top or below another plot.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n\n\n\n\n\n\nUsing both the ggthemes and patchwork packages, a composite figure with The Economist theme is created.\n\nNote: The subplots are tagged with “A”, “B”, and “C” using the plot_annotation() function in the patchwork package. Also, the theme() function in the ggplot2 package is used with the “plot.title” argument to reduce the size of the titles to fit the composite figure.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\npatchwork = (p1 / p2) | p3\npatchwork & theme_economist() + theme(plot.title = element_text(size=10))\n\n\n\n\n\n\n\n\n\nPatchwork R Package Goes Nerd Viral\nR Packages: ggrepel, ggthemes, hrbrthemes, and patchwork\nggplot Tips: Arranging Plots\nggplot2 Theme Elements Demonstration\nggplot2 Theme Elements Reference Sheet\n\n~~~ End of Hands-on Exercise 2 ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#overview-and-learning-outcomes",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#overview-and-learning-outcomes",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 2 of the R for Visual Analytics book.\nThe learning outcomes are:\n\nControl the placement of annotation on a graph using functions provided in the ggrepel package;\nCreate professional publication quality figures using functions provided in the ggthemes and hrbrthemes packages; and\nPlot composite figures by combining ggplot2 graphs using the patchwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#getting-started",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "In this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nggrepel for geoms to repel overlapping text labels for ggplot2;\nggthemes for extra themes, geoms, and scales for ggplot2;\nhrbrthemes for typography-centric themes and theme components for ggplot2; and\npatchwork for preparing composite figures created using ggplot2.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, ggrepel, \n               ggthemes, hrbrthemes,\n               patchwork)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, exam_data.\n\nexam_data = read_csv(\"data/Exam_data.csv\")\n\nThe tibble data frame, exam_data, has 7 columns and 322 rows.\n\nIt consists of the year-end examination grades of a cohort of 322 Primary 3 students from a local school.\nThe 7 variables/attributes are:\n\nCategorical: ID, CLASS, GENDER, and RACE.\nContinuous: MATHS, ENGLISH, and SCIENCE."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-annotation-ggrepel",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-annotation-ggrepel",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "A challenge in plotting a statistical graph is annotation, especially with a large number of data points.\nThe scatterplot below shows many overlapping annotations because of the sheer number of dots on it.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           y = ENGLISH)) +\n  geom_point() +\n  geom_smooth(method = lm, \n              size = 0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\nThe ggrepel package is an extension of the ggplot2 package, which provides the geoms for ggplot2 to repel overlapping text as shown in the plot above. This is done by replacing the geom_text() and geom_label() functions in the ggplot2 package with the geom_text_repel() and geom_label_repel() functions respectively in the ggrepel package.\n\n\nThe previous scatterplot is re-plotted by replacing the geom_label() function in the ggplot2 package with the geom_label_repel() function in the ggrepel package.\n\nNote: The annotations avoid the main clusters and labels potential outlier dots.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           y = ENGLISH)) +\n  geom_point() +\n  geom_smooth(method = lm, \n              size = 0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\") +\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-themes",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-themes",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "The ggplot2 package comes with eight built-in themes, they are: theme_grey(), theme_bw(), theme_classic(), theme_dark(), theme_light(), theme_linedraw(), theme_minimal(), and theme_void().\nThe following plot uses the default grey theme for the histogram showing the distribution of Maths scores.\n\nNote: There is no need to explicitly include the theme_grey() function as it is the default theme.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins = 20, \n                 boundary = 100,\n                 color = \"grey25\", \n                 fill = \"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\nThe ggthemes package provides ggplot2 themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, ‘Stata’, ‘Excel’, and The Wall Street Journal, etc. It also provides some extra geoms and scales for ggplot2.\nThe previous plot is re-plotted with The Economist theme below, with the classic light blue background.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins = 20, \n                 boundary = 100,\n                 color = \"grey25\", \n                 fill = \"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_economist()\n\n\n\n\n\n\n\nThe hrbrthemes package has two goals. The first is that it provides a base theme that focuses on typographic elements, such as where various labels are placed and the types of fonts used.\nThe previous plot is re-plotted with the theme_ipsum function below.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins = 20, \n                 boundary = 100,\n                 color = \"grey25\", \n                 fill = \"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\n\n\n\nThe second goal of the hrbrthemes package is productivity for a production workflow. A “production workflow” is when the output of the work is to be put into a publication of some kind. While working through the analysis, the visual elements do not need to be perfect but are there to validate/support the work and are more of a starting point for the finished product.\nThe previous plot is re-plotted with the theme_ipsum function below, with the addition of some arguments to customise the elements.\n\nNote: The various arguments are:\n\n“axis_title_size” argument is used to increase the font size of the axis title to 18;\n“base_size” argument is used to increase the default axis label to 15; and\n“grid” argument is used to remove the x-axis grid lines and show only the y-axis grid lines.\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins = 20, \n                 boundary = 100,\n                 color = \"grey25\", \n                 fill = \"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-single-graph",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-single-graph",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "Often, multiple graphs are required to tell a compelling visual story. There are several ggplot2 extensions that provide functions to compose a figure with multiple graphs. A composite plot is created by combining multiple graphs.\nTo start, three separate statistical graphics are created below, showing the distribution of Maths scores, the distribution of English scores, and the English scores versus Maths scores.\n\nMathsEnglishMaths vs. EnglishCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 = ggplot(data = exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins = 20, \n                 boundary = 100,\n                 color = \"grey25\", \n                 fill = \"grey90\") + \n  coord_cartesian(xlim = c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\np2 = ggplot(data = exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins = 20, \n                 boundary = 100,\n                 color = \"grey25\", \n                 fill = \"grey90\") +\n  coord_cartesian(xlim = c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\np3 = ggplot(data = exam_data, \n             aes(x = MATHS, \n                 y = ENGLISH)) +\n  geom_point() +\n  geom_smooth(method = lm, \n              size = 0.5) +  \n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\nThere are several ggplot2 extension functions that support the preparation of a composite figure by combining several graphs. These functions include: grid.arrange() in the gridExtra package and plot_grid() in the cowplot package.\nThe ggplot2 extension patchwork package is specially designed for combining separate ggplot2 graphs into a single figure. It has a simple syntax to create layouts easily:\n\nTwo-column layout using “+”;\n“()” to create a subplot group; and\nTwo-row layout using “/”.\n\n\n\n\nA composite figure of two histograms showing the distributions of Maths scores and English scores respectively is created using the syntax of the patchwork package.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\np1 + p2\n\n\n\n\n\n\n\nA more complex composite figure can be plotted using other appropriate operators.\nFor example, the composite figure below is plotted by using:\n\n“/” to stack two ggplot2 graphs;\n“|” to place the plots beside each other; and\n“()” to define the sequence of the plotting.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n(p1 / p2) | p3\n\n\n\n\n\n\n\nThe patchwork package also allow subplots to be identified in text using its auto-tagging capabilities.\n\nRoman NumeralsArabic NumeralsAlphabetsCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'I')\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = '1')\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\n\nBesides providing functions to place plots next to each other based on the provided layout, the inset_element() function in the patchwork package can place one or several plots or graphic elements freely on top or below another plot.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n\n\n\n\n\n\nUsing both the ggthemes and patchwork packages, a composite figure with The Economist theme is created.\n\nNote: The subplots are tagged with “A”, “B”, and “C” using the plot_annotation() function in the patchwork package. Also, the theme() function in the ggplot2 package is used with the “plot.title” argument to reduce the size of the titles to fit the composite figure.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\npatchwork = (p1 / p2) | p3\npatchwork & theme_economist() + theme(plot.title = element_text(size=10))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#references",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "Patchwork R Package Goes Nerd Viral\nR Packages: ggrepel, ggthemes, hrbrthemes, and patchwork\nggplot Tips: Arranging Plots\nggplot2 Theme Elements Demonstration\nggplot2 Theme Elements Reference Sheet\n\n~~~ End of Hands-on Exercise 2 ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03B.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03B.html",
    "title": "Hands-on Exercise 3B",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 4 of the R for Visual Analytics book.\nThe learning outcomes are:\n\nLearn how to create animated data visualisation using the gganimate and plotly packages; and\nLearn how to reshape data using the tidyr package, and process, wrangle, and transform data using the dplyr package.\n\n\n\nWhen creating animations, the plot does not actually move. Instead, many individual plots are built and stitched together as movie frames, just like an old-school flip book or cartoon. Each frame is a different plot when conveying motion, which is built using some relevant subset of the aggregate data (e.g., subsets based on time variable). The subset drives the flow of the animation when stitched back together.\n\n\n\n\n\n\n\n\nThe key concepts and terminology related to this type of animated data visualisation are:\n\nFrame: In an animated line graph, each frame represents a different point in time or a different category. When the frame changes, the data points on the graph are updated to reflect the new data.\nAnimation Attributes: The animation attributes are the settings that control how the animation behaves. For example, one can specify the duration of each frame, the easing function used to transition between frames, and whether to start the animation from the current frame or from the beginning.\n\n\n\n\n\n\n\nIn this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nplotly for plotting interactive statistical graphs;\ngganimate (ggplot extension) for creating animated statistical graphs.\ngifski for converting video frames to GIF animations using pngquant’s fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.\ngapminder fpr providing data available at Gapminder.org. The country_colors scheme is relevant.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, readxl,\n               plotly, gganimate, \n               gifski, gapminder)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_xls() function in the readxl package and stored as the R object, globalPop.\nThe mutate_each_() function in the dplyr package is used to convert all character data type into factor data type, and the mutate() function in the dplyr package is used to convert the data values for the Year field into integer data type.\n\ncol = c(\"Country\", \"Continent\")\n\nglobalPop = read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_each_(funs(factor(.)), col) %&gt;%\n  mutate(Year = as.integer(Year))\n\nHowever, the mutate_each_() function was deprecated in dplyr 0.7.0. and the funs() function was deprecated in dplyr 0.8.0. In view of this, the code chunk is rewritten using the mutate_at() function in the dplyr package.\n\ncol = c(\"Country\", \"Continent\")\n\nglobalPop = read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_at(col, as.factor) %&gt;%\n  mutate(Year = as.integer(Year))\n\nAlternatively, the across() function in the dplyr package can be used with the mutate() function to obtain the same output.\n\ncol = c(\"Country\", \"Continent\")\n\nglobalPop = read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate(across(col, as.factor)) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\n\nThe tibble data frame, globalPop, has 6 columns and 6,204 rows.\n\nIt consists of the populations of 222 countries, across 6 continents.\nIt also shows the percentages of the Old and Young subsets of the populations.\n\n\nn_distinct(globalPop$Country)\n\n[1] 222\n\nn_distinct(globalPop$Continent)\n\n[1] 6\n\n\n\n\n\n\nThe gganimate package extends the Grammar of Graphics as implemented by ggplot2 to include the description of animation. It does this by providing a range of new grammar classes that can be added to the plot object to customise how it should change with time:\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_*()/exit_*() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\n\n\nA static population bubble plot is created using basic ggplot2 functions.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') \n\n\n\n\n\n\n\nTo create an animated population bubble plot, the transition_time() function in the gganimate package is used to create transition through distinct states in time (i.e. Year), and the ease_aes() function in the gganimate package is used to control the easing of aesthetics. The default is ‘linear’. Other easing methods are quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')\n\n\n\n\n\n\n\n\nBoth the plot_ly() and ggplotly() functions in the plotly package can be used to support key frame animations through the “frame” aesthetic argument. They also support the “ids” aesthetic argument to ensure smooth transitions between objects with the same id (which helps facilitate object constancy).\n\n\nAn animated population bubble plot is created using the ggplotly() function to convert the static graphic object into an animated svg object.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\ngg = ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nggplotly(gg)\n\n\n\n\nAlthough the “show.legend” argument was set as FALSE, the legend still appeared on the plot. To overcome this problem, the theme() function with the “legend.position” argument set to none is used.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\ngg = ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') + \n  theme(legend.position='none')\n\nggplotly(gg)\n\n\n\n\n\n\n\nAn animated population bubble plot is created using the plot_ly() function.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nbp = globalPop %&gt;%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers') %&gt;%\n  layout(showlegend = FALSE)\n\nbp\n\n\n\n\n\n\n\n\n\ngganimate and example.\nBuilding An Animation Step-by-Step with gganimate.\nCreating A Composite gif with Multiple gganimate Panels.\n\n~~~ End of Hands-on Exercise 3B ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03B.html#overview-and-learning-outcomes",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03B.html#overview-and-learning-outcomes",
    "title": "Hands-on Exercise 3B",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 4 of the R for Visual Analytics book.\nThe learning outcomes are:\n\nLearn how to create animated data visualisation using the gganimate and plotly packages; and\nLearn how to reshape data using the tidyr package, and process, wrangle, and transform data using the dplyr package.\n\n\n\nWhen creating animations, the plot does not actually move. Instead, many individual plots are built and stitched together as movie frames, just like an old-school flip book or cartoon. Each frame is a different plot when conveying motion, which is built using some relevant subset of the aggregate data (e.g., subsets based on time variable). The subset drives the flow of the animation when stitched back together.\n\n\n\n\n\n\n\n\nThe key concepts and terminology related to this type of animated data visualisation are:\n\nFrame: In an animated line graph, each frame represents a different point in time or a different category. When the frame changes, the data points on the graph are updated to reflect the new data.\nAnimation Attributes: The animation attributes are the settings that control how the animation behaves. For example, one can specify the duration of each frame, the easing function used to transition between frames, and whether to start the animation from the current frame or from the beginning."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03B.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03B.html#getting-started",
    "title": "Hands-on Exercise 3B",
    "section": "",
    "text": "In this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nplotly for plotting interactive statistical graphs;\ngganimate (ggplot extension) for creating animated statistical graphs.\ngifski for converting video frames to GIF animations using pngquant’s fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.\ngapminder fpr providing data available at Gapminder.org. The country_colors scheme is relevant.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, readxl,\n               plotly, gganimate, \n               gifski, gapminder)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_xls() function in the readxl package and stored as the R object, globalPop.\nThe mutate_each_() function in the dplyr package is used to convert all character data type into factor data type, and the mutate() function in the dplyr package is used to convert the data values for the Year field into integer data type.\n\ncol = c(\"Country\", \"Continent\")\n\nglobalPop = read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_each_(funs(factor(.)), col) %&gt;%\n  mutate(Year = as.integer(Year))\n\nHowever, the mutate_each_() function was deprecated in dplyr 0.7.0. and the funs() function was deprecated in dplyr 0.8.0. In view of this, the code chunk is rewritten using the mutate_at() function in the dplyr package.\n\ncol = c(\"Country\", \"Continent\")\n\nglobalPop = read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_at(col, as.factor) %&gt;%\n  mutate(Year = as.integer(Year))\n\nAlternatively, the across() function in the dplyr package can be used with the mutate() function to obtain the same output.\n\ncol = c(\"Country\", \"Continent\")\n\nglobalPop = read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate(across(col, as.factor)) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\n\nThe tibble data frame, globalPop, has 6 columns and 6,204 rows.\n\nIt consists of the populations of 222 countries, across 6 continents.\nIt also shows the percentages of the Old and Young subsets of the populations.\n\n\nn_distinct(globalPop$Country)\n\n[1] 222\n\nn_distinct(globalPop$Continent)\n\n[1] 6"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03B.html#animated-data-visualisation-gganimate-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03B.html#animated-data-visualisation-gganimate-methods",
    "title": "Hands-on Exercise 3B",
    "section": "",
    "text": "The gganimate package extends the Grammar of Graphics as implemented by ggplot2 to include the description of animation. It does this by providing a range of new grammar classes that can be added to the plot object to customise how it should change with time:\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_*()/exit_*() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\n\n\nA static population bubble plot is created using basic ggplot2 functions.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') \n\n\n\n\n\n\n\nTo create an animated population bubble plot, the transition_time() function in the gganimate package is used to create transition through distinct states in time (i.e. Year), and the ease_aes() function in the gganimate package is used to control the easing of aesthetics. The default is ‘linear’. Other easing methods are quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03B.html#animated-data-visualisation-plotly-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03B.html#animated-data-visualisation-plotly-methods",
    "title": "Hands-on Exercise 3B",
    "section": "",
    "text": "Both the plot_ly() and ggplotly() functions in the plotly package can be used to support key frame animations through the “frame” aesthetic argument. They also support the “ids” aesthetic argument to ensure smooth transitions between objects with the same id (which helps facilitate object constancy).\n\n\nAn animated population bubble plot is created using the ggplotly() function to convert the static graphic object into an animated svg object.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\ngg = ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nggplotly(gg)\n\n\n\n\nAlthough the “show.legend” argument was set as FALSE, the legend still appeared on the plot. To overcome this problem, the theme() function with the “legend.position” argument set to none is used.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\ngg = ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') + \n  theme(legend.position='none')\n\nggplotly(gg)\n\n\n\n\n\n\n\nAn animated population bubble plot is created using the plot_ly() function.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nbp = globalPop %&gt;%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers') %&gt;%\n  layout(showlegend = FALSE)\n\nbp"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03B.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03B.html#references",
    "title": "Hands-on Exercise 3B",
    "section": "",
    "text": "gganimate and example.\nBuilding An Animation Step-by-Step with gganimate.\nCreating A Composite gif with Multiple gganimate Panels.\n\n~~~ End of Hands-on Exercise 3B ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04B.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04B.html",
    "title": "Hands-on Exercise 4B",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 10 of the R for Visual Analytics book.\nThe learning outcomes are:\n\nUse the ggstatsplot package to create visual graphics with rich statistical information;\n\nIt is an extension of the ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\nThis provides alternative statistical inference methods by default.\nIt also follows the best practices for statistical reporting. For all statistical tests reported in the plots, the default template abides by the APA standard for statistical reporting.\n\nUse the performance package to visualise model diagnostics; and\nUse the parameters package to visualise model parameters.\n\n\n\n\n\n\nIn this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data; and\nreadxl for reading Excel files;\nggstatsplot for plotting statistics;\nperformance for assessment of model performance;\nparameters for processing of model parameters; and\nsee for model visualisation in ggplot2 and easystats.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, readxl,\n               ggstatsplot, \n               performance, parameters,\n               see)\n\n\n\n\nThe datasets for this hands-on exercise are imported into the R environment:\n\nUsing the read_csv() function in the readr package and stored as the R object, exam.\nUsing the read_xls() function in the readxl package and stored as the R object, car_resale.\n\n\nexam = read_csv(\"data/Exam_data.csv\")\ncar_resale = read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\n\nThe tibble data frame, exam, has 7 columns and 322 rows.\n\nIt consists of the year-end examination grades of a cohort of 322 Primary 3 students from a local school.\nThe 7 variables/attributes are:\n\nCategorical: ID, CLASS, GENDER, and RACE.\nContinuous: MATHS, ENGLISH, and SCIENCE.\n\n\nThe tibble data frame, car_resale, has 38 columns and 1,436 rows.\n\nhead(car_resale)\n\n# A tibble: 6 × 38\n     Id Model      Price Age_08_04 Mfg_Month Mfg_Year    KM Quarterly_Tax Weight\n  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n1    81 TOYOTA Co… 18950        25         8     2002 20019           100   1180\n2     1 TOYOTA Co… 13500        23        10     2002 46986           210   1165\n3     2 TOYOTA Co… 13750        23        10     2002 72937           210   1165\n4     3  TOYOTA C… 13950        24         9     2002 41711           210   1165\n5     4 TOYOTA Co… 14950        26         7     2002 48000           210   1165\n6     5 TOYOTA Co… 13750        30         3     2002 38500           210   1170\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;,\n#   Radio &lt;dbl&gt;, Mistlamps &lt;dbl&gt;, Sport_Model &lt;dbl&gt;, Backseat_Divider &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nThe gghistostats() function is used to to build a plot of a one-sample test for English scores.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nset.seed(1234)\n\ngghistostats(data = exam,\n             x = ENGLISH,\n             type = \"bayes\",\n             test.value = 60,\n             xlab = \"English Scores\")\n\n\n\n\nDefault information:\n\nStatistical details;\nBayes factor;\nSample sizes; and\nDistribution summary.\n\n\nObservation: Using the test value of 60, the value of LOG(BF01) of -31.45 shows that there is strong evidence that the null hypothesis (that the mean value is 60) is rejected. In fact, the mean score is higher than 60.\n\n\n\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favour of one theory among two competing theories.\nThe Bayes factor is a way to evaluate the data in favour of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favour of a given hypothesis.\nWhen comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes factor is often written as B10.\nThe Schwarz criterion is one of the easiest ways to calculate a rough approximation of the Bayes factor.\n\n\n\nA Bayes factor can be any positive number. One of the most common interpretations was first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013.\n\n\n\nThe ggbetweenstats() is used to build a plot of a two-sample mean test of Maths scores by gender.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(data = exam,\n               x = GENDER, \n               y = MATHS,\n               type = \"np\",\n               messages = FALSE)\n\n\n\n\nDefault information:\n\nStatistical details;\nBayes factor;\nSample sizes; and\nDistribution summary.\n\n\nObservation: The p-value of 0.91 means that there is insufficient evidence to reject the null hypothesis that the mean Maths scores between the two genders are the same.\n\n\n\n\nThe ggbetweenstats() is used to build a plot for a one-way ANOVA test on English scores by race.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(data = exam,\n               x = RACE, \n               y = ENGLISH,\n               type = \"p\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE)\n\n\n\n\nFor “pairwise.display” argument:\n\n“ns” means only non-significant;\n“s” means only significant; and\n“all” means everything.\n\n\nObservation: The p-value of 1.71e-04 means that there is sufficient evidence to reject the null hypothesis that the mean English scores between the four races are the same.\n\n\n\n\nThe ggscatterstats() function is used to build a plot for a significant test of correlation between Maths scores and English scores.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggscatterstats(data = exam,\n               x = MATHS,\n               y = ENGLISH,\n               marginal = FALSE)\n\n\n\n\n\nObservation: The p-value of 1.70e-83 means that there is sufficient evidence to reject the null hypothesis that there is no correlation between Maths scores and English scores.\n\n\n\n\nThe Maths scores is binned into a 4-class variable using the cut() function in the base R package.\n\nexam1 = exam %&gt;% \n  mutate(MATHS_bins = cut(MATHS, \n                          breaks = c(0,60,75,85,100)))\n\nThe ggbarstats() function is used to build a plot for a significant test of association.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)\n\n\n\n\n\nObservation: The p-value of 0.79 means that there is insufficient evidence to reject the null hypothesis that there is no association between gender and binned Maths scores.\n\n\n\n\n\nModel diagnostic and model parameters are visualised using the performance and parameters packages. The Toyota Corolla case study is used. The goal is to build a model to discover factors affecting prices of used cars by considering a set of explanatory variables.\n\n\nA multiple linear regression model is calibrated using the lm() function of the stats package.\n\nmodel = lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n\nThe check_collinearity() function is used to check for multicollinearity amongst the factors.\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\nplot(check_collinearity(model))\n\n\n\n\n\nObservation: There is high collinearity between age of car and manufacturing year, which is expected.\n\n\n\n\nThe check_normality() function is used to check the normality assumption regarding the residuals in the model.\n\nmodel1 = lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\nplot(check_normality(model1))\n\n\n\n\n\nObservation: Most residuals are distributed around zero, which implies that the model captures the main patterns and sources of variation in the data, and the errors are random and independent.\n\n\n\n\nThe check_heteroscedasticity() function is used to check for the homogeneity of variances in the model.\n\nplot(check_heteroscedasticity(model1))\n\n\n\n\n\nObservation: There is heteroscedasticity, i.e., the variances are not homogeneous.\n\n\n\n\nThe complete check can be performed using the check_model() function.\n\ncheck_model(model1)\n\n\n\n\n\n\n\n\nThe parameters of a regression model can be visualised using the plot() function in the see package, or the ggcoefstats() function in the ggstatsplot package.\n\n\n\nplot(parameters(model1))\n\n\n\n\n\n\n\n\nggcoefstats(model1, \n            output = \"plot\")\n\n\n\n\n~~~ End of Hands-on Exercise 4B ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04B.html#overview-and-learning-outcomes",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04B.html#overview-and-learning-outcomes",
    "title": "Hands-on Exercise 4B",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 10 of the R for Visual Analytics book.\nThe learning outcomes are:\n\nUse the ggstatsplot package to create visual graphics with rich statistical information;\n\nIt is an extension of the ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\nThis provides alternative statistical inference methods by default.\nIt also follows the best practices for statistical reporting. For all statistical tests reported in the plots, the default template abides by the APA standard for statistical reporting.\n\nUse the performance package to visualise model diagnostics; and\nUse the parameters package to visualise model parameters."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04B.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04B.html#getting-started",
    "title": "Hands-on Exercise 4B",
    "section": "",
    "text": "In this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data; and\nreadxl for reading Excel files;\nggstatsplot for plotting statistics;\nperformance for assessment of model performance;\nparameters for processing of model parameters; and\nsee for model visualisation in ggplot2 and easystats.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, readxl,\n               ggstatsplot, \n               performance, parameters,\n               see)\n\n\n\n\nThe datasets for this hands-on exercise are imported into the R environment:\n\nUsing the read_csv() function in the readr package and stored as the R object, exam.\nUsing the read_xls() function in the readxl package and stored as the R object, car_resale.\n\n\nexam = read_csv(\"data/Exam_data.csv\")\ncar_resale = read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\n\nThe tibble data frame, exam, has 7 columns and 322 rows.\n\nIt consists of the year-end examination grades of a cohort of 322 Primary 3 students from a local school.\nThe 7 variables/attributes are:\n\nCategorical: ID, CLASS, GENDER, and RACE.\nContinuous: MATHS, ENGLISH, and SCIENCE.\n\n\nThe tibble data frame, car_resale, has 38 columns and 1,436 rows.\n\nhead(car_resale)\n\n# A tibble: 6 × 38\n     Id Model      Price Age_08_04 Mfg_Month Mfg_Year    KM Quarterly_Tax Weight\n  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n1    81 TOYOTA Co… 18950        25         8     2002 20019           100   1180\n2     1 TOYOTA Co… 13500        23        10     2002 46986           210   1165\n3     2 TOYOTA Co… 13750        23        10     2002 72937           210   1165\n4     3  TOYOTA C… 13950        24         9     2002 41711           210   1165\n5     4 TOYOTA Co… 14950        26         7     2002 48000           210   1165\n6     5 TOYOTA Co… 13750        30         3     2002 38500           210   1170\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;,\n#   Radio &lt;dbl&gt;, Mistlamps &lt;dbl&gt;, Sport_Model &lt;dbl&gt;, Backseat_Divider &lt;dbl&gt;, …"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04B.html#using-ggstatsplot-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04B.html#using-ggstatsplot-methods",
    "title": "Hands-on Exercise 4B",
    "section": "",
    "text": "The gghistostats() function is used to to build a plot of a one-sample test for English scores.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nset.seed(1234)\n\ngghistostats(data = exam,\n             x = ENGLISH,\n             type = \"bayes\",\n             test.value = 60,\n             xlab = \"English Scores\")\n\n\n\n\nDefault information:\n\nStatistical details;\nBayes factor;\nSample sizes; and\nDistribution summary.\n\n\nObservation: Using the test value of 60, the value of LOG(BF01) of -31.45 shows that there is strong evidence that the null hypothesis (that the mean value is 60) is rejected. In fact, the mean score is higher than 60.\n\n\n\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favour of one theory among two competing theories.\nThe Bayes factor is a way to evaluate the data in favour of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favour of a given hypothesis.\nWhen comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes factor is often written as B10.\nThe Schwarz criterion is one of the easiest ways to calculate a rough approximation of the Bayes factor.\n\n\n\nA Bayes factor can be any positive number. One of the most common interpretations was first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013.\n\n\n\nThe ggbetweenstats() is used to build a plot of a two-sample mean test of Maths scores by gender.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(data = exam,\n               x = GENDER, \n               y = MATHS,\n               type = \"np\",\n               messages = FALSE)\n\n\n\n\nDefault information:\n\nStatistical details;\nBayes factor;\nSample sizes; and\nDistribution summary.\n\n\nObservation: The p-value of 0.91 means that there is insufficient evidence to reject the null hypothesis that the mean Maths scores between the two genders are the same.\n\n\n\n\nThe ggbetweenstats() is used to build a plot for a one-way ANOVA test on English scores by race.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(data = exam,\n               x = RACE, \n               y = ENGLISH,\n               type = \"p\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE)\n\n\n\n\nFor “pairwise.display” argument:\n\n“ns” means only non-significant;\n“s” means only significant; and\n“all” means everything.\n\n\nObservation: The p-value of 1.71e-04 means that there is sufficient evidence to reject the null hypothesis that the mean English scores between the four races are the same.\n\n\n\n\nThe ggscatterstats() function is used to build a plot for a significant test of correlation between Maths scores and English scores.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggscatterstats(data = exam,\n               x = MATHS,\n               y = ENGLISH,\n               marginal = FALSE)\n\n\n\n\n\nObservation: The p-value of 1.70e-83 means that there is sufficient evidence to reject the null hypothesis that there is no correlation between Maths scores and English scores.\n\n\n\n\nThe Maths scores is binned into a 4-class variable using the cut() function in the base R package.\n\nexam1 = exam %&gt;% \n  mutate(MATHS_bins = cut(MATHS, \n                          breaks = c(0,60,75,85,100)))\n\nThe ggbarstats() function is used to build a plot for a significant test of association.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)\n\n\n\n\n\nObservation: The p-value of 0.79 means that there is insufficient evidence to reject the null hypothesis that there is no association between gender and binned Maths scores."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04B.html#visualising-models-using-performance-package",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04B.html#visualising-models-using-performance-package",
    "title": "Hands-on Exercise 4B",
    "section": "",
    "text": "Model diagnostic and model parameters are visualised using the performance and parameters packages. The Toyota Corolla case study is used. The goal is to build a model to discover factors affecting prices of used cars by considering a set of explanatory variables.\n\n\nA multiple linear regression model is calibrated using the lm() function of the stats package.\n\nmodel = lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n\nThe check_collinearity() function is used to check for multicollinearity amongst the factors.\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\nplot(check_collinearity(model))\n\n\n\n\n\nObservation: There is high collinearity between age of car and manufacturing year, which is expected.\n\n\n\n\nThe check_normality() function is used to check the normality assumption regarding the residuals in the model.\n\nmodel1 = lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\nplot(check_normality(model1))\n\n\n\n\n\nObservation: Most residuals are distributed around zero, which implies that the model captures the main patterns and sources of variation in the data, and the errors are random and independent.\n\n\n\n\nThe check_heteroscedasticity() function is used to check for the homogeneity of variances in the model.\n\nplot(check_heteroscedasticity(model1))\n\n\n\n\n\nObservation: There is heteroscedasticity, i.e., the variances are not homogeneous.\n\n\n\n\nThe complete check can be performed using the check_model() function.\n\ncheck_model(model1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04B.html#visualising-regression-parameters",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04B.html#visualising-regression-parameters",
    "title": "Hands-on Exercise 4B",
    "section": "",
    "text": "The parameters of a regression model can be visualised using the plot() function in the see package, or the ggcoefstats() function in the ggstatsplot package.\n\n\n\nplot(parameters(model1))\n\n\n\n\n\n\n\n\nggcoefstats(model1, \n            output = \"plot\")\n\n\n\n\n~~~ End of Hands-on Exercise 4B ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04D.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04D.html",
    "title": "Hands-on Exercise 4D",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 12 of the R for Visual Analytics book.\nA funnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities. In this hands-on exercise, the learning outcomes are:\n\nPlot a funnel plot using the funnelPlotR package;\nPlot a static funnel plot using the ggplot2 package; and\nPlot an interactive funnel plot using both plotly R and ggplot2 packages.\n\n\n\n\n\n\nIn this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nFunnelPlotR for creating funnel plot;\nknitr for building static html table; and\nplotly for plotting interactive statistical graphs.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, FunnelPlotR,\n               knitr, plotly)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, covid19.\n\ncovid19 = read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)\n\nThe tibble data frame, covid19, has 7 columns and 267 rows.\n\nhead(covid19)\n\n# A tibble: 6 × 7\n  `Sub-district ID` City        District `Sub-district` Positive Recovered Death\n              &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;    &lt;fct&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1        3172051003 JAKARTA UT… PADEMAN… ANCOL              1776      1691    26\n2        3173041007 JAKARTA BA… TAMBORA  ANGKE              1783      1720    29\n3        3175041005 JAKARTA TI… KRAMAT … BALE KAMBANG       2049      1964    31\n4        3175031003 JAKARTA TI… JATINEG… BALI MESTER         827       797    13\n5        3175101006 JAKARTA TI… CIPAYUNG BAMBU APUS         2866      2792    27\n6        3174031002 JAKARTA SE… MAMPANG… BANGKA             1828      1757    26\n\n\n\n\n\n\nThe FunnelPlotR package uses ggplot to generate funnel plots. It requires a “numerator” (events of interest), “denominator” (population to be considered) and “group”.\nThe key arguments selected for customisation are:\n\n“limit” to set plot limits (95 or 99);\n“label_outliers” to label outliers (true or false);\n“Poisson_limits” to add Poisson limits to the plot;\n“OD_adjust” to add overdispersed limits to the plot;\n“xrange” and “yrange” to specify the range to display for axes, acts like a zoom function; and\nOther aesthetic components such as graph title, axis labels, etc.\n\n\n\nThe funnel_plot function is used to create the basic plot, with the following arguments:\n\nThe “group” argument is different from that in the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If City is chosen, there are only six data points.\nBy default, “data_type” argument is “SR”.\nThe “limit” argument states the plot limits. The accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\nPlotCode\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\nfunnel_plot(\n  numerator = covid19$Positive,\n  denominator = covid19$Death,\n  group = covid19$`Sub-district`)\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. Plot is adjusted for overdispersion.\n\n\n\nThe funnel plot is improved by changing the “data_type” argument to “PR” (i.e., proportions), and stating the “xrange” and “yrange” arguments to set the range of the axes.\n\nPlotCode\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  xrange = c(0, 6500),  #&lt;&lt;\n  yrange = c(0, 0.05)   #&lt;&lt;\n)\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. Plot is adjusted for overdispersion.\n\n\n\nThe funnel plot is further improved by adding the “label” argument and setting it as “NA” to remove the default label outliers feature, as well as including the “title”, “x_label”, and “y_label” arguments to add a plot title and axes titles.\n\nPlotCode\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by\\nCumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. Plot is adjusted for overdispersion.\n\n\n\n\nA funnel plot can also be created using ggplot2.\n\n\nFirst, the cumulative death rate and standard error of the cumulative death rate is derived.\n\ndf = covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nNext, the fit.mean is computed using the weighted.mean() function in the stats package.\n\nfit.mean = weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n\nThen, the lower and upper limits for a 95% confidence interval is computed using the fit.mean.\n\nnumber.seq = seq(1, max(df$Positive), 1)\nnumber.ll95 = fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 = fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 = fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 = fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n\nA static funnel plot is created using ggplot2 functions.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\np = ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n\nInteractivity is then added to the funnel plot using the ggplotly() function in the plotly package.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nfp_ggplotly = ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly\n\n\n\n\n\n\n\n\n\nfunnelPlotR package.\nFunnel Plots for Indirectly-standardised ratios.\nChanging funnel plot options\nggplot2 package.\n\n~~~ End of Hands-on Exercise 4D ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04D.html#overview-and-learning-outcomes",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04D.html#overview-and-learning-outcomes",
    "title": "Hands-on Exercise 4D",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 12 of the R for Visual Analytics book.\nA funnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities. In this hands-on exercise, the learning outcomes are:\n\nPlot a funnel plot using the funnelPlotR package;\nPlot a static funnel plot using the ggplot2 package; and\nPlot an interactive funnel plot using both plotly R and ggplot2 packages."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04D.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04D.html#getting-started",
    "title": "Hands-on Exercise 4D",
    "section": "",
    "text": "In this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nFunnelPlotR for creating funnel plot;\nknitr for building static html table; and\nplotly for plotting interactive statistical graphs.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, FunnelPlotR,\n               knitr, plotly)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, covid19.\n\ncovid19 = read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)\n\nThe tibble data frame, covid19, has 7 columns and 267 rows.\n\nhead(covid19)\n\n# A tibble: 6 × 7\n  `Sub-district ID` City        District `Sub-district` Positive Recovered Death\n              &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;    &lt;fct&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1        3172051003 JAKARTA UT… PADEMAN… ANCOL              1776      1691    26\n2        3173041007 JAKARTA BA… TAMBORA  ANGKE              1783      1720    29\n3        3175041005 JAKARTA TI… KRAMAT … BALE KAMBANG       2049      1964    31\n4        3175031003 JAKARTA TI… JATINEG… BALI MESTER         827       797    13\n5        3175101006 JAKARTA TI… CIPAYUNG BAMBU APUS         2866      2792    27\n6        3174031002 JAKARTA SE… MAMPANG… BANGKA             1828      1757    26"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04D.html#funnelplotr-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04D.html#funnelplotr-methods",
    "title": "Hands-on Exercise 4D",
    "section": "",
    "text": "The FunnelPlotR package uses ggplot to generate funnel plots. It requires a “numerator” (events of interest), “denominator” (population to be considered) and “group”.\nThe key arguments selected for customisation are:\n\n“limit” to set plot limits (95 or 99);\n“label_outliers” to label outliers (true or false);\n“Poisson_limits” to add Poisson limits to the plot;\n“OD_adjust” to add overdispersed limits to the plot;\n“xrange” and “yrange” to specify the range to display for axes, acts like a zoom function; and\nOther aesthetic components such as graph title, axis labels, etc.\n\n\n\nThe funnel_plot function is used to create the basic plot, with the following arguments:\n\nThe “group” argument is different from that in the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If City is chosen, there are only six data points.\nBy default, “data_type” argument is “SR”.\nThe “limit” argument states the plot limits. The accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\nPlotCode\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\nfunnel_plot(\n  numerator = covid19$Positive,\n  denominator = covid19$Death,\n  group = covid19$`Sub-district`)\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. Plot is adjusted for overdispersion.\n\n\n\nThe funnel plot is improved by changing the “data_type” argument to “PR” (i.e., proportions), and stating the “xrange” and “yrange” arguments to set the range of the axes.\n\nPlotCode\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  xrange = c(0, 6500),  #&lt;&lt;\n  yrange = c(0, 0.05)   #&lt;&lt;\n)\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. Plot is adjusted for overdispersion.\n\n\n\nThe funnel plot is further improved by adding the “label” argument and setting it as “NA” to remove the default label outliers feature, as well as including the “title”, “x_label”, and “y_label” arguments to add a plot title and axes titles.\n\nPlotCode\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by\\nCumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. Plot is adjusted for overdispersion."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04D.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04D.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "title": "Hands-on Exercise 4D",
    "section": "",
    "text": "A funnel plot can also be created using ggplot2.\n\n\nFirst, the cumulative death rate and standard error of the cumulative death rate is derived.\n\ndf = covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nNext, the fit.mean is computed using the weighted.mean() function in the stats package.\n\nfit.mean = weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n\nThen, the lower and upper limits for a 95% confidence interval is computed using the fit.mean.\n\nnumber.seq = seq(1, max(df$Positive), 1)\nnumber.ll95 = fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 = fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 = fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 = fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n\nA static funnel plot is created using ggplot2 functions.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\np = ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n\nInteractivity is then added to the funnel plot using the ggplotly() function in the plotly package.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nfp_ggplotly = ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04D.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04D.html#references",
    "title": "Hands-on Exercise 4D",
    "section": "",
    "text": "funnelPlotR package.\nFunnel Plots for Indirectly-standardised ratios.\nChanging funnel plot options\nggplot2 package.\n\n~~~ End of Hands-on Exercise 4D ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05B.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05B.html",
    "title": "Hands-on Exercise 5B",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 6 of the R for Visual Analytics book.\nCorrelation coefficient is a popular statistic used to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0.\n\nA value of 1 shows a perfect linear relationship between the two variables.\nA value of -1.0 shows a perfect inverse relationship between the two variables.\nA value of 0.0 shows no linear relationship between the two variables.\n\nWhen multivariate data are used, the correlation coefficeints of the pair comparisons are displayed in a table form known as a correlation matrix or a scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix:\n\nReveal the relationship between high-dimensional variables pair-wisely.\nInput into other analyses - e.g., correlation matrices commonly used as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nCheck other analyses - e.g., a high amount of correlations in a linear regression suggests that its estimates will be unreliable.\n\nWhen the dataset is large, both in terms of the number of observations and the number of variables, a corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude; and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception.\n\nThe learning outcomes are:\n\nCreate correlation matrix using the pairs() function in the graphics package.\nPlot corrgram using corrplot package.\nCreate an interactive correlation matrix using the plotly package.\n\n\n\n\n\n\nIn this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nggstatsplot for plotting statistics; and\ncorrplot for plotting correlogram.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, ggstatsplot,\n               corrplot)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, wine. It contains data regarding the Wine Quality Data Set of the UCI Machine Learning Repository.\n\nwine = read_csv(\"data/wine_quality.csv\")\n\nThe tibble data frame, wine, has 13 columns and 6,497 rows. Besides quality and type, the rest of the variables are numerical and continuous data type.\n\n\n\n\n\n\nThe pairs() function in the graphics package is used to plot a scatter plot matrix (11 by 11).\n\npairs(wine[,1:11])\n\n\n\n\nThe required input of the pairs() function can be a matrix or data frame. In the plot below, columns 2 to 12 of wine is used to build the scatterplot matrix. The variables are: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, and alcohol.\n\npairs(wine[,2:12])\n\n\n\n\n\n\n\nThe plot may be customised via the arguments in the pairs() function. For example, it is a common practice to show either the upper half or lower half of the correlation matrix instead of both given that it is symmetric.\n\nLower PanelUpper Panel\n\n\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\n\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\n\n\n\n\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, the panel.cor() function will be used. This will also show higher correlations in a larger font.\n\npanel.cor = function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr = par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr = abs(cor(x, y, use=\"complete.obs\"))\ntxt = format(c(r, 0.123456789), digits=digits)[1]\ntxt = paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor = 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)\n\n\n\n\n\n\n\n\nA major limitation of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e. more than 500 observations). To over come this problem, a corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used.\nThe are at least three R packages provide functions to plot corrgram:\n\ncorrgram\nellipse; and\ncorrplot.\n\nAlso, some R packages like ggstatsplot package also provides functions for building corrgram.\n\n\nOn of the advantage of using the ggcorrmat() function in the ggstatsplot package over other methods to visualise a correlation matrix is its ability to provide a comprehensive and yet professional statistical report.\n\nggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\n\n\nA title and a subtitle are added to enhance the plot. Also, the matrix is rearranged in order.\n\nggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\n\nNote:\n\n“cor.vars” argument is used to compute the correlation matrix needed to build the corrgram.\n“ggcorrplot.args” argument provides additional (mostly aesthetic) arguments that will be passed to the function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\n\nAlso, specific components of the plot such as the font size of the x-axis, y-axis, and the statistical report can be controlled as follows.\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))\n\n\n\n\nSince the ggstasplot package is an extension of ggplot2, it also supports faceting. This feature is available via the grouped_ggcorrmat() function.\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"))\n\n\n\n\n\nNote:\n\n“grouping.var” argument is for building a facet plot.\nBehind the function, the patchwork package is used to create the multiplot. The “plotgrid.args” argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, the “annotation.args” argument is calling the plot annotation arguments of patchwork package.\n\n\n\n\n\n\nAn Introduction to corrplot Package provides a basic understanding of the corrplot package.\n\n\nThe cor() function in the stats package is used to compute the correlation matrix of wine.\n\nwine.cor = cor(wine[, 1:11])\n\nThen, the corrplot() function in the corrplot package is used to plot the corrgram using the default settings.\n\ncorrplot(wine.cor)\n\n\n\n\n\nNote:\n\nDefault visual object used to plot the corrgram is circle.\nDefault layout is a symmetric matrix.\nDefault colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients.\nIntensity of the colour (i.e., saturation) is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. Lighter colours indicates relatively weaker linear relationship.\n\n\n\n\n\nIn the corrplot package, there are seven visual geometrics (parameter method) that can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle, which can be changed via the “method” argument.\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\n\nThe corrplor() function supports three layout types, namely: “full”, “upper” or “lower”. The default is “full” which display the full matrix. The default setting can be changed via the “type” argument.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, the “diag” and “tl.col” arguments are used to turn off the diagonal cells and change the axis text label colour to black colour respectively.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\nWith the corrplot package, it is possible to design corrgram with mixed visual matrix in one half and numerical matrix in the other half. In order to create a coorgram with mixed layout, the corrplot.mixed() function, a wrapped function for mixed visualisation style, will be used.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\nNote:\n\nThe “lower” and “upper” arguments are used to define the visualisation method used. In this case ellipse is used to map the lower half of the corrgram and numerical matrix (i.e. number) is used to map the upper half of the corrgram.\nThe “tl.pos” argument is used to specify the placement of the axis label.\nThe “diag” argument is used to specify the glyph on the principal diagonal of the corrgram.\n\n\n\n\n\nIn statistical analysis, we are also interested to know which pair of variables with statistically significant correlation coefficients.\nA corrgram is combined with the significant test. The cor.mtest() function is used to compute the p-values and confidence interval for each pair of variables. Then, it is added to the “p.mat” argument.\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\nThe corrgram reveals that not all correlation pairs are statistically significant. For example the correlation between total sulfur dioxide and free surfur dioxide is statistically significant at significant level of 0.1 but not the pair between total sulfur dioxide and citric acid.\n\n\n\nMatrix reorder is very important for mining the hidden structure and patterns in a corrgram.\nBy default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be overwritten using the “order“ argument.\nCurrently, the corrplot package support four sorting methods, they are:\n\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n\n“alphabet” for alphabetical order.\n\nMore algorithms can be found in the seriation package.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\nIf setting “order” argument as “hclust”, the plot can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)\n\n\n\n\n\n\n\n\n\nMichael Friendly (2002). “Corrgrams: Exploratory displays for correlation matrices”. The American Statistician, 56, 316–324.\nD.J. Murdoch, E.D. Chow (1996). “A graphical display of large correlation matrices”. The American Statistician, 50, 178–180.\nggscatmat() and ggpair``s() functions of GGally.\ncorrgram calculates correlation of variables and displays the results graphically. Included panel functions can display points, shading, ellipses, and correlation values with confidence intervals.\n\n~~~ End of Hands-on Exercise 5B ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05B.html#overview-and-learning-outcomes",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05B.html#overview-and-learning-outcomes",
    "title": "Hands-on Exercise 5B",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 6 of the R for Visual Analytics book.\nCorrelation coefficient is a popular statistic used to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0.\n\nA value of 1 shows a perfect linear relationship between the two variables.\nA value of -1.0 shows a perfect inverse relationship between the two variables.\nA value of 0.0 shows no linear relationship between the two variables.\n\nWhen multivariate data are used, the correlation coefficeints of the pair comparisons are displayed in a table form known as a correlation matrix or a scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix:\n\nReveal the relationship between high-dimensional variables pair-wisely.\nInput into other analyses - e.g., correlation matrices commonly used as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nCheck other analyses - e.g., a high amount of correlations in a linear regression suggests that its estimates will be unreliable.\n\nWhen the dataset is large, both in terms of the number of observations and the number of variables, a corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude; and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception.\n\nThe learning outcomes are:\n\nCreate correlation matrix using the pairs() function in the graphics package.\nPlot corrgram using corrplot package.\nCreate an interactive correlation matrix using the plotly package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05B.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05B.html#getting-started",
    "title": "Hands-on Exercise 5B",
    "section": "",
    "text": "In this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nggstatsplot for plotting statistics; and\ncorrplot for plotting correlogram.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, ggstatsplot,\n               corrplot)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, wine. It contains data regarding the Wine Quality Data Set of the UCI Machine Learning Repository.\n\nwine = read_csv(\"data/wine_quality.csv\")\n\nThe tibble data frame, wine, has 13 columns and 6,497 rows. Besides quality and type, the rest of the variables are numerical and continuous data type."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05B.html#building-correlation-matrix-pairs",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05B.html#building-correlation-matrix-pairs",
    "title": "Hands-on Exercise 5B",
    "section": "",
    "text": "The pairs() function in the graphics package is used to plot a scatter plot matrix (11 by 11).\n\npairs(wine[,1:11])\n\n\n\n\nThe required input of the pairs() function can be a matrix or data frame. In the plot below, columns 2 to 12 of wine is used to build the scatterplot matrix. The variables are: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, and alcohol.\n\npairs(wine[,2:12])\n\n\n\n\n\n\n\nThe plot may be customised via the arguments in the pairs() function. For example, it is a common practice to show either the upper half or lower half of the correlation matrix instead of both given that it is symmetric.\n\nLower PanelUpper Panel\n\n\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\n\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\n\n\n\n\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, the panel.cor() function will be used. This will also show higher correlations in a larger font.\n\npanel.cor = function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr = par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr = abs(cor(x, y, use=\"complete.obs\"))\ntxt = format(c(r, 0.123456789), digits=digits)[1]\ntxt = paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor = 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05B.html#visualising-correlation-matrix-ggcormat",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05B.html#visualising-correlation-matrix-ggcormat",
    "title": "Hands-on Exercise 5B",
    "section": "",
    "text": "A major limitation of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e. more than 500 observations). To over come this problem, a corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used.\nThe are at least three R packages provide functions to plot corrgram:\n\ncorrgram\nellipse; and\ncorrplot.\n\nAlso, some R packages like ggstatsplot package also provides functions for building corrgram.\n\n\nOn of the advantage of using the ggcorrmat() function in the ggstatsplot package over other methods to visualise a correlation matrix is its ability to provide a comprehensive and yet professional statistical report.\n\nggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\n\n\nA title and a subtitle are added to enhance the plot. Also, the matrix is rearranged in order.\n\nggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\n\nNote:\n\n“cor.vars” argument is used to compute the correlation matrix needed to build the corrgram.\n“ggcorrplot.args” argument provides additional (mostly aesthetic) arguments that will be passed to the function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\n\nAlso, specific components of the plot such as the font size of the x-axis, y-axis, and the statistical report can be controlled as follows.\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))\n\n\n\n\nSince the ggstasplot package is an extension of ggplot2, it also supports faceting. This feature is available via the grouped_ggcorrmat() function.\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"))\n\n\n\n\n\nNote:\n\n“grouping.var” argument is for building a facet plot.\nBehind the function, the patchwork package is used to create the multiplot. The “plotgrid.args” argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, the “annotation.args” argument is calling the plot annotation arguments of patchwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05B.html#visualising-correlation-matrix-corrplot-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05B.html#visualising-correlation-matrix-corrplot-package",
    "title": "Hands-on Exercise 5B",
    "section": "",
    "text": "An Introduction to corrplot Package provides a basic understanding of the corrplot package.\n\n\nThe cor() function in the stats package is used to compute the correlation matrix of wine.\n\nwine.cor = cor(wine[, 1:11])\n\nThen, the corrplot() function in the corrplot package is used to plot the corrgram using the default settings.\n\ncorrplot(wine.cor)\n\n\n\n\n\nNote:\n\nDefault visual object used to plot the corrgram is circle.\nDefault layout is a symmetric matrix.\nDefault colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients.\nIntensity of the colour (i.e., saturation) is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. Lighter colours indicates relatively weaker linear relationship.\n\n\n\n\n\nIn the corrplot package, there are seven visual geometrics (parameter method) that can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle, which can be changed via the “method” argument.\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\n\nThe corrplor() function supports three layout types, namely: “full”, “upper” or “lower”. The default is “full” which display the full matrix. The default setting can be changed via the “type” argument.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, the “diag” and “tl.col” arguments are used to turn off the diagonal cells and change the axis text label colour to black colour respectively.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\nWith the corrplot package, it is possible to design corrgram with mixed visual matrix in one half and numerical matrix in the other half. In order to create a coorgram with mixed layout, the corrplot.mixed() function, a wrapped function for mixed visualisation style, will be used.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\nNote:\n\nThe “lower” and “upper” arguments are used to define the visualisation method used. In this case ellipse is used to map the lower half of the corrgram and numerical matrix (i.e. number) is used to map the upper half of the corrgram.\nThe “tl.pos” argument is used to specify the placement of the axis label.\nThe “diag” argument is used to specify the glyph on the principal diagonal of the corrgram.\n\n\n\n\n\nIn statistical analysis, we are also interested to know which pair of variables with statistically significant correlation coefficients.\nA corrgram is combined with the significant test. The cor.mtest() function is used to compute the p-values and confidence interval for each pair of variables. Then, it is added to the “p.mat” argument.\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\nThe corrgram reveals that not all correlation pairs are statistically significant. For example the correlation between total sulfur dioxide and free surfur dioxide is statistically significant at significant level of 0.1 but not the pair between total sulfur dioxide and citric acid.\n\n\n\nMatrix reorder is very important for mining the hidden structure and patterns in a corrgram.\nBy default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be overwritten using the “order“ argument.\nCurrently, the corrplot package support four sorting methods, they are:\n\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n\n“alphabet” for alphabetical order.\n\nMore algorithms can be found in the seriation package.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\nIf setting “order” argument as “hclust”, the plot can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05B.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05B.html#references",
    "title": "Hands-on Exercise 5B",
    "section": "",
    "text": "Michael Friendly (2002). “Corrgrams: Exploratory displays for correlation matrices”. The American Statistician, 56, 316–324.\nD.J. Murdoch, E.D. Chow (1996). “A graphical display of large correlation matrices”. The American Statistician, 50, 178–180.\nggscatmat() and ggpair``s() functions of GGally.\ncorrgram calculates correlation of variables and displays the results graphically. Included panel functions can display points, shading, ellipses, and correlation values with confidence intervals.\n\n~~~ End of Hands-on Exercise 5B ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05D.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05D.html",
    "title": "Hands-on Exercise 5D",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 15 of the R for Visual Analytics book.\nA parallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables and seeing the relationships between them. For example, the variables contributing to a happiness index.\nParallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualisations. As pointed out by Stephen Few (2006), “This certainly isn’t a chart that you would present to the board of directors or place on your web site for the general public. In fact, the strength of parallel coordinates isn’t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.” For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nThe learning outcomes are:\n\nPlot a static parallel coordinates plots using the ggparcoord() function in the GGally package;\nPlot an interactive parallel coordinates plots using the parcoords package; and\nPlot an interactive parallel coordinates plots using the parallelPlot package.\n\n\n\n\n\n\nIn this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nGGally (ggplot2 extension) for plotting pairwise plot matrix, scatterplot plot matrix, parallel coordinates plot, survival plot, and several functions to plot networks.\nparallelPlot for plotting interactive parallel coordinates plot.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, GGally,\n               parallelPlot)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, wh. The data is from the World Happiness 2018 report.\n\nwh = read_csv(\"data/WHData-2018.csv\")\n\nThe tibble data frame, wh, has 12 columns and 156 rows. Other than the “Country” and “Region” variables, the remaining variables are continuous numerical data.\n\n\n\n\nIn this sub-section, the ggparcoord() function in the GGally package is used to plot static parallel coordinates plots.\n\n\nA basic static plot is plotted below.\n\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\n\nNote: Only two arguments, “data” and “columns” are used. The “data” argument is used to map the data object (i.e. wh) and the “columns” argument is used to select the columns for preparing the parallel coordinates plot.\n\n\n\n\nThe basic parallel coordinates failed to reveal any meaningful understanding of the World Happiness measures. Hence, further arguments would be added to the ggparcoord() function to reveal more insights.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\nNote:\n\nThe “groupColumn” argument is used to group the observations (i.e. parallel lines) using a single variable (i.e. “Region”) and colour the parallel coordinates lines by “Region”.\nThe “scale” argument is used to scale the variables in the parallel coordinates plot using the “uniminmax” method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nThe “alphaLines” argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nThe “boxplot” argument is set to “TRUE” to add the boxplot. The default is FALSE.\nThe “title” argument is used to provide a title for the parallel coordinates plot.\n\n\n\n\n\nSince the ggparcoord() function is developed by extending the ggplot2 package, some ggplot2 functions can be added when plotting a parallel coordinates plot.\nThe facet_wrap() function in the ggplot2 package is used below to plot 10 small multiple parallel coordinates plots, for each geographical region. However, one of the aesthetic defects of the plot is that some of the variable names overlap on the x-axis.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\nTo make the x-axis text labels readable, they are rotated by 30 degrees using the theme() function in the ggplot2 package. The “axis.text.x” argument is set with the value of “element_text(angle = 30)”.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\nRotating the x-axis text labels to 30 degrees causes them to overlap with the plot. This can be avoided by adjusting the text location using the “hjust” argument in the theme() function by setting the value of “element_text(angle = 30, hjust = 1)”.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))\n\n\n\n\n\n\n\n\nThe parallelPlot package is specially designed to plot parallel coordinates plots using the htmlwidgets package and d3.js (JavaScript library). In this sub-section, the parallelPlot package is used to build interactive parallel coordinates plots.\n\n\nThe parallelPlot() function in the parallelPlot package is used to make the basic plot below. Some of the axis labels are too long.\n\nwh = wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\n\n\n\nThe “rotateTitle” argument is added to avoid overlapping axis labels.\nOne of the useful interactive feature of a plot made with the parallelPlot() function is the ability to click on a variable of interest which would then show the different colour intensities across the plot.\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\nThe default blue colour scheme can be changed using the “continousCS” argument.\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\nThe histogram along the axis of each variable can be plotted using the “histoVisibility”.\n\nhistoVisibility = rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)\n\n\n\n\n\n~~~ End of Hands-on Exercise 5D ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05D.html#overview-and-learning-outcomes",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05D.html#overview-and-learning-outcomes",
    "title": "Hands-on Exercise 5D",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 15 of the R for Visual Analytics book.\nA parallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables and seeing the relationships between them. For example, the variables contributing to a happiness index.\nParallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualisations. As pointed out by Stephen Few (2006), “This certainly isn’t a chart that you would present to the board of directors or place on your web site for the general public. In fact, the strength of parallel coordinates isn’t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.” For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nThe learning outcomes are:\n\nPlot a static parallel coordinates plots using the ggparcoord() function in the GGally package;\nPlot an interactive parallel coordinates plots using the parcoords package; and\nPlot an interactive parallel coordinates plots using the parallelPlot package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05D.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05D.html#getting-started",
    "title": "Hands-on Exercise 5D",
    "section": "",
    "text": "In this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nGGally (ggplot2 extension) for plotting pairwise plot matrix, scatterplot plot matrix, parallel coordinates plot, survival plot, and several functions to plot networks.\nparallelPlot for plotting interactive parallel coordinates plot.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, GGally,\n               parallelPlot)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the read_csv() function in the readr package and stored as the R object, wh. The data is from the World Happiness 2018 report.\n\nwh = read_csv(\"data/WHData-2018.csv\")\n\nThe tibble data frame, wh, has 12 columns and 156 rows. Other than the “Country” and “Region” variables, the remaining variables are continuous numerical data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05D.html#plotting-static-parallel-coordinates-plot",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05D.html#plotting-static-parallel-coordinates-plot",
    "title": "Hands-on Exercise 5D",
    "section": "",
    "text": "In this sub-section, the ggparcoord() function in the GGally package is used to plot static parallel coordinates plots.\n\n\nA basic static plot is plotted below.\n\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\n\nNote: Only two arguments, “data” and “columns” are used. The “data” argument is used to map the data object (i.e. wh) and the “columns” argument is used to select the columns for preparing the parallel coordinates plot.\n\n\n\n\nThe basic parallel coordinates failed to reveal any meaningful understanding of the World Happiness measures. Hence, further arguments would be added to the ggparcoord() function to reveal more insights.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\nNote:\n\nThe “groupColumn” argument is used to group the observations (i.e. parallel lines) using a single variable (i.e. “Region”) and colour the parallel coordinates lines by “Region”.\nThe “scale” argument is used to scale the variables in the parallel coordinates plot using the “uniminmax” method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nThe “alphaLines” argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nThe “boxplot” argument is set to “TRUE” to add the boxplot. The default is FALSE.\nThe “title” argument is used to provide a title for the parallel coordinates plot.\n\n\n\n\n\nSince the ggparcoord() function is developed by extending the ggplot2 package, some ggplot2 functions can be added when plotting a parallel coordinates plot.\nThe facet_wrap() function in the ggplot2 package is used below to plot 10 small multiple parallel coordinates plots, for each geographical region. However, one of the aesthetic defects of the plot is that some of the variable names overlap on the x-axis.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\nTo make the x-axis text labels readable, they are rotated by 30 degrees using the theme() function in the ggplot2 package. The “axis.text.x” argument is set with the value of “element_text(angle = 30)”.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\nRotating the x-axis text labels to 30 degrees causes them to overlap with the plot. This can be avoided by adjusting the text location using the “hjust” argument in the theme() function by setting the value of “element_text(angle = 30, hjust = 1)”.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05D.html#plotting-interactive-parallel-coordinates-plot-parallelplot-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05D.html#plotting-interactive-parallel-coordinates-plot-parallelplot-package",
    "title": "Hands-on Exercise 5D",
    "section": "",
    "text": "The parallelPlot package is specially designed to plot parallel coordinates plots using the htmlwidgets package and d3.js (JavaScript library). In this sub-section, the parallelPlot package is used to build interactive parallel coordinates plots.\n\n\nThe parallelPlot() function in the parallelPlot package is used to make the basic plot below. Some of the axis labels are too long.\n\nwh = wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\n\n\n\nThe “rotateTitle” argument is added to avoid overlapping axis labels.\nOne of the useful interactive feature of a plot made with the parallelPlot() function is the ability to click on a variable of interest which would then show the different colour intensities across the plot.\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\nThe default blue colour scheme can be changed using the “continousCS” argument.\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\nThe histogram along the axis of each variable can be plotted using the “histoVisibility”.\n\nhistoVisibility = rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)\n\n\n\n\n\n~~~ End of Hands-on Exercise 5D ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "title": "Hands-on Exercise 6",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 17 of the R for Visual Analytics book.\nThe learning outcomes are:\n\nPlot a calender heatmap using functions in the ggplot2 package.\nPlot a cycle plot using functions in the ggplot2 package.\nPlot a slopegraph.\nPlot a horizon chart.\n\n\n\n\n\n\nIn this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nreadxl for reading Excel files;\ndata.table for extending the data.frame package;\nscales for scale functions for visualisation;\nviridis for colourblind-friendly colour maps;\nlubridate for dealing with dates;\nggthemes for extra themes, geoms, and scales for ggplot2;\ngridExtra for miscellaneous functions for “grid” graphics;\nknitr for dynamic report generation;\nCGPfunctions for miscellaneous function for statistics; and\nggHoriPlot for building horizon plots in ggplot2.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, scales, viridis, \n               readxl, data.table, lubridate, \n               ggthemes, gridExtra, knitr, \n               CGPfunctions, ggHoriPlot)\n\n\n\n\nTwo of the datasets for this hands-on exercise are imported into the R environment using the read_csv() function in the readr package and stored as the R object, attacks and rice respectively.\nOne of the datasets is imported into the R environment using the read_excel() function in the readxl package and stored as the R object, air.\n\nattacks = read_csv(\"data/eventlog.csv\")\nair = read_excel(\"data/arrivals_by_air.xlsx\")\nrice = read_csv(\"data/rice.csv\")\n\n\n\n\nThe tibble data frame, attacks, has 3 columns and 199,999 rows. The columns are:\n\n“timestamp” field stores the date-time values in POSIXct format.\n“source_country” field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\n“tz” field stores the time zone of the source IP address.\n\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nFirst, the “weekday” and “hour of day” fields would need to be derived. A function is written to do so.\n\nmake_hr_wkday = function(ts, sc, tz) {\n  real_times = ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt = data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)}\n\n\nNote:\n\nThe ymd_hms() and hour() functions are in the lubridate package.\nThe weekdays() function is in the base R package.\n\n\nNext, the attacks tibble data frame is derived.\n\nwkday_levels = c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks = attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\nNote: Besides extracting the necessary data into the attacks data frame, the mutate() function in the dplyr package is also used to conver the “wkday” and “hour” fields into factor so that they will be ordered when plotting.\n\nThe tidied tibble table, attacks, after the processing is shown below.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n\nThe tibble data frame, air, has 36 columns and 240 rows.\n\n\n\nThe tibble data frame, rice, has 4 columns and 550 rows.\n\n\n\n\n\n\nA single calendar heatmap is plotted.\nFirst, a tibble data table, grouped, is derived by aggregating the attack by “wkday” and”hour“ fields.\nThen, a new field, “n”, is derived by using the group_by() and count() functions.\nThe na.omit() function is used to exclude missing values.\nThe geom_tile() function is used to plot tiles (grids) at each x and y position.\nThe “color” and “size” arguments are used to specify the border color and line size of the tiles.\nThe theme_tufte() function in the ggthemes package is used to remove unnecessary chart junk.\nThe coord_equal() function is used to ensure the plot will have an aspect ratio of 1:1.\nThe scale_fill_gradient() function is used to create a two colour gradient (low-high).\n\ngrouped = attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\nFirst, an object for attacks by country is derived.\nIn order to identify the top 4 countries with the highest number of attacks, the following steps are required:\n\nCount the number of attacks by country;\nCalculate the percent of attacks by country; and\nSave the results in a tibble data frame.\n\n\nattacks_by_country = count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\nNext, the attack records of the top 4 countries from the attacks data frame is extracted and saved in a new tibble data frame, top4_attacks.\n\ntop4 = attacks_by_country$source_country[1:4]\n\ntop4_attacks = attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\nLast, the multiple calender heatmap is plotted using functions in the ggplot2 package.\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\nA cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam using functions in the ggplot2 package is plotted.\nFirst, two new fields, “month”, and “year” are derived from the “Month-Year” field.\n\nair$month = factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \n\nair$year = year(ymd(air$`Month-Year`))\n\nNext, the data for the target country (i.e., Vietnam) is extracted.\n\nVietnam = air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\nThen, the group_by and summarise() functions in the dplyr package are used to compute average arrivals by month.\n\nhline.data = Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\nThe cycle plot is then plotted using functions in the ggplot2 package.\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")\n\n\n\n\n\n\n\nA slopegraph is plotted to show the changes in the rice yield of the top 11 Asian countries from 1961 to 1980.\nFor effective data visualisation design, the factor() function is used convert the value type ofthe “Year” field from numeric to factor.\n\nNote: Reference for using newggslopegraph.\n\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Countries\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")\n\n\n\n\n~~~ End of Hands-on Exercise 6 ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#overview-and-learning-outcomes",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#overview-and-learning-outcomes",
    "title": "Hands-on Exercise 6",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 17 of the R for Visual Analytics book.\nThe learning outcomes are:\n\nPlot a calender heatmap using functions in the ggplot2 package.\nPlot a cycle plot using functions in the ggplot2 package.\nPlot a slopegraph.\nPlot a horizon chart."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getting-started",
    "title": "Hands-on Exercise 6",
    "section": "",
    "text": "In this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nreadxl for reading Excel files;\ndata.table for extending the data.frame package;\nscales for scale functions for visualisation;\nviridis for colourblind-friendly colour maps;\nlubridate for dealing with dates;\nggthemes for extra themes, geoms, and scales for ggplot2;\ngridExtra for miscellaneous functions for “grid” graphics;\nknitr for dynamic report generation;\nCGPfunctions for miscellaneous function for statistics; and\nggHoriPlot for building horizon plots in ggplot2.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, scales, viridis, \n               readxl, data.table, lubridate, \n               ggthemes, gridExtra, knitr, \n               CGPfunctions, ggHoriPlot)\n\n\n\n\nTwo of the datasets for this hands-on exercise are imported into the R environment using the read_csv() function in the readr package and stored as the R object, attacks and rice respectively.\nOne of the datasets is imported into the R environment using the read_excel() function in the readxl package and stored as the R object, air.\n\nattacks = read_csv(\"data/eventlog.csv\")\nair = read_excel(\"data/arrivals_by_air.xlsx\")\nrice = read_csv(\"data/rice.csv\")\n\n\n\n\nThe tibble data frame, attacks, has 3 columns and 199,999 rows. The columns are:\n\n“timestamp” field stores the date-time values in POSIXct format.\n“source_country” field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\n“tz” field stores the time zone of the source IP address.\n\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nFirst, the “weekday” and “hour of day” fields would need to be derived. A function is written to do so.\n\nmake_hr_wkday = function(ts, sc, tz) {\n  real_times = ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt = data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)}\n\n\nNote:\n\nThe ymd_hms() and hour() functions are in the lubridate package.\nThe weekdays() function is in the base R package.\n\n\nNext, the attacks tibble data frame is derived.\n\nwkday_levels = c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks = attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\nNote: Besides extracting the necessary data into the attacks data frame, the mutate() function in the dplyr package is also used to conver the “wkday” and “hour” fields into factor so that they will be ordered when plotting.\n\nThe tidied tibble table, attacks, after the processing is shown below.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n\nThe tibble data frame, air, has 36 columns and 240 rows.\n\n\n\nThe tibble data frame, rice, has 4 columns and 550 rows."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-calendar-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-calendar-heatmap",
    "title": "Hands-on Exercise 6",
    "section": "",
    "text": "A single calendar heatmap is plotted.\nFirst, a tibble data table, grouped, is derived by aggregating the attack by “wkday” and”hour“ fields.\nThen, a new field, “n”, is derived by using the group_by() and count() functions.\nThe na.omit() function is used to exclude missing values.\nThe geom_tile() function is used to plot tiles (grids) at each x and y position.\nThe “color” and “size” arguments are used to specify the border color and line size of the tiles.\nThe theme_tufte() function in the ggthemes package is used to remove unnecessary chart junk.\nThe coord_equal() function is used to ensure the plot will have an aspect ratio of 1:1.\nThe scale_fill_gradient() function is used to create a two colour gradient (low-high).\n\ngrouped = attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\nFirst, an object for attacks by country is derived.\nIn order to identify the top 4 countries with the highest number of attacks, the following steps are required:\n\nCount the number of attacks by country;\nCalculate the percent of attacks by country; and\nSave the results in a tibble data frame.\n\n\nattacks_by_country = count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\nNext, the attack records of the top 4 countries from the attacks data frame is extracted and saved in a new tibble data frame, top4_attacks.\n\ntop4 = attacks_by_country$source_country[1:4]\n\ntop4_attacks = attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\nLast, the multiple calender heatmap is plotted using functions in the ggplot2 package.\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-cycle-plot",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-cycle-plot",
    "title": "Hands-on Exercise 6",
    "section": "",
    "text": "A cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam using functions in the ggplot2 package is plotted.\nFirst, two new fields, “month”, and “year” are derived from the “Month-Year” field.\n\nair$month = factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \n\nair$year = year(ymd(air$`Month-Year`))\n\nNext, the data for the target country (i.e., Vietnam) is extracted.\n\nVietnam = air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\nThen, the group_by and summarise() functions in the dplyr package are used to compute average arrivals by month.\n\nhline.data = Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\nThe cycle plot is then plotted using functions in the ggplot2 package.\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-slopegraph",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-slopegraph",
    "title": "Hands-on Exercise 6",
    "section": "",
    "text": "A slopegraph is plotted to show the changes in the rice yield of the top 11 Asian countries from 1961 to 1980.\nFor effective data visualisation design, the factor() function is used convert the value type ofthe “Year” field from numeric to factor.\n\nNote: Reference for using newggslopegraph.\n\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Countries\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")\n\n\n\n\n~~~ End of Hands-on Exercise 6 ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07B.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07B.html",
    "title": "Hands-on Exercise 7B",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 22 of the R for Visual Analytics book.\nProportional symbol maps (also known as graduate symbol maps) are a class of maps that use the visual variable of size to represent differences in the magnitude of a discrete, abruptly changing phenomenon, e.g. counts of people. Like choropleth maps, you can create classed or unclassed versions of these maps. The classed ones are known as range-graded or graduated symbols, and the unclassed are called proportional symbols, where the area of the symbols are proportional to the values of the attribute being mapped. In this hands-on exercise, you will learn how to create a proportional symbol map showing the number of wins by Singapore Pools’ outlets using an R package called tmap.\nBy the end of this hands-on exercise, you will acquire the following skills by using appropriate R packages:\n\nTo import an aspatial data file into R.\nTo convert it into simple point feature data frame and at the same time, to assign an appropriate projection reference to the newly create simple point feature data frame.\nTo plot interactive proportional symbol maps.\n\n\n\n\n\n\nIn this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nsf for handling geospatial data; and\ntmap for thematic mapping.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, sf, tmap)\n\n\n\n\nThe dataset for this hands-on exercise, SGPools_svy21, which is in the csv file format, is imported into the R environment using the read_csv() function in the readr package and stored as the R object, sgpools.\n\nsgpools = read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\nThe tibble data frame, sgpools, has 7 columns and 306 rows. The “XCOORD” and “YCOORD” variables are the x-coordinates and y-coordinates of Singapore Pools outlets/branches. They are in the Singapore SVY21 Projected Coordinates System.\n\nlist(sgpools) \n\n[[1]]\n# A tibble: 306 × 7\n   NAME           ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Mar… 2 Bayf…    18972 30842. 29599. Branch                        5\n 2 Livewire (Res… 26 Sen…    98138 26704. 26526. Branch                       11\n 3 SportsBuzz (K… Lotus …   738078 20118. 44888. Branch                        0\n 4 SportsBuzz (P… 1 Sele…   188306 29777. 31382. Branch                       44\n 5 Prime Serango… Blk 54…   552542 32239. 39519. Branch                        0\n 6 Singapore Poo… 1A Woo…   731001 21012. 46987. Branch                        3\n 7 Singapore Poo… Blk 64…   370064 33990. 34356. Branch                       17\n 8 Singapore Poo… Blk 88…   370088 33847. 33976. Branch                       16\n 9 Singapore Poo… Blk 30…   540308 33910. 41275. Branch                       21\n10 Singapore Poo… Blk 20…   560202 29246. 38943. Branch                       25\n# ℹ 296 more rows\n\n\nThe sgpools data frame is then converted into a simple feature data frame using the st_as_sf() function in the sf package.\nThe “coords” argument requires the column name of the x-coordinates first followed by the column name of the y-coordinates.\nThe “crs” argument requires the coordinates system in epsg format. EPSG: 3414 is the Singapore SVY21 Projected Coordinate System.\nFigure below shows the data table of sgpools_sf. Notice that\n\nsgpools_sf = st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\nA new column called geometry has been added into the sf data frame, sgpools_sf, which is a point feature class.\n\nlist(sgpools_sf)\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 × 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * &lt;chr&gt;                        &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marina Bay Sands)  2 Bayf…    18972 Branch                        5\n 2 Livewire (Resorts World Sen… 26 Sen…    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus …   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele…   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54…   552542 Branch                        0\n 6 Singapore Pools Woodlands C… 1A Woo…   731001 Branch                        3\n 7 Singapore Pools 64 Circuit … Blk 64…   370064 Branch                       17\n 8 Singapore Pools 88 Circuit … Blk 88…   370088 Branch                       16\n 9 Singapore Pools Anchorvale … Blk 30…   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio … Blk 20…   560202 Branch                       25\n# ℹ 296 more rows\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\n\n\n\n\nThe “view” mode is turned on via the tmap_mode() function to create an interactive proportional symbol map.\n\ntmap_mode(\"view\")\n\n\n\nAn interactive point symbol map is created below to show where the Singapore Pools outlets/branches are.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n\nA numerical variable is assigned to the “size” attribute to draw a proportional symbol map. The variable “Gp1Gp2Winnings” is assigned in the plot below.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n\nThe proportional symbol map can be further improved by using the “col” attribute. The “OUTLET_TYPE“ variable is used as the colour attribute variable in the plot below.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1)\n\n\n\n\n\n\n\n\n\nAn impressive and little-know feature of tmap’s view mode is that it also works with faceted plots. The “sync” argument the in tm_facets() function can be used to produce multiple maps with synchronised zoom and pan settings.\n\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore ending the session, the tmap_mode() function is used to switch the mode back to “plot”.\n\ntmap_mode(\"plot\")\n\n\n\n\n\n\n\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions\n\n~~~ End of Hands-on Exercise 7B ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07B.html#overview-and-learning-outcomes",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07B.html#overview-and-learning-outcomes",
    "title": "Hands-on Exercise 7B",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 22 of the R for Visual Analytics book.\nProportional symbol maps (also known as graduate symbol maps) are a class of maps that use the visual variable of size to represent differences in the magnitude of a discrete, abruptly changing phenomenon, e.g. counts of people. Like choropleth maps, you can create classed or unclassed versions of these maps. The classed ones are known as range-graded or graduated symbols, and the unclassed are called proportional symbols, where the area of the symbols are proportional to the values of the attribute being mapped. In this hands-on exercise, you will learn how to create a proportional symbol map showing the number of wins by Singapore Pools’ outlets using an R package called tmap.\nBy the end of this hands-on exercise, you will acquire the following skills by using appropriate R packages:\n\nTo import an aspatial data file into R.\nTo convert it into simple point feature data frame and at the same time, to assign an appropriate projection reference to the newly create simple point feature data frame.\nTo plot interactive proportional symbol maps."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07B.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07B.html#getting-started",
    "title": "Hands-on Exercise 7B",
    "section": "",
    "text": "In this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nsf for handling geospatial data; and\ntmap for thematic mapping.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, sf, tmap)\n\n\n\n\nThe dataset for this hands-on exercise, SGPools_svy21, which is in the csv file format, is imported into the R environment using the read_csv() function in the readr package and stored as the R object, sgpools.\n\nsgpools = read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\nThe tibble data frame, sgpools, has 7 columns and 306 rows. The “XCOORD” and “YCOORD” variables are the x-coordinates and y-coordinates of Singapore Pools outlets/branches. They are in the Singapore SVY21 Projected Coordinates System.\n\nlist(sgpools) \n\n[[1]]\n# A tibble: 306 × 7\n   NAME           ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Mar… 2 Bayf…    18972 30842. 29599. Branch                        5\n 2 Livewire (Res… 26 Sen…    98138 26704. 26526. Branch                       11\n 3 SportsBuzz (K… Lotus …   738078 20118. 44888. Branch                        0\n 4 SportsBuzz (P… 1 Sele…   188306 29777. 31382. Branch                       44\n 5 Prime Serango… Blk 54…   552542 32239. 39519. Branch                        0\n 6 Singapore Poo… 1A Woo…   731001 21012. 46987. Branch                        3\n 7 Singapore Poo… Blk 64…   370064 33990. 34356. Branch                       17\n 8 Singapore Poo… Blk 88…   370088 33847. 33976. Branch                       16\n 9 Singapore Poo… Blk 30…   540308 33910. 41275. Branch                       21\n10 Singapore Poo… Blk 20…   560202 29246. 38943. Branch                       25\n# ℹ 296 more rows\n\n\nThe sgpools data frame is then converted into a simple feature data frame using the st_as_sf() function in the sf package.\nThe “coords” argument requires the column name of the x-coordinates first followed by the column name of the y-coordinates.\nThe “crs” argument requires the coordinates system in epsg format. EPSG: 3414 is the Singapore SVY21 Projected Coordinate System.\nFigure below shows the data table of sgpools_sf. Notice that\n\nsgpools_sf = st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\nA new column called geometry has been added into the sf data frame, sgpools_sf, which is a point feature class.\n\nlist(sgpools_sf)\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 × 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * &lt;chr&gt;                        &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marina Bay Sands)  2 Bayf…    18972 Branch                        5\n 2 Livewire (Resorts World Sen… 26 Sen…    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus …   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele…   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54…   552542 Branch                        0\n 6 Singapore Pools Woodlands C… 1A Woo…   731001 Branch                        3\n 7 Singapore Pools 64 Circuit … Blk 64…   370064 Branch                       17\n 8 Singapore Pools 88 Circuit … Blk 88…   370088 Branch                       16\n 9 Singapore Pools Anchorvale … Blk 30…   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio … Blk 20…   560202 Branch                       25\n# ℹ 296 more rows\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07B.html#drawing-proportional-symbol-map",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07B.html#drawing-proportional-symbol-map",
    "title": "Hands-on Exercise 7B",
    "section": "",
    "text": "The “view” mode is turned on via the tmap_mode() function to create an interactive proportional symbol map.\n\ntmap_mode(\"view\")\n\n\n\nAn interactive point symbol map is created below to show where the Singapore Pools outlets/branches are.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n\nA numerical variable is assigned to the “size” attribute to draw a proportional symbol map. The variable “Gp1Gp2Winnings” is assigned in the plot below.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n\nThe proportional symbol map can be further improved by using the “col” attribute. The “OUTLET_TYPE“ variable is used as the colour attribute variable in the plot below.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1)\n\n\n\n\n\n\n\n\n\nAn impressive and little-know feature of tmap’s view mode is that it also works with faceted plots. The “sync” argument the in tm_facets() function can be used to produce multiple maps with synchronised zoom and pan settings.\n\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore ending the session, the tmap_mode() function is used to switch the mode back to “plot”.\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07B.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07B.html#references",
    "title": "Hands-on Exercise 7B",
    "section": "",
    "text": "tmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions\n\n~~~ End of Hands-on Exercise 7B ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html",
    "title": "Hands-on Exercise 8",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 27 of the R for Visual Analytics book.\nThe learning outcomes are:\n\nCreate graph object data frames, manipulate them using appropriate functions of the dplyr, lubridate, and tidygraph packages.\nBuild network graph visualisation using appropriate functions in the ggraph package.\nCompute network geometrics using the tidygraph package.\nBuild advanced graph visualisations by incorporating network geometrics.\nBuild interactive network visualisation using the visNetwork package.\n\n\n\n\n\n\nIn this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nlubridate and clock for dealing with dates;\nigraph, tidygraph, ggraph and visNetwork for network data modelling and visualisation; and\ngraphlayouts for implementing graph layout algorithms.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, lubridate,\n               clock, igraph,\n               tidygraph, ggraph,\n               visNetwork, graphlayouts)\n\n\n\n\nThe two datasets for this hands-on exercise are imported into the R environment using the read_csv() function in the readr package and stored as the R object, GAStech_nodes and GAStech_edges. The first is the nodes data, the second is the edges (link) data. They contain data from an oil exploration and extraction company.\n\nGAStech_nodes = read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges = read_csv(\"data/GAStech_email_edge-v2.csv\")\n\nThe tibble data frames, GAStech_nodes and GAStech_edges, have 4 columns and 54 rows, and 8 columns and 9,063 rows respectively. The former contains the names, departments, and titles of the company’s employees. The latter consists of two weeks’ worth of email correspondences between the employees.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\nThe GAStech_edges has a “SentDate” variable that is treated as character data type instead of date data type. This will need to be changed.\n\n\n\nFirst, the GAStech_edges tibble data frame is manipulated using the dmy() and wday() functions in the lubridate package\n\nThe dmy() function transforms the “SentDate” variable to the date data type.\nThe wday() function returns the day of the week as a decimal number or an ordered factor if label is TRUE. The “abbr” argument set as FALSE keeps the days spellings in full, i.e. Monday. The function will create a new column in the data frame i.e. Weekday. The values are in ordinal scale.\n\n\nGAStech_edges = GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 10\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    &lt;date&gt; 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     &lt;ord&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\nA close examination of the GAStech_edges tibble data frame reveals that it consists of individual email flow records. This is not very useful for visualisation. Hence, they are aggregate by date, senders, receivers, main subject and day of the week using the filter(), group_by(), summarise(), and ungroup() functions in the dplyr packages. The output tibble data frame is GAStech_edges_aggregated. A new field “WEight” is added to count the aggregated entries that form each row.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\nglimpse(GAStech_edges_aggregated)\n\nRows: 1,372\nColumns: 4\n$ source  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday &lt;ord&gt; Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  &lt;int&gt; 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…\n\n\n\n\n\n\nIn this section, a graph data model is created using the tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. The package provides a way to switch between the two tables and provides dplyr functions for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\n\n\nTwo functions of the tidygraph package can be used to create network objects:\n\nThe tbl_graph() function creates a tbl_graph network object from nodes and edges data.\nThe as_tbl_graph() function converts network data and objects to a tbl_graph network. Below are network data and objects supported by the function:\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\n\nThe activate() verb from the tidygraph package serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly, the .E() function will give the edge data and the .G() function will give the tbl_graph object itself.\n\n\n\nIn this sub-section, the tbl_graph() function in the tinygraph package is used to build an tidygraph’s network graph data frame.\n\nGAStech_graph = tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4,541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”. It states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\nThe nodes tibble data frame is activated by default, but which tibble data frame is active can be changed with the activate() function. Thus, to rearrange the rows in the edges tibble to list those with the highest “weight” first, the activate() and arrange() functions in the tidygraph package can be used.\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows\n\n\n\n\n\n\nThe ggraph package is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges, and\nlayouts.\n\n\n\nThe ggraph(), geom-edge_link(), and geom_node_point() functions in the ggraph package are used to plot a network graph using GAStech_graph.\nThe basic plotting function is the ggraph() function, which takes the data to be used for the graph and the type of layout desired. Both of the arguments for function are built around igraph. Therefore, the function can use either an igraph object or a tbl_graph object.\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\nThe theme_graph() function in the ggraph package is used to remove the x- and y- axes.\nThe ggraph package introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. The theme_graph() function, besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden). The theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() function in the individual plots.\n\ng = ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\nFurthermore, the theme_graph() makes it easy to change the coloring of the plot.\n\ng = ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\nThe ggraph package supports many layouts for standard used: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl.\n\n\n\n\n\nThe network graph using Fruchterman and Reingold layout is plotted by setting the “layout” argument in the ggraph() function as “fr”.\n\ng = ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\nEach node can be coloured by referring to their respective departments.\nThe geom_node_point() function in the ggraph package is equivalent in functionality to the geo_point() function in the ggplot2 package. It allows for simple plotting of nodes in different shapes, colours and sizes. In the plot below, colour and size are used.\n\ng = ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\nThe thickness of the edges are mapped with the “Weight” variable.\nThe geom_edge_link() function in the ggraph package draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. For instance, the “width” argument can be used to map the width of the line in proportion to the “Weight” attribute and the “alpha” argument is used to introduce opacity on the line.\n\ng = ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nAnother very useful feature of the ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaningful way by spreading nodes and edges out based on their attributes. In this sub-section, the faceting technique is used to visualise network data.\nThere are three functions in the ggraph package to implement faceting:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\n\nThe facet_edges()functionis used to plot the facet graphs below.\n\nset_graph_style()\n\ng = ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\nThe theme() function is then used to change the position of the legend.\n\nset_graph_style()\n\ng = ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\nLastly, a frame is added to each graph using the th_foreground() function.\n\nset_graph_style() \n\ng = ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nThe facet_nodes() function is used to plot the facet graphs below.\n\nset_graph_style()\n\ng = ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures: degree, betweenness, closeness and eigenvector. The mutate() function in the dplyr package is used to perform the computation. The algorithm used is the centrality_betweenness() function in the tidygraph package.\n\ng = GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n\nIt is important to note that from the ggraph v2.0 package onwards, the tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\ng = GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\nThe tidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. This link provides information about community detection functions provided by tidygraph.\nThe group_edge_betweenness() function is used below.\n\ng = GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThe visNetwork is a package for network visualisation, using vis.js javascript library. The visNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\nThe resulting graph is fun to play around with: can move the nodes and the graph will use an algorithm to keep the nodes properly spaced; and can zoom in and out on the plot and move it around to re-center it.\n\n\n\nThe data model is prepared prior to plotting.\n\nGAStech_edges_aggregated = GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\nAn interactive network graph is plotted using the visNetwork() function with the prepared data.\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n\nThe Fruchterman and Reingold layout is used. More information about the visIgraphLayout() function’s argument can be found here.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\n\n\n\nThe visNetwork() function looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field. The “Department” field is renamed to “group”.\n\nGAStech_nodes = GAStech_nodes %&gt;%\n  rename(group = Department) \n\nThereafter, the visNetwork() function shades the nodes by assigning unique colour to each category in the “group” field.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\nThe visEdges() function is then used to symbolise the edges. The “arrows” argument is used to define where to place the arrow. The “smooth”argument is used to plot the edges using a smooth curve. More information about the visEdges() function’s argument can be found here.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\nThe visOptions() function is used to incorporate interactivity features in the data visualisation. The “highlightNearest” argument highlights the nearest nodes when clicking a particular node. The”nodesIdSelection” argument adds an id node selection creating an HTML select element. More information about the visOption() function’s argument can be found here.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n~~~ End of Hands-on Exercise 8 ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#overview-and-learning-outcomes",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#overview-and-learning-outcomes",
    "title": "Hands-on Exercise 8",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 27 of the R for Visual Analytics book.\nThe learning outcomes are:\n\nCreate graph object data frames, manipulate them using appropriate functions of the dplyr, lubridate, and tidygraph packages.\nBuild network graph visualisation using appropriate functions in the ggraph package.\nCompute network geometrics using the tidygraph package.\nBuild advanced graph visualisations by incorporating network geometrics.\nBuild interactive network visualisation using the visNetwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#getting-started",
    "title": "Hands-on Exercise 8",
    "section": "",
    "text": "In this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nlubridate and clock for dealing with dates;\nigraph, tidygraph, ggraph and visNetwork for network data modelling and visualisation; and\ngraphlayouts for implementing graph layout algorithms.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, lubridate,\n               clock, igraph,\n               tidygraph, ggraph,\n               visNetwork, graphlayouts)\n\n\n\n\nThe two datasets for this hands-on exercise are imported into the R environment using the read_csv() function in the readr package and stored as the R object, GAStech_nodes and GAStech_edges. The first is the nodes data, the second is the edges (link) data. They contain data from an oil exploration and extraction company.\n\nGAStech_nodes = read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges = read_csv(\"data/GAStech_email_edge-v2.csv\")\n\nThe tibble data frames, GAStech_nodes and GAStech_edges, have 4 columns and 54 rows, and 8 columns and 9,063 rows respectively. The former contains the names, departments, and titles of the company’s employees. The latter consists of two weeks’ worth of email correspondences between the employees.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\nThe GAStech_edges has a “SentDate” variable that is treated as character data type instead of date data type. This will need to be changed.\n\n\n\nFirst, the GAStech_edges tibble data frame is manipulated using the dmy() and wday() functions in the lubridate package\n\nThe dmy() function transforms the “SentDate” variable to the date data type.\nThe wday() function returns the day of the week as a decimal number or an ordered factor if label is TRUE. The “abbr” argument set as FALSE keeps the days spellings in full, i.e. Monday. The function will create a new column in the data frame i.e. Weekday. The values are in ordinal scale.\n\n\nGAStech_edges = GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 10\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    &lt;date&gt; 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     &lt;ord&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\nA close examination of the GAStech_edges tibble data frame reveals that it consists of individual email flow records. This is not very useful for visualisation. Hence, they are aggregate by date, senders, receivers, main subject and day of the week using the filter(), group_by(), summarise(), and ungroup() functions in the dplyr packages. The output tibble data frame is GAStech_edges_aggregated. A new field “WEight” is added to count the aggregated entries that form each row.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\nglimpse(GAStech_edges_aggregated)\n\nRows: 1,372\nColumns: 4\n$ source  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday &lt;ord&gt; Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  &lt;int&gt; 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#creating-network-objects-using-tidygraph",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#creating-network-objects-using-tidygraph",
    "title": "Hands-on Exercise 8",
    "section": "",
    "text": "In this section, a graph data model is created using the tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. The package provides a way to switch between the two tables and provides dplyr functions for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\n\n\nTwo functions of the tidygraph package can be used to create network objects:\n\nThe tbl_graph() function creates a tbl_graph network object from nodes and edges data.\nThe as_tbl_graph() function converts network data and objects to a tbl_graph network. Below are network data and objects supported by the function:\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\n\nThe activate() verb from the tidygraph package serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly, the .E() function will give the edge data and the .G() function will give the tbl_graph object itself.\n\n\n\nIn this sub-section, the tbl_graph() function in the tinygraph package is used to build an tidygraph’s network graph data frame.\n\nGAStech_graph = tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4,541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”. It states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\nThe nodes tibble data frame is activated by default, but which tibble data frame is active can be changed with the activate() function. Thus, to rearrange the rows in the edges tibble to list those with the highest “weight” first, the activate() and arrange() functions in the tidygraph package can be used.\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#plotting-static-network-graphs-ggraph-package",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#plotting-static-network-graphs-ggraph-package",
    "title": "Hands-on Exercise 8",
    "section": "",
    "text": "The ggraph package is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges, and\nlayouts.\n\n\n\nThe ggraph(), geom-edge_link(), and geom_node_point() functions in the ggraph package are used to plot a network graph using GAStech_graph.\nThe basic plotting function is the ggraph() function, which takes the data to be used for the graph and the type of layout desired. Both of the arguments for function are built around igraph. Therefore, the function can use either an igraph object or a tbl_graph object.\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\nThe theme_graph() function in the ggraph package is used to remove the x- and y- axes.\nThe ggraph package introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. The theme_graph() function, besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden). The theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() function in the individual plots.\n\ng = ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\nFurthermore, the theme_graph() makes it easy to change the coloring of the plot.\n\ng = ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\nThe ggraph package supports many layouts for standard used: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl.\n\n\n\n\n\nThe network graph using Fruchterman and Reingold layout is plotted by setting the “layout” argument in the ggraph() function as “fr”.\n\ng = ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\nEach node can be coloured by referring to their respective departments.\nThe geom_node_point() function in the ggraph package is equivalent in functionality to the geo_point() function in the ggplot2 package. It allows for simple plotting of nodes in different shapes, colours and sizes. In the plot below, colour and size are used.\n\ng = ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\nThe thickness of the edges are mapped with the “Weight” variable.\nThe geom_edge_link() function in the ggraph package draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. For instance, the “width” argument can be used to map the width of the line in proportion to the “Weight” attribute and the “alpha” argument is used to introduce opacity on the line.\n\ng = ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#creating-facet-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#creating-facet-graphs",
    "title": "Hands-on Exercise 8",
    "section": "",
    "text": "Another very useful feature of the ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaningful way by spreading nodes and edges out based on their attributes. In this sub-section, the faceting technique is used to visualise network data.\nThere are three functions in the ggraph package to implement faceting:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\n\nThe facet_edges()functionis used to plot the facet graphs below.\n\nset_graph_style()\n\ng = ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\nThe theme() function is then used to change the position of the legend.\n\nset_graph_style()\n\ng = ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\nLastly, a frame is added to each graph using the th_foreground() function.\n\nset_graph_style() \n\ng = ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nThe facet_nodes() function is used to plot the facet graphs below.\n\nset_graph_style()\n\ng = ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#network-metrics-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#network-metrics-analysis",
    "title": "Hands-on Exercise 8",
    "section": "",
    "text": "Centrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures: degree, betweenness, closeness and eigenvector. The mutate() function in the dplyr package is used to perform the computation. The algorithm used is the centrality_betweenness() function in the tidygraph package.\n\ng = GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n\nIt is important to note that from the ggraph v2.0 package onwards, the tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\ng = GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\nThe tidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. This link provides information about community detection functions provided by tidygraph.\nThe group_edge_betweenness() function is used below.\n\ng = GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#building-interactive-network-graph-visnetwork-package",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#building-interactive-network-graph-visnetwork-package",
    "title": "Hands-on Exercise 8",
    "section": "",
    "text": "The visNetwork is a package for network visualisation, using vis.js javascript library. The visNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\nThe resulting graph is fun to play around with: can move the nodes and the graph will use an algorithm to keep the nodes properly spaced; and can zoom in and out on the plot and move it around to re-center it.\n\n\n\nThe data model is prepared prior to plotting.\n\nGAStech_edges_aggregated = GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\nAn interactive network graph is plotted using the visNetwork() function with the prepared data.\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n\nThe Fruchterman and Reingold layout is used. More information about the visIgraphLayout() function’s argument can be found here.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\n\n\n\nThe visNetwork() function looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field. The “Department” field is renamed to “group”.\n\nGAStech_nodes = GAStech_nodes %&gt;%\n  rename(group = Department) \n\nThereafter, the visNetwork() function shades the nodes by assigning unique colour to each category in the “group” field.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\nThe visEdges() function is then used to symbolise the edges. The “arrows” argument is used to define where to place the arrow. The “smooth”argument is used to plot the edges using a smooth curve. More information about the visEdges() function’s argument can be found here.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\nThe visOptions() function is used to incorporate interactivity features in the data visualisation. The “highlightNearest” argument highlights the nearest nodes when clicking a particular node. The”nodesIdSelection” argument adds an id node selection creating an HTML select element. More information about the visOption() function’s argument can be found here.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n~~~ End of Hands-on Exercise 8 ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html",
    "title": "Hands-on Exercise 10",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 31 of the R for Visual Analytics book.\nThe learning outcomes are:\n\nCreate a bullet chart using the ggplot2 package.\nCreate sparklines using the ggplot2 package.\nBuild industry standard dashboard using R Shiny.\n\n\n\n\n\n\nIn this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nlubridate for dealing with dates;\nggthemes for extra themes, geoms, and scales for ggplot2;\ngtExtras for additional helper functions to assist in creating beautiful tables with gt, a package specially designed for anyone to make wonderful-looking tables using the R programming language;\nreactable for functions to create interactive data tables for R, based on the React Table library and made with reactR;\nreactablefmtr for various features to streamline and enhance the styling of interactive reactable tables with easy-to-use and highly-customizable functions and themes; and\nRODBC for accessing databases.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, lubridate,\n               ggthemes, gtExtras,\n               reactable, reactablefmtr,\n               RODBC)\n\nThe dataui package for building interactive sparklines is also installed.\n\nremotes::install_github(\"timelyportfolio/dataui\")\nlibrary(dataui)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the odbcConnectAccess() function in the RODBCpackage.\n\ncon = odbcConnectAccess2007('data/Coffee Chain.mdb')\ncoffeechain = sqlFetch(con, 'CoffeeChain Query')\nwrite_rds(coffeechain, \"data/CoffeeChain.rds\")\nodbcClose(con)\n\nThe data frame, coffeechain, has 19 columns and 4,248 rows. It is saved in the rds file format in imported into the R environment.\n\ncoffeechain = read_rds(\"data/rds/CoffeeChain.rds\")\n\n\n\n\nThe Sales and Budgeted Sales are aggregated at the Product level using the group_by() and summarise() functions in the dplyr package.\n\nproduct = coffeechain %&gt;%\n  group_by(`Product`) %&gt;%\n  summarise(`target` = sum(`Budget Sales`),\n            `current` = sum(`Sales`)) %&gt;%\n  ungroup()\n\n\n\n\n\nA bullet chart is plotted using functions in the ggplot2 package, i.e., geom_col() and geom_errorbar().\n\nggplot(product, aes(Product, current)) + \n  geom_col(aes(Product, max(target) * 1.01),\n           fill=\"grey85\", width=0.85) +\n  geom_col(aes(Product, target * 0.75),\n           fill=\"grey60\", width=0.85) +\n  geom_col(aes(Product, target * 0.5),\n           fill=\"grey50\", width=0.85) +\n  geom_col(aes(Product, current), \n           width=0.35,\n           fill = \"black\") + \n  geom_errorbar(aes(y = target,\n                    x = Product, \n                    ymin = target,\n                    ymax= target), \n                width = .4,\n                colour = \"red\",\n                size = 1) +\n  coord_flip()\n\n\n\n\n\n\n\nTo plot sparklines, the data is manipulated to generate a table containing the monthly sales values by product type.\n\nsales_report &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  mutate(Month = month(Date)) %&gt;%\n  group_by(Month, Product) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup() %&gt;%\n  select(Month, Product, Sales)\n\nThe minimum, maximum, end of month sales values as well as the 25th and 75th quantile values are then derived.\n\nmins = group_by(sales_report, Product) %&gt;% \n  slice(which.min(Sales))\n\nmaxs = group_by(sales_report, Product) %&gt;% \n  slice(which.max(Sales))\n\nends = group_by(sales_report, Product) %&gt;% \n  filter(Month == max(Month))\n\nquarts = sales_report %&gt;%\n  group_by(Product) %&gt;%\n  summarise(quart1 = quantile(Sales, \n                              0.25),\n            quart2 = quantile(Sales, \n                              0.75)) %&gt;%\n  right_join(sales_report)\n\nA sparklines graph is then plotted.\n\nggplot(sales_report, aes(x=Month, y=Sales)) + \n  facet_grid(Product ~ ., scales = \"free_y\") + \n  geom_ribbon(data = quarts, aes(ymin = quart1, max = quart2), \n              fill = 'grey90') +\n  geom_line(size=0.3) +\n  geom_point(data = mins, col = 'red') +\n  geom_point(data = maxs, col = 'blue') +\n  geom_text(data = mins, aes(label = Sales), vjust = -1) +\n  geom_text(data = maxs, aes(label = Sales), vjust = 2.5) +\n  geom_text(data = ends, aes(label = Sales), hjust = 0, nudge_x = 0.5) +\n  geom_text(data = ends, aes(label = Product), hjust = 0, nudge_x = 1.0) +\n  expand_limits(x = max(sales_report$Month) + \n                  (0.25 * (max(sales_report$Month) - min(sales_report$Month)))) +\n  scale_x_continuous(breaks = seq(1, 12, 1)) +\n  scale_y_continuous(expand = c(0.1, 0)) +\n  theme_tufte(base_size = 3, base_family = \"Helvetica\") +\n  theme(axis.title=element_blank(), axis.text.y = element_blank(), \n        axis.ticks = element_blank(), strip.text = element_blank())\n\n\n\n\n\n\n\nIn this sub-section, a static information dashboard is created using the gt and gtExtras packages.\n\n\nA bullet chart using the gt() and gt_plt_bullet() functions in the gt and gtExtras packages respectively.\n\nproduct %&gt;%\n  gt::gt() %&gt;%\n  gt_plt_bullet(column = current, \n              target = target, \n              width = 60,\n              palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()\n\n\n\n\n\n  \n    \n    \n      Product\n      current\n    \n  \n  \n    Amaretto\n          \n    Caffe Latte\n          \n    Caffe Mocha\n          \n    Chamomile\n          \n    Colombian\n          \n    Darjeeling\n          \n    Decaf Espresso\n          \n    Decaf Irish Cream\n          \n    Earl Grey\n          \n    Green Tea\n          \n    Lemon\n          \n    Mint\n          \n    Regular Espresso\n          \n  \n  \n  \n\n\n\n\n\n\n\nThe sales report of products by month for 2013 is extracted using the mutate(), filter(), group_by(), and summarise() functions in the dplyr package.\n\nreport = coffeechain %&gt;%\n  mutate(Year = year(Date)) %&gt;%\n  filter(Year == \"2013\") %&gt;%\n  mutate (Month = month(Date, \n                        label = TRUE, \n                        abbr = TRUE)) %&gt;%\n  group_by(Product, Month) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup()\n\nThe report data frame is converted into list columns.\n\nreport %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\n# A tibble: 13 × 2\n   Product           `Monthly Sales`\n   &lt;chr&gt;             &lt;list&gt;         \n 1 Amaretto          &lt;dbl [12]&gt;     \n 2 Caffe Latte       &lt;dbl [12]&gt;     \n 3 Caffe Mocha       &lt;dbl [12]&gt;     \n 4 Chamomile         &lt;dbl [12]&gt;     \n 5 Colombian         &lt;dbl [12]&gt;     \n 6 Darjeeling        &lt;dbl [12]&gt;     \n 7 Decaf Espresso    &lt;dbl [12]&gt;     \n 8 Decaf Irish Cream &lt;dbl [12]&gt;     \n 9 Earl Grey         &lt;dbl [12]&gt;     \n10 Green Tea         &lt;dbl [12]&gt;     \n11 Lemon             &lt;dbl [12]&gt;     \n12 Mint              &lt;dbl [12]&gt;     \n13 Regular Espresso  &lt;dbl [12]&gt;     \n\n\n\n\nThe monthly sales is plotted as sparklines using the gt() and gt_plt_sparkline() functions in the gt and gtExtras packages respectively.\n\nreport %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\") %&gt;%\n   gt() %&gt;%\n   gt_plt_sparkline('Monthly Sales',\n                    same_limit = FALSE)\n\n\n\n\n\n  \n    \n    \n      Product\n      Monthly Sales\n    \n  \n  \n    Amaretto\n          1.2K\n    Caffe Latte\n          1.5K\n    Caffe Mocha\n          3.7K\n    Chamomile\n          3.3K\n    Colombian\n          5.5K\n    Darjeeling\n          3.0K\n    Decaf Espresso\n          3.2K\n    Decaf Irish Cream\n          2.7K\n    Earl Grey\n          3.0K\n    Green Tea\n          1.5K\n    Lemon\n          4.4K\n    Mint\n          1.5K\n    Regular Espresso\n          1.1K\n  \n  \n  \n\n\n\n\n\n\n\nThe summary statistics are calculated using the group_by() and summarise() functions in the dplyr package, as well as the gt() and fmt_number() functions in the gt package.\n\nreport %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            ) %&gt;%\n  gt() %&gt;%\n  fmt_number(columns = 4,\n    decimals = 2)\n\n\n\n\n\n  \n    \n    \n      Product\n      Min\n      Max\n      Average\n    \n  \n  \n    Amaretto\n1016\n1210\n1,119.00\n    Caffe Latte\n1398\n1653\n1,528.33\n    Caffe Mocha\n3322\n3828\n3,613.92\n    Chamomile\n2967\n3395\n3,217.42\n    Colombian\n5132\n5961\n5,457.25\n    Darjeeling\n2926\n3281\n3,112.67\n    Decaf Espresso\n3181\n3493\n3,326.83\n    Decaf Irish Cream\n2463\n2901\n2,648.25\n    Earl Grey\n2730\n3005\n2,841.83\n    Green Tea\n1339\n1476\n1,398.75\n    Lemon\n3851\n4418\n4,080.83\n    Mint\n1388\n1669\n1,519.17\n    Regular Espresso\n890\n1218\n1,023.42\n  \n  \n  \n\n\n\n\n\n\n\nThe derived statistics are then combined with the table using the left_join() function in the dplyr package.\n\nspark = report %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\nsales = report %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            )\n\nsales_data = left_join(sales, spark)\n\n\n\n\nThe combined table is then plotted using the gt() and gt_plt_sparkline() functions in the gt and gtExtras packages respectively.\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales',\n                   same_limit = FALSE)\n\n\n\n\n\n  \n    \n    \n      Product\n      Min\n      Max\n      Average\n      Monthly Sales\n    \n  \n  \n    Amaretto\n1016\n1210\n1119.000\n          1.2K\n    Caffe Latte\n1398\n1653\n1528.333\n          1.5K\n    Caffe Mocha\n3322\n3828\n3613.917\n          3.7K\n    Chamomile\n2967\n3395\n3217.417\n          3.3K\n    Colombian\n5132\n5961\n5457.250\n          5.5K\n    Darjeeling\n2926\n3281\n3112.667\n          3.0K\n    Decaf Espresso\n3181\n3493\n3326.833\n          3.2K\n    Decaf Irish Cream\n2463\n2901\n2648.250\n          2.7K\n    Earl Grey\n2730\n3005\n2841.833\n          3.0K\n    Green Tea\n1339\n1476\n1398.750\n          1.5K\n    Lemon\n3851\n4418\n4080.833\n          4.4K\n    Mint\n1388\n1669\n1519.167\n          1.5K\n    Regular Espresso\n890\n1218\n1023.417\n          1.1K\n  \n  \n  \n\n\n\n\n\n\n\nThe bullet chart and sparklines with summary statistics are then combined together using the left_join() function in the dplyr package.\n\nbullet = coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  group_by(`Product`) %&gt;%\n  summarise(`Target` = sum(`Budget Sales`),\n            `Actual` = sum(`Sales`)) %&gt;%\n  ungroup() \n\nsales_data = sales_data %&gt;%\n  left_join(bullet)\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales') %&gt;%\n  gt_plt_bullet(column = Actual, \n                target = Target, \n                width = 28,\n                palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()\n\n\n\n\n\n  \n    \n    \n      Product\n      Min\n      Max\n      Average\n      Monthly Sales\n      Actual\n    \n  \n  \n    Amaretto\n1016\n1210\n1119.000\n          1.2K\n          \n    Caffe Latte\n1398\n1653\n1528.333\n          1.5K\n          \n    Caffe Mocha\n3322\n3828\n3613.917\n          3.7K\n          \n    Chamomile\n2967\n3395\n3217.417\n          3.3K\n          \n    Colombian\n5132\n5961\n5457.250\n          5.5K\n          \n    Darjeeling\n2926\n3281\n3112.667\n          3.0K\n          \n    Decaf Espresso\n3181\n3493\n3326.833\n          3.2K\n          \n    Decaf Irish Cream\n2463\n2901\n2648.250\n          2.7K\n          \n    Earl Grey\n2730\n3005\n2841.833\n          3.0K\n          \n    Green Tea\n1339\n1476\n1398.750\n          1.5K\n          \n    Lemon\n3851\n4418\n4080.833\n          4.4K\n          \n    Mint\n1388\n1669\n1519.167\n          1.5K\n          \n    Regular Espresso\n890\n1218\n1023.417\n          1.1K\n          \n  \n  \n  \n\n\n\n\n\n\n\n\n\nIn this sub-section, an interactive information dashboard is created using the reactable and reactablefmtr packages.\n\n\nSimilar to when using the gtExtras package, the list field is required when using the reactablefmtr package.\n\nreport = report %&gt;%\n  group_by(Product) %&gt;%\n  summarize(`Monthly Sales` = list(Sales))\n\nThe reactable() function in the reactable package and the react_sparkline() function in the reactablefmtr package in the “cell” argument are then used to plot the sparklines.\n\nreactable(\n  report,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(report)\n      )))\n\n\n\n\n\n\n\n\n\nThe default page size is 10. The default setting is changed to 13.\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(report)\n    )\n  )\n)\n\n\n\n\n\n\n\n\n\nThe “highlight_points” argument is added to show the minimum and maximum value points, and the “label” argument is added to label the first and last values.\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        labels = c(\"first\", \"last\")\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\n\n\nThe “statline” argument is added to show the mean level.\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        statline = \"mean\"\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\n\n\nThe “bandline” argument can be added to show a band of range (instead of showing a single reference line).\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        line_width = 1,\n        bandline = \"innerquartiles\",\n        bandline_color = \"green\"\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\n\n\nInstead of displaying the values as sparklines, they can also be displayed as sparkbars using the react_sparkbar() function in the reactablefmtr package in the “cell” argument.\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkbar(\n        report,\n        highlight_bars = highlight_bars(\n          min = \"red\", max = \"blue\"),\n        bandline = \"innerquartiles\",\n        statline = \"mean\")\n    )\n  )\n)\n\n\n\n\n\n\n~~~ End of Hands-on Exercise 10 ~~~"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#overview-and-learning-outcomes",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#overview-and-learning-outcomes",
    "title": "Hands-on Exercise 10",
    "section": "",
    "text": "This hands-on exercise is based on Chapter 31 of the R for Visual Analytics book.\nThe learning outcomes are:\n\nCreate a bullet chart using the ggplot2 package.\nCreate sparklines using the ggplot2 package.\nBuild industry standard dashboard using R Shiny."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#getting-started",
    "title": "Hands-on Exercise 10",
    "section": "",
    "text": "In this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nlubridate for dealing with dates;\nggthemes for extra themes, geoms, and scales for ggplot2;\ngtExtras for additional helper functions to assist in creating beautiful tables with gt, a package specially designed for anyone to make wonderful-looking tables using the R programming language;\nreactable for functions to create interactive data tables for R, based on the React Table library and made with reactR;\nreactablefmtr for various features to streamline and enhance the styling of interactive reactable tables with easy-to-use and highly-customizable functions and themes; and\nRODBC for accessing databases.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed. If yes, they are then loaded into the R environment. If no, they are installed, then loaded into the R environment.\n\npacman::p_load(tidyverse, lubridate,\n               ggthemes, gtExtras,\n               reactable, reactablefmtr,\n               RODBC)\n\nThe dataui package for building interactive sparklines is also installed.\n\nremotes::install_github(\"timelyportfolio/dataui\")\nlibrary(dataui)\n\n\n\n\nThe dataset for this hands-on exercise is imported into the R environment using the odbcConnectAccess() function in the RODBCpackage.\n\ncon = odbcConnectAccess2007('data/Coffee Chain.mdb')\ncoffeechain = sqlFetch(con, 'CoffeeChain Query')\nwrite_rds(coffeechain, \"data/CoffeeChain.rds\")\nodbcClose(con)\n\nThe data frame, coffeechain, has 19 columns and 4,248 rows. It is saved in the rds file format in imported into the R environment.\n\ncoffeechain = read_rds(\"data/rds/CoffeeChain.rds\")\n\n\n\n\nThe Sales and Budgeted Sales are aggregated at the Product level using the group_by() and summarise() functions in the dplyr package.\n\nproduct = coffeechain %&gt;%\n  group_by(`Product`) %&gt;%\n  summarise(`target` = sum(`Budget Sales`),\n            `current` = sum(`Sales`)) %&gt;%\n  ungroup()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#plotting-bullet-chart",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#plotting-bullet-chart",
    "title": "Hands-on Exercise 10",
    "section": "",
    "text": "A bullet chart is plotted using functions in the ggplot2 package, i.e., geom_col() and geom_errorbar().\n\nggplot(product, aes(Product, current)) + \n  geom_col(aes(Product, max(target) * 1.01),\n           fill=\"grey85\", width=0.85) +\n  geom_col(aes(Product, target * 0.75),\n           fill=\"grey60\", width=0.85) +\n  geom_col(aes(Product, target * 0.5),\n           fill=\"grey50\", width=0.85) +\n  geom_col(aes(Product, current), \n           width=0.35,\n           fill = \"black\") + \n  geom_errorbar(aes(y = target,\n                    x = Product, \n                    ymin = target,\n                    ymax= target), \n                width = .4,\n                colour = \"red\",\n                size = 1) +\n  coord_flip()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#plotting-sparklines-using-ggplot2",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#plotting-sparklines-using-ggplot2",
    "title": "Hands-on Exercise 10",
    "section": "",
    "text": "To plot sparklines, the data is manipulated to generate a table containing the monthly sales values by product type.\n\nsales_report &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  mutate(Month = month(Date)) %&gt;%\n  group_by(Month, Product) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup() %&gt;%\n  select(Month, Product, Sales)\n\nThe minimum, maximum, end of month sales values as well as the 25th and 75th quantile values are then derived.\n\nmins = group_by(sales_report, Product) %&gt;% \n  slice(which.min(Sales))\n\nmaxs = group_by(sales_report, Product) %&gt;% \n  slice(which.max(Sales))\n\nends = group_by(sales_report, Product) %&gt;% \n  filter(Month == max(Month))\n\nquarts = sales_report %&gt;%\n  group_by(Product) %&gt;%\n  summarise(quart1 = quantile(Sales, \n                              0.25),\n            quart2 = quantile(Sales, \n                              0.75)) %&gt;%\n  right_join(sales_report)\n\nA sparklines graph is then plotted.\n\nggplot(sales_report, aes(x=Month, y=Sales)) + \n  facet_grid(Product ~ ., scales = \"free_y\") + \n  geom_ribbon(data = quarts, aes(ymin = quart1, max = quart2), \n              fill = 'grey90') +\n  geom_line(size=0.3) +\n  geom_point(data = mins, col = 'red') +\n  geom_point(data = maxs, col = 'blue') +\n  geom_text(data = mins, aes(label = Sales), vjust = -1) +\n  geom_text(data = maxs, aes(label = Sales), vjust = 2.5) +\n  geom_text(data = ends, aes(label = Sales), hjust = 0, nudge_x = 0.5) +\n  geom_text(data = ends, aes(label = Product), hjust = 0, nudge_x = 1.0) +\n  expand_limits(x = max(sales_report$Month) + \n                  (0.25 * (max(sales_report$Month) - min(sales_report$Month)))) +\n  scale_x_continuous(breaks = seq(1, 12, 1)) +\n  scale_y_continuous(expand = c(0.1, 0)) +\n  theme_tufte(base_size = 3, base_family = \"Helvetica\") +\n  theme(axis.title=element_blank(), axis.text.y = element_blank(), \n        axis.ticks = element_blank(), strip.text = element_blank())"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#static-information-dashboard-design-gt-and-gtextras-packages",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#static-information-dashboard-design-gt-and-gtextras-packages",
    "title": "Hands-on Exercise 10",
    "section": "",
    "text": "In this sub-section, a static information dashboard is created using the gt and gtExtras packages.\n\n\nA bullet chart using the gt() and gt_plt_bullet() functions in the gt and gtExtras packages respectively.\n\nproduct %&gt;%\n  gt::gt() %&gt;%\n  gt_plt_bullet(column = current, \n              target = target, \n              width = 60,\n              palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()\n\n\n\n\n\n  \n    \n    \n      Product\n      current\n    \n  \n  \n    Amaretto\n          \n    Caffe Latte\n          \n    Caffe Mocha\n          \n    Chamomile\n          \n    Colombian\n          \n    Darjeeling\n          \n    Decaf Espresso\n          \n    Decaf Irish Cream\n          \n    Earl Grey\n          \n    Green Tea\n          \n    Lemon\n          \n    Mint\n          \n    Regular Espresso\n          \n  \n  \n  \n\n\n\n\n\n\n\nThe sales report of products by month for 2013 is extracted using the mutate(), filter(), group_by(), and summarise() functions in the dplyr package.\n\nreport = coffeechain %&gt;%\n  mutate(Year = year(Date)) %&gt;%\n  filter(Year == \"2013\") %&gt;%\n  mutate (Month = month(Date, \n                        label = TRUE, \n                        abbr = TRUE)) %&gt;%\n  group_by(Product, Month) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup()\n\nThe report data frame is converted into list columns.\n\nreport %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\n# A tibble: 13 × 2\n   Product           `Monthly Sales`\n   &lt;chr&gt;             &lt;list&gt;         \n 1 Amaretto          &lt;dbl [12]&gt;     \n 2 Caffe Latte       &lt;dbl [12]&gt;     \n 3 Caffe Mocha       &lt;dbl [12]&gt;     \n 4 Chamomile         &lt;dbl [12]&gt;     \n 5 Colombian         &lt;dbl [12]&gt;     \n 6 Darjeeling        &lt;dbl [12]&gt;     \n 7 Decaf Espresso    &lt;dbl [12]&gt;     \n 8 Decaf Irish Cream &lt;dbl [12]&gt;     \n 9 Earl Grey         &lt;dbl [12]&gt;     \n10 Green Tea         &lt;dbl [12]&gt;     \n11 Lemon             &lt;dbl [12]&gt;     \n12 Mint              &lt;dbl [12]&gt;     \n13 Regular Espresso  &lt;dbl [12]&gt;     \n\n\n\n\nThe monthly sales is plotted as sparklines using the gt() and gt_plt_sparkline() functions in the gt and gtExtras packages respectively.\n\nreport %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\") %&gt;%\n   gt() %&gt;%\n   gt_plt_sparkline('Monthly Sales',\n                    same_limit = FALSE)\n\n\n\n\n\n  \n    \n    \n      Product\n      Monthly Sales\n    \n  \n  \n    Amaretto\n          1.2K\n    Caffe Latte\n          1.5K\n    Caffe Mocha\n          3.7K\n    Chamomile\n          3.3K\n    Colombian\n          5.5K\n    Darjeeling\n          3.0K\n    Decaf Espresso\n          3.2K\n    Decaf Irish Cream\n          2.7K\n    Earl Grey\n          3.0K\n    Green Tea\n          1.5K\n    Lemon\n          4.4K\n    Mint\n          1.5K\n    Regular Espresso\n          1.1K\n  \n  \n  \n\n\n\n\n\n\n\nThe summary statistics are calculated using the group_by() and summarise() functions in the dplyr package, as well as the gt() and fmt_number() functions in the gt package.\n\nreport %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            ) %&gt;%\n  gt() %&gt;%\n  fmt_number(columns = 4,\n    decimals = 2)\n\n\n\n\n\n  \n    \n    \n      Product\n      Min\n      Max\n      Average\n    \n  \n  \n    Amaretto\n1016\n1210\n1,119.00\n    Caffe Latte\n1398\n1653\n1,528.33\n    Caffe Mocha\n3322\n3828\n3,613.92\n    Chamomile\n2967\n3395\n3,217.42\n    Colombian\n5132\n5961\n5,457.25\n    Darjeeling\n2926\n3281\n3,112.67\n    Decaf Espresso\n3181\n3493\n3,326.83\n    Decaf Irish Cream\n2463\n2901\n2,648.25\n    Earl Grey\n2730\n3005\n2,841.83\n    Green Tea\n1339\n1476\n1,398.75\n    Lemon\n3851\n4418\n4,080.83\n    Mint\n1388\n1669\n1,519.17\n    Regular Espresso\n890\n1218\n1,023.42\n  \n  \n  \n\n\n\n\n\n\n\nThe derived statistics are then combined with the table using the left_join() function in the dplyr package.\n\nspark = report %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\nsales = report %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            )\n\nsales_data = left_join(sales, spark)\n\n\n\n\nThe combined table is then plotted using the gt() and gt_plt_sparkline() functions in the gt and gtExtras packages respectively.\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales',\n                   same_limit = FALSE)\n\n\n\n\n\n  \n    \n    \n      Product\n      Min\n      Max\n      Average\n      Monthly Sales\n    \n  \n  \n    Amaretto\n1016\n1210\n1119.000\n          1.2K\n    Caffe Latte\n1398\n1653\n1528.333\n          1.5K\n    Caffe Mocha\n3322\n3828\n3613.917\n          3.7K\n    Chamomile\n2967\n3395\n3217.417\n          3.3K\n    Colombian\n5132\n5961\n5457.250\n          5.5K\n    Darjeeling\n2926\n3281\n3112.667\n          3.0K\n    Decaf Espresso\n3181\n3493\n3326.833\n          3.2K\n    Decaf Irish Cream\n2463\n2901\n2648.250\n          2.7K\n    Earl Grey\n2730\n3005\n2841.833\n          3.0K\n    Green Tea\n1339\n1476\n1398.750\n          1.5K\n    Lemon\n3851\n4418\n4080.833\n          4.4K\n    Mint\n1388\n1669\n1519.167\n          1.5K\n    Regular Espresso\n890\n1218\n1023.417\n          1.1K\n  \n  \n  \n\n\n\n\n\n\n\nThe bullet chart and sparklines with summary statistics are then combined together using the left_join() function in the dplyr package.\n\nbullet = coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  group_by(`Product`) %&gt;%\n  summarise(`Target` = sum(`Budget Sales`),\n            `Actual` = sum(`Sales`)) %&gt;%\n  ungroup() \n\nsales_data = sales_data %&gt;%\n  left_join(bullet)\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales') %&gt;%\n  gt_plt_bullet(column = Actual, \n                target = Target, \n                width = 28,\n                palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()\n\n\n\n\n\n  \n    \n    \n      Product\n      Min\n      Max\n      Average\n      Monthly Sales\n      Actual\n    \n  \n  \n    Amaretto\n1016\n1210\n1119.000\n          1.2K\n          \n    Caffe Latte\n1398\n1653\n1528.333\n          1.5K\n          \n    Caffe Mocha\n3322\n3828\n3613.917\n          3.7K\n          \n    Chamomile\n2967\n3395\n3217.417\n          3.3K\n          \n    Colombian\n5132\n5961\n5457.250\n          5.5K\n          \n    Darjeeling\n2926\n3281\n3112.667\n          3.0K\n          \n    Decaf Espresso\n3181\n3493\n3326.833\n          3.2K\n          \n    Decaf Irish Cream\n2463\n2901\n2648.250\n          2.7K\n          \n    Earl Grey\n2730\n3005\n2841.833\n          3.0K\n          \n    Green Tea\n1339\n1476\n1398.750\n          1.5K\n          \n    Lemon\n3851\n4418\n4080.833\n          4.4K\n          \n    Mint\n1388\n1669\n1519.167\n          1.5K\n          \n    Regular Espresso\n890\n1218\n1023.417\n          1.1K"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#interactive-information-dashboard-design-reactable-and-reactablefmtr-packages",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#interactive-information-dashboard-design-reactable-and-reactablefmtr-packages",
    "title": "Hands-on Exercise 10",
    "section": "",
    "text": "In this sub-section, an interactive information dashboard is created using the reactable and reactablefmtr packages.\n\n\nSimilar to when using the gtExtras package, the list field is required when using the reactablefmtr package.\n\nreport = report %&gt;%\n  group_by(Product) %&gt;%\n  summarize(`Monthly Sales` = list(Sales))\n\nThe reactable() function in the reactable package and the react_sparkline() function in the reactablefmtr package in the “cell” argument are then used to plot the sparklines.\n\nreactable(\n  report,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(report)\n      )))\n\n\n\n\n\n\n\n\n\nThe default page size is 10. The default setting is changed to 13.\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(report)\n    )\n  )\n)\n\n\n\n\n\n\n\n\n\nThe “highlight_points” argument is added to show the minimum and maximum value points, and the “label” argument is added to label the first and last values.\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        labels = c(\"first\", \"last\")\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\n\n\nThe “statline” argument is added to show the mean level.\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        statline = \"mean\"\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\n\n\nThe “bandline” argument can be added to show a band of range (instead of showing a single reference line).\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        line_width = 1,\n        bandline = \"innerquartiles\",\n        bandline_color = \"green\"\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\n\n\nInstead of displaying the values as sparklines, they can also be displayed as sparkbars using the react_sparkbar() function in the reactablefmtr package in the “cell” argument.\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkbar(\n        report,\n        highlight_bars = highlight_bars(\n          min = \"red\", max = \"blue\"),\n        bandline = \"innerquartiles\",\n        statline = \"mean\")\n    )\n  )\n)\n\n\n\n\n\n\n~~~ End of Hands-on Exercise 10 ~~~"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html",
    "title": "In-class Exercise 6",
    "section": "",
    "text": "&lt;placeholder&gt;\n~~~ End of In-class Exercise 6 ~~~"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608: Visual Analytics and Applications",
    "section": "",
    "text": "Welcome to ISSS608 Visual Analytics and Applications!\nIn this webpage, I am going to share my visual analytics learning journey.\n\n\n\n\n\nCredit: Prima Consulting UK"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "Singapore has come a long way since gaining independence in 1965. Without natural resources, human capital development through a well-planned education system is a critical part of Singapore’s transformation from third world to first.\nDespite the success, there is still a correlation between socio-economic status and education achievement, as well as ingrained perceptions that some schools are better than others.\nHence, there is a need to use data to analyse the performance of Singapore students across different subjects, and identify any relationships between the performance in various subjects and factors such as gender, socioeconomic status, and type of school.\n\n\n\nIn this take-home exercise, the objective is to improve on an original visualisation by a fellow course-mate by focusing on what works, what does not work, why they do not work, and how to make it better.\nThe specific tasks are:\n\nSelect a Take-home Exercise 1 prepared by a course-mate;\nCritic the submission in terms of clarity and aesthetics;\nPrepare a sketch for the alternative design using the data visualisation design principles and best practices from Lesson 1 and Lesson 2; and\nRemake the original design using ggplot2, ggplot2 extensions, and tidyverse packages.\n\n\n\n\n\n\n\nThe R packages used in this take-home exercise are:\n\nhaven for importing SAS files;\ntidyverse (i.e. readr, tidyr, dplyr, ggplot2) for performing data science tasks such as importing, tidying, and wrangling data, as well as creating graphics based on The Grammar of Graphics;\nreshape2 for transforming data between wide and long formats;\nggthemes for extra themes, geoms, and scales for ggplot2;\nggridges for creating ridgeline plots;\nggdist for visualising distributions and uncertainty; and\nggpubr for creating publication ready ggplot2 plots.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed in the computer. If yes, they are then loaded into the R environment. If no, they are installed, and then loaded into the R environment.\n\npacman::p_load(haven, tidyverse, reshape2,\n               ggthemes, ggridges, ggdist,\n               ggpubr)\n\n\n\n\nThe OECD Programme for International Student Assessment (PISA) measures how well 15-year-old students in different countries are “prepared to meet the challenges of today’s knowledge societies” by looking at “their ability to use their knowledge and skills to meet real-life challenges”. The PISA surveys take place very three years, the latest being conducted in 2022.\nThe PISA 2022 database contains the full set of responses from individual students, school principals, and parents. There are a total of five data files and their contents are as follows:\n\nStudent questionnaire data file;\nSchool questionnaire data file;\nTeacher questionnaire data file;\nCognitive item data file; and\nQuestionnaire timing data file.\n\nFor the purpose of this take-home exercise, the “Student questionnaire data file” is used.\n\n\n\n\n\n\nThe dataset used in this take-home exercise is the 2022 PISA student questionnaire data file, cy08msp_stu_qqq.sas7bdat, which is in the SAS file format.\nThe file is imported into the R environment using the read_sas() function in the haven package and stored as the R object, stu_qqq.\n\nstu_qqq = read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\nThe tibble data frame, stu_qqq, has 1,279 columns (variables) and 613,744 rows (observations).\n\n\n\nThere are 6,606 rows with the country code (i.e., CNT) value of “SGP”, which represents Singapore. This count is cross-verified by the information provided in the “CY08MSP_STU_QQQ” sheet in the codebook. The codebook also stated that Singapore students’ made up 1.0763% of the entire global student population who took part in the 2022 PISA.\nThe filter() function in the dplyr package is used to obtain these rows, and stored as the R object, stu_qqq_SG.\n\nstu_qqq_SG = stu_qqq %&gt;% filter(CNT == \"SGP\")\n\nThe tibble data frame, stu_qqq_SG, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(stu_qqq_SG, \"data/stu_qqq_SG.rds\")\n\n\nstu_qqq_SG = read_rds(\"data/stu_qqq_SG.rds\")\n\n\n\n\nUsing the chosen submission, the dataset is prepared according to the original submission.\n\n\nCode\n#  PREPARE MAIN DATA-FRAME\nstudent_columns = \"CNTSTUID\"\ngender_columns = \"ST004D01T\"\nschool_columns = \"CNTSCHID\"\n\neducation_column_mother = \"ST005Q01JA\"\neducation_column_father = \"ST007Q01JA\"\n\ntraining_column_mother = \"ST006Q01JA\"\ntraining_column_father = \"ST008Q01JA\"\npossession_room_column = \"ST250Q01JA\"\npossession_computer_column = \"ST250Q02JA\"\npossession_software_column = \"ST250Q03JA\"\npossession_phone_column = \"ST250Q04JA\"\npossession_internet_column = \"ST250Q05JA\"\npossession_book_column = \"ST255Q01JA\"\n\nmath_columns = c(\"PV1MATH\", \"PV2MATH\", \"PV3MATH\", \"PV4MATH\", \"PV5MATH\", \"PV6MATH\", \"PV7MATH\", \"PV8MATH\", \"PV9MATH\", \"PV10MATH\")\nreading_columns = c(\"PV1READ\", \"PV2READ\", \"PV3READ\", \"PV4READ\", \"PV5READ\", \"PV6READ\", \"PV7READ\", \"PV8READ\", \"PV9READ\", \"PV10READ\")\nscience_columns = c(\"PV1SCIE\", \"PV2SCIE\", \"PV3SCIE\", \"PV4SCIE\", \"PV5SCIE\", \"PV6SCIE\", \"PV7SCIE\", \"PV8SCIE\", \"PV9SCIE\", \"PV10SCIE\")\n\nstudent_ID = stu_qqq_SG[, student_columns, drop = FALSE]\ngender = stu_qqq_SG[, gender_columns, drop = FALSE]\nschool_ID = stu_qqq_SG[, school_columns, drop = FALSE]\neducation_mother = stu_qqq_SG[, education_column_mother, drop = FALSE]\neducation_father = stu_qqq_SG[, education_column_father, drop = FALSE]\n\ntraining_mother = stu_qqq_SG[, training_column_mother, drop = FALSE]\ntraining_father = stu_qqq_SG[, training_column_father, drop = FALSE]\npossession_room = stu_qqq_SG[, possession_room_column, drop = FALSE]\npossession_computer = stu_qqq_SG[, possession_computer_column, drop = FALSE]\npossession_software = stu_qqq_SG[, possession_software_column, drop = FALSE]\npossession_phone = stu_qqq_SG[, possession_phone_column, drop = FALSE]\npossession_internet = stu_qqq_SG[, possession_internet_column, drop = FALSE]\npossession_book = stu_qqq_SG[, possession_book_column, drop = FALSE]\n\nmath_avg = rowMeans(stu_qqq_SG[, math_columns, drop = FALSE])\nreading_avg = rowMeans(stu_qqq_SG[, reading_columns, drop = FALSE])\nscience_avg = rowMeans(stu_qqq_SG[, science_columns, drop = FALSE])\n\nstu_df = data.frame(Student_ID = student_ID,\n  Gender = gender,\n  School_ID = school_ID,\n  Education_mother = education_mother,\n  Education_father = education_father,\n \n#  Training_mother = training_mother,\n#  Training_father = training_father,\n  Possession_room = possession_room,\n  Possession_computer = possession_computer,\n  Possession_software = possession_software,\n  Possession_phone = possession_phone,\n  Possession_internet = possession_internet,\n  Possession_book = possession_book,\n  \n  Math_Average = round(math_avg,digits=2),\n  Reading_Average = round(reading_avg,digits=2),\n  Science_Average = round(science_avg,digits=2),\nAverage_score=round(((math_avg+reading_avg+science_avg)/3),digits=2))\n\nnames(stu_df) = c(\"Student_ID\",\"Gender\",\"School_ID\",\"Education_mother\",\n                   \"Education_father\",\"Possession_room\",\"Possession_computer\",\n                   \"Possession_software\",\"Possession_phone\",\n                   \"Possession_internet\",\"Possession_book\",\"Math_Average\",\n                   \"Reading_Average\",\"Science_Average\",\"Average_Score\")\n\n\nThe finalised tibble data frames, stu_df is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(stu_df, \"data/stu_df.rds\")\n\n\nstu_df = read_rds(\"data/stu_df.rds\")\n\n\n\n\n\n\n\n\n\nThe original visualisation at Section 3 (Distribution of Singapore Students’ Performance) of the submission is reproduced below. The plot on the average score (not specific subjects) is chosen.\n\n\n\n\n\nThe observation made:\n\n“In this case, all the histogram are left skewed. The left-skewed (negative skewness) nature of the histogram indicates that there is a concentration of scores towards the higher end, with a tail extending towards lower scores. The mean is influenced by these lower scores, pulling it towards the left. The median, being less sensitive to extreme values, is positioned higher in the distribution.”\n\n\n\n\nClarity\n\n😊 Right Visualisation. The use of a histogram provides a clear representation of the distribution of the average scores among the students, and the use of the mean and median vertical dashed lines shows the left-skewness of the distribution.\n😕 Missing Key Message in Title. The key message stated in the observation is not included in the title (which is factual). Stating the key message in the title would help the reader zoom in on the intended message of the plot.\n😕 Missing Source Caption. It would be useful to credit the source of the data directly in the plot to give the reader the appropriate context for interpreting plot.\n\nAesthetics\n\n😊 Graphical Integrity Maintained. The y-axis starts from zero, ensuring that the histogram is not distorted.\n😊 Simple Colours & Font Used. The plot uses an easy-to-read font and colour. However, this can be enhanced by using a professional theme.\n😕 Overcrowding of Details & Labels. The x-axis labels use values that are too detailed (not rounded up). Also, each column of the histogram is labelled with the corresponding frequency for that range of scores. These details are overwhelming and distract from the intended message. The placement of the mean and median values can also be better adjusted to avoid overlapping with the histogram. The decimal values in the mean and median may also be removed for simplicity to improve readability given that the difference in the two values is more than 1 point.\n😕 Orientation of Label. The y-axis title can be rotated for easier reading.\n\n\n\n\nThe sketch of the alternative design is as follows:\n\n\n\n\n\n\n\n\nAn improved plot is made as follows:\n\nImproved PlotCode\n\n\n\n\n\n\n\n\n\n\naverage_sci = mean(stu_df$Average_Score, na.rm = TRUE)\nmedian_sci = median(stu_df$Average_Score, na.rm = TRUE)\n\nggplot(stu_df, aes(x = Average_Score)) +\n  geom_histogram(bins = 20, \n                 fill = \"cadetblue3\", \n                 color = \"grey95\") +\n  labs(title = \"More Students Score Above Mean Average Score\",\n       subtitle = \"Distribution of Average Scores\",\n       x = \"Average Score\",\n       y = \"Frequency\", \n       caption = \"Source: PISA 2022\") +\n  theme_minimal() +\n  scale_x_continuous(n.breaks = 10) +\n  scale_y_continuous(n.breaks = 6) +\n  geom_vline(xintercept = average_sci, color = \"navy\", linetype = \"dashed\", size = 1) +\n  geom_text(aes(x = average_sci-60, y = 900, label = paste(\"Mean:\", round(average_sci))), vjust = 2, color = \"navy\",size =4) +\n  geom_vline(xintercept = median_sci, color = \"maroon\", linetype = \"dotdash\", size = 1) +\n  geom_text(aes(x = median_sci+100, y = 900, label = paste(\"Median:\", round(median_sci))), vjust = 2, color = \"maroon\",size=4) +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\n\n\n\n\n\n\nThe original visualisation at Section 4.1 (Performance by Gender) of the submission is reproduced below. Again, the plot on the average score (not specific subjects) is chosen.\n\n\n\n\n\nThe relevant observations made:\n\n“From the above violin plots, we can conclude:\n\nMale plot is more spread out, which indicates greater variability in scores within the male group\nFemale group and male group have similar average performance.\nAlmost all the distribution are left skewed, indicating a concentration of students with relatively higher scores, but a few students have much lower scores.”\n\n\n\n\n\nClarity\n\n😊😕 (Almost) Right Visualisation. The use of two violin plots shows the distributions of the average scores for female and male students. The use of the box plots in the middle of the violin plots shows the median average scores. However, as the two violin plots do not overlap, the comparison of average scores based on gender is more difficult to make visually. A density plot grouped by gender, with vertical lines added to show the median average scores, may be more appropriate for comparison purposes.\n😕 Missing Key Message in Title. The key message stated in the observation is not included in the title (which is factual). Stating the key message in the title would help the reader zoom in on the intended message of the plot.\n😕 Missing Source Caption. It would be useful to credit the source of the data directly in the plot to give the reader the appropriate context for interpreting plot.\n\nAesthetics\n\n😊 Simple Colours & Font Used. The plot uses an easy-to-read font and the stereotypical colours associated with each gender (pink for female; blue for male). However, this can be enhanced by using a simpler/minimalist theme for the background to keep the focus on the coloured density plots.\n😕 Redundant Legend. The legend is redundant given that the x-axis is labelled.\n😕 Unclear Labels for Mean and Median Values. The subtitle states the colours corresponding to the mean and median values for each gender which requires the reader to read and match accordingly, and reconfirm each time he/she views the plot. This is less intuitive than putting the labels together with the values.\n😕 Orientation of Label. The y-axis title can be rotated for easier reading.\n😕 Mis-titled Y-axis. The y-axis title was mistakenly labelled as “Reading Score” instead of “Average Score”.\n\n\n\n\nThe sketch of the alternative design is as follows:\n\n\n\n\n\n\n\n\nAn improved plot is made as follows:\n\nImproved PlotCode\n\n\n\n\n\n\n\n\n\n\nstu_df$Gender = ifelse(\n  stu_df$Gender == 1, \n  \"Female\", \"Male\")\n\nf = stu_df %&gt;%\n  filter(Gender == \"Female\")\n\nm = stu_df %&gt;%\n  filter(Gender == \"Male\")\n\nstu_df = stu_df %&gt;%\n  mutate(Gender = fct_relevel(Gender, \n                               \"Female\", \n                               \"Male\"))\n\nggplot(stu_df,\n       aes(x = Average_Score,\n           fill = Gender)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(aes(xintercept=median(f$Average_Score)),\n             color=\"red\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) +\n  geom_vline(aes(xintercept=median(m$Average_Score)),\n             color=\"blue\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) +\n  ggtitle(label = \"On Average, Boys and Girls Perform Similarly\",\n          subtitle= \"Distribution of Average Scores by Gender\") + \n  labs(caption = \"Source: PISA 2022\") +\n  ylab(\"Density\") + xlab(\"Average Score\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1)) +\n  theme_minimal() +\n  geom_vline(aes(xintercept=median(f$Average_Score)),\n             color=\"#CC3366\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) +\n  geom_vline(aes(xintercept=median(m$Average_Score)),\n             color=\"#033336\", \n             linetype=\"dotdash\", \n             linewidth=1,\n             alpha = 0.5) +\n  geom_text(aes(x = 690, y = 0.0045, label = paste(\"Median:\", round(median(m$Average_Score)))), vjust = 2, color = \"#033336\",size =4) +\n  geom_text(aes(x = 480, y = 0.0045, label = paste(\"Median:\", round(median(f$Average_Score)))), vjust = 2, color = \"#CC3366\",size=4) +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\n\nNote: The same makeover approach is also applicable to Section 4.3.3 (Performance Distribution by Internet Access) as it also uses violin plots.\n\n\n\n\n\n\n\nThe original visualisation at Section 4.3.2 (Distribution of by Number of Book in Students’ Home) of the submission is reproduced below. The plot on the reading score is chosen.\n\n\n\n\n\nThe observation made:\n\n“From the density plot, we can interpret that:\n\nFor students with no books in their homes, the density plot is right-skewed. This suggests that a lack of books is associated with a concentration of lower academic scores. The right skewness indicates that the majority of students in this group may have below-average scores.\nAs the number of books in the home increases, the density plot becomes more left-skewed. This trend suggests a positive correlation between the abundance of books and higher academic performance. The left skewness indicates a concentration of higher scores, with more students performing above the average.\nThe observation that the mean score of students increases as the number of books in the home increases aligns with the general trend of a left-skewed density plot. This indicates that, on average, students with access to a greater number of books tend to achieve higher academic scores.\nA notable deviation from the general trend occurs for students whose homes have “more than 500 books.” In this category, the mean score decreases, contrary to the overall positive relationship observed. This suggests that there may be diminishing returns in terms of academic performance when the number of books surpasses a certain threshold.\n\nIn summary, the density plot illustrates a positive association between the number of books in the home and student performance. However, the deviation observed for the “more than 500 books” category suggests a nuanced relationship, highlighting the need to consider optimal conditions for leveraging the positive influence of books on academic outcomes.”\n\n\n\n\nClarity\n\n😊😕 (Almost) Right Visualisation. The use of density plots, with vertical lines showing the median reading scores, is useful for comparing the distributions of reading scores based on the number of books at home. However, this can be further enhanced with the use of a ridgelines plot.\n😕 Missing Key Message in Title. The key message stated in the observation is not included in the title (which is factual). Stating the key message in the title would help the reader zoom in on the intended message of the plot.\n😕 Difficult to Compare Distributions. Given that there are many categories, it is difficult to compare the distributions. It may be useful to showcase the quartile cut-offs to allow for better comparisons across different density plots.\n😕 Missing Source Caption. It would be useful to credit the source of the data directly in the plot to give the reader the appropriate context for interpreting plot.\n\nAesthetics\n\n😕 Unclear Colours Used. The plot uses an easy-to-read font. However, the choice of gradient colours is not appropriate as the answers form a categorical, not continuous, variable. The use of gradient colours does not allow the reader to differentiate the distinct categories of answers to the question in the questionnaire on the number of books at home.\n😕 Lack of Labels for Categories. The categories representing different ranges of the number of books at home requires a separate table for interpretation. This inconveniences the reader. Hence, it would be better to label each density plot directly for easier identification.\n😕 No Labels for Mean Values. The values of the dashed vertical lines are not labelled. It may be useful to label some of them to allow for direct comparison. Alternatively, to prevent clutter, it may be useful to add just two vertical lines - overall mean and overall median for broad comparisons.\n😕 Orientation of Label. The y-axis title can be rotated for easier reading.\n\n\n\n\nThe sketch of the alternative design is as follows:\n\n\n\n\n\n\n\n\nAn improved plot is made as follows:\n\nImproved PlotCode\n\n\n\n\n\n\n\n\n\n\nstu_df$Possession_book = as.character(stu_df$Possession_book)\nstu_df = stu_df %&gt;% \n  mutate(Possession_book = recode(Possession_book,\n                        \"1\" = \"0 Book\",\n                        \"2\" = \"1-10 Books\",\n                        \"3\" = \"11-25 Books\",\n                        \"4\" = \"26-100 Books\",\n                        \"5\" = \"101-200 Books\",\n                        \"6\" = \"201-500 Books\",\n                        \"7\" = \"&gt;500 Books\"))\n\nr = stu_df %&gt;% \n  select(Possession_book, Reading_Average) %&gt;%\n  na.omit() %&gt;%\n  mutate(Books = fct_relevel(Possession_book, \n                              \"0 Book\", \n                              \"1-10 Books\",\n                              \"11-25 Books\",\n                              \"26-100 Books\",\n                              \"101-200 Books\",\n                              \"201-500 Books\",\n                              \"&gt;500 Books\"))\n                        \nggplot(r, \n       aes(x = Reading_Average, \n           y = Books,\n           fill = factor(after_stat(quantile)))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 2,\n    quantile_lines = TRUE) +\n  scale_fill_brewer(type = \"seq\", palette = \"Set3\") +\n  ggtitle(label = \"More Books at Home Generally Correspond to\\nHigher Reading Scores\",\n          subtitle = \"Distribution of Reading Scores vis-a-vis Number of Books at Home\") +\n  xlab(\"Score\") + ylab(\"No.of\\nBooks\") +\n  labs(caption = \"Source: PISA 2022\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1), \n        legend.position = \"none\") +\n  geom_vline(aes(xintercept=median(Reading_Average)),\n             color=\"red\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) + \n  annotate(\"text\", x=690, y=0.75, label=paste(\"Overall Median:\", round(median(r$Reading_Average))), color = \"red\") +\n  geom_vline(aes(xintercept=mean(Reading_Average)),\n             color=\"blue\", \n             linetype=\"dotdash\", \n             linewidth=1,\n             alpha = 0.5) + \n  annotate(\"text\", x=420, y=0.75, label=paste(\"Overall Mean:\", round(mean(r$Reading_Average))), color = \"blue\")\n\n\n\n\n\nNote: The same makeover approach is also applicable to Section 4.3.1 (Performance Distribution by Highest Level of Schooling of Parents) as it also uses density plots.\n\n\n\n\n\n\nIn conclusion, the ggplot2 package is a powerful package for exploratory data analysis through visualisation. However, it is insufficient if effort is not made to select, clean, categorise, and moderate the data properly, and the visualisation process does not take into account information design, visual encoding, and interface design.\nThe step-by-step process of a data visualisation makeover in this take-home exercise highlights the importance of paying attention to details when making a plot. A personal checklist of important points to consider that are elucidated from the makeover attempts above are summarised as follows:\n\nDoes the plot have a clear key message?\nIs the correct type of chart used?\nIs the plot telling a lie? Are the scales of the axes distorted?\nAre the components of the graph (titles, axes, labels, tick marks, legend, grid lines, caption) accurate, relevant, and easy-to-view, or are they erroneous, confusing or redundant?\nIs annotation required?\nAre colours and fonts used appropriately in the plot?\nTake note of data-ink by reducing non-data-ink, and enhancing data-ink. The use of an appropriate theme helps!\nAim for a Quadrant I plot that is clear and beautiful!\n\n\n\n\n\nR for Visual Analytics.\nR for Data Science.\nFundamentals of Data Visualisation.\nPISA 2022 Database and Technical Report.\n\n~~~ End of Take-home Exercise 2 ~~~"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#introduction",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#introduction",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "Singapore has come a long way since gaining independence in 1965. Without natural resources, human capital development through a well-planned education system is a critical part of Singapore’s transformation from third world to first.\nDespite the success, there is still a correlation between socio-economic status and education achievement, as well as ingrained perceptions that some schools are better than others.\nHence, there is a need to use data to analyse the performance of Singapore students across different subjects, and identify any relationships between the performance in various subjects and factors such as gender, socioeconomic status, and type of school.\n\n\n\nIn this take-home exercise, the objective is to improve on an original visualisation by a fellow course-mate by focusing on what works, what does not work, why they do not work, and how to make it better.\nThe specific tasks are:\n\nSelect a Take-home Exercise 1 prepared by a course-mate;\nCritic the submission in terms of clarity and aesthetics;\nPrepare a sketch for the alternative design using the data visualisation design principles and best practices from Lesson 1 and Lesson 2; and\nRemake the original design using ggplot2, ggplot2 extensions, and tidyverse packages."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#getting-started",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "The R packages used in this take-home exercise are:\n\nhaven for importing SAS files;\ntidyverse (i.e. readr, tidyr, dplyr, ggplot2) for performing data science tasks such as importing, tidying, and wrangling data, as well as creating graphics based on The Grammar of Graphics;\nreshape2 for transforming data between wide and long formats;\nggthemes for extra themes, geoms, and scales for ggplot2;\nggridges for creating ridgeline plots;\nggdist for visualising distributions and uncertainty; and\nggpubr for creating publication ready ggplot2 plots.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed in the computer. If yes, they are then loaded into the R environment. If no, they are installed, and then loaded into the R environment.\n\npacman::p_load(haven, tidyverse, reshape2,\n               ggthemes, ggridges, ggdist,\n               ggpubr)\n\n\n\n\nThe OECD Programme for International Student Assessment (PISA) measures how well 15-year-old students in different countries are “prepared to meet the challenges of today’s knowledge societies” by looking at “their ability to use their knowledge and skills to meet real-life challenges”. The PISA surveys take place very three years, the latest being conducted in 2022.\nThe PISA 2022 database contains the full set of responses from individual students, school principals, and parents. There are a total of five data files and their contents are as follows:\n\nStudent questionnaire data file;\nSchool questionnaire data file;\nTeacher questionnaire data file;\nCognitive item data file; and\nQuestionnaire timing data file.\n\nFor the purpose of this take-home exercise, the “Student questionnaire data file” is used."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-wrangling",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "The dataset used in this take-home exercise is the 2022 PISA student questionnaire data file, cy08msp_stu_qqq.sas7bdat, which is in the SAS file format.\nThe file is imported into the R environment using the read_sas() function in the haven package and stored as the R object, stu_qqq.\n\nstu_qqq = read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\nThe tibble data frame, stu_qqq, has 1,279 columns (variables) and 613,744 rows (observations).\n\n\n\nThere are 6,606 rows with the country code (i.e., CNT) value of “SGP”, which represents Singapore. This count is cross-verified by the information provided in the “CY08MSP_STU_QQQ” sheet in the codebook. The codebook also stated that Singapore students’ made up 1.0763% of the entire global student population who took part in the 2022 PISA.\nThe filter() function in the dplyr package is used to obtain these rows, and stored as the R object, stu_qqq_SG.\n\nstu_qqq_SG = stu_qqq %&gt;% filter(CNT == \"SGP\")\n\nThe tibble data frame, stu_qqq_SG, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(stu_qqq_SG, \"data/stu_qqq_SG.rds\")\n\n\nstu_qqq_SG = read_rds(\"data/stu_qqq_SG.rds\")\n\n\n\n\nUsing the chosen submission, the dataset is prepared according to the original submission.\n\n\nCode\n#  PREPARE MAIN DATA-FRAME\nstudent_columns = \"CNTSTUID\"\ngender_columns = \"ST004D01T\"\nschool_columns = \"CNTSCHID\"\n\neducation_column_mother = \"ST005Q01JA\"\neducation_column_father = \"ST007Q01JA\"\n\ntraining_column_mother = \"ST006Q01JA\"\ntraining_column_father = \"ST008Q01JA\"\npossession_room_column = \"ST250Q01JA\"\npossession_computer_column = \"ST250Q02JA\"\npossession_software_column = \"ST250Q03JA\"\npossession_phone_column = \"ST250Q04JA\"\npossession_internet_column = \"ST250Q05JA\"\npossession_book_column = \"ST255Q01JA\"\n\nmath_columns = c(\"PV1MATH\", \"PV2MATH\", \"PV3MATH\", \"PV4MATH\", \"PV5MATH\", \"PV6MATH\", \"PV7MATH\", \"PV8MATH\", \"PV9MATH\", \"PV10MATH\")\nreading_columns = c(\"PV1READ\", \"PV2READ\", \"PV3READ\", \"PV4READ\", \"PV5READ\", \"PV6READ\", \"PV7READ\", \"PV8READ\", \"PV9READ\", \"PV10READ\")\nscience_columns = c(\"PV1SCIE\", \"PV2SCIE\", \"PV3SCIE\", \"PV4SCIE\", \"PV5SCIE\", \"PV6SCIE\", \"PV7SCIE\", \"PV8SCIE\", \"PV9SCIE\", \"PV10SCIE\")\n\nstudent_ID = stu_qqq_SG[, student_columns, drop = FALSE]\ngender = stu_qqq_SG[, gender_columns, drop = FALSE]\nschool_ID = stu_qqq_SG[, school_columns, drop = FALSE]\neducation_mother = stu_qqq_SG[, education_column_mother, drop = FALSE]\neducation_father = stu_qqq_SG[, education_column_father, drop = FALSE]\n\ntraining_mother = stu_qqq_SG[, training_column_mother, drop = FALSE]\ntraining_father = stu_qqq_SG[, training_column_father, drop = FALSE]\npossession_room = stu_qqq_SG[, possession_room_column, drop = FALSE]\npossession_computer = stu_qqq_SG[, possession_computer_column, drop = FALSE]\npossession_software = stu_qqq_SG[, possession_software_column, drop = FALSE]\npossession_phone = stu_qqq_SG[, possession_phone_column, drop = FALSE]\npossession_internet = stu_qqq_SG[, possession_internet_column, drop = FALSE]\npossession_book = stu_qqq_SG[, possession_book_column, drop = FALSE]\n\nmath_avg = rowMeans(stu_qqq_SG[, math_columns, drop = FALSE])\nreading_avg = rowMeans(stu_qqq_SG[, reading_columns, drop = FALSE])\nscience_avg = rowMeans(stu_qqq_SG[, science_columns, drop = FALSE])\n\nstu_df = data.frame(Student_ID = student_ID,\n  Gender = gender,\n  School_ID = school_ID,\n  Education_mother = education_mother,\n  Education_father = education_father,\n \n#  Training_mother = training_mother,\n#  Training_father = training_father,\n  Possession_room = possession_room,\n  Possession_computer = possession_computer,\n  Possession_software = possession_software,\n  Possession_phone = possession_phone,\n  Possession_internet = possession_internet,\n  Possession_book = possession_book,\n  \n  Math_Average = round(math_avg,digits=2),\n  Reading_Average = round(reading_avg,digits=2),\n  Science_Average = round(science_avg,digits=2),\nAverage_score=round(((math_avg+reading_avg+science_avg)/3),digits=2))\n\nnames(stu_df) = c(\"Student_ID\",\"Gender\",\"School_ID\",\"Education_mother\",\n                   \"Education_father\",\"Possession_room\",\"Possession_computer\",\n                   \"Possession_software\",\"Possession_phone\",\n                   \"Possession_internet\",\"Possession_book\",\"Math_Average\",\n                   \"Reading_Average\",\"Science_Average\",\"Average_Score\")\n\n\nThe finalised tibble data frames, stu_df is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(stu_df, \"data/stu_df.rds\")\n\n\nstu_df = read_rds(\"data/stu_df.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-visualisation-makeover",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-visualisation-makeover",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "The original visualisation at Section 3 (Distribution of Singapore Students’ Performance) of the submission is reproduced below. The plot on the average score (not specific subjects) is chosen.\n\n\n\n\n\nThe observation made:\n\n“In this case, all the histogram are left skewed. The left-skewed (negative skewness) nature of the histogram indicates that there is a concentration of scores towards the higher end, with a tail extending towards lower scores. The mean is influenced by these lower scores, pulling it towards the left. The median, being less sensitive to extreme values, is positioned higher in the distribution.”\n\n\n\n\nClarity\n\n😊 Right Visualisation. The use of a histogram provides a clear representation of the distribution of the average scores among the students, and the use of the mean and median vertical dashed lines shows the left-skewness of the distribution.\n😕 Missing Key Message in Title. The key message stated in the observation is not included in the title (which is factual). Stating the key message in the title would help the reader zoom in on the intended message of the plot.\n😕 Missing Source Caption. It would be useful to credit the source of the data directly in the plot to give the reader the appropriate context for interpreting plot.\n\nAesthetics\n\n😊 Graphical Integrity Maintained. The y-axis starts from zero, ensuring that the histogram is not distorted.\n😊 Simple Colours & Font Used. The plot uses an easy-to-read font and colour. However, this can be enhanced by using a professional theme.\n😕 Overcrowding of Details & Labels. The x-axis labels use values that are too detailed (not rounded up). Also, each column of the histogram is labelled with the corresponding frequency for that range of scores. These details are overwhelming and distract from the intended message. The placement of the mean and median values can also be better adjusted to avoid overlapping with the histogram. The decimal values in the mean and median may also be removed for simplicity to improve readability given that the difference in the two values is more than 1 point.\n😕 Orientation of Label. The y-axis title can be rotated for easier reading.\n\n\n\n\nThe sketch of the alternative design is as follows:\n\n\n\n\n\n\n\n\nAn improved plot is made as follows:\n\nImproved PlotCode\n\n\n\n\n\n\n\n\n\n\naverage_sci = mean(stu_df$Average_Score, na.rm = TRUE)\nmedian_sci = median(stu_df$Average_Score, na.rm = TRUE)\n\nggplot(stu_df, aes(x = Average_Score)) +\n  geom_histogram(bins = 20, \n                 fill = \"cadetblue3\", \n                 color = \"grey95\") +\n  labs(title = \"More Students Score Above Mean Average Score\",\n       subtitle = \"Distribution of Average Scores\",\n       x = \"Average Score\",\n       y = \"Frequency\", \n       caption = \"Source: PISA 2022\") +\n  theme_minimal() +\n  scale_x_continuous(n.breaks = 10) +\n  scale_y_continuous(n.breaks = 6) +\n  geom_vline(xintercept = average_sci, color = \"navy\", linetype = \"dashed\", size = 1) +\n  geom_text(aes(x = average_sci-60, y = 900, label = paste(\"Mean:\", round(average_sci))), vjust = 2, color = \"navy\",size =4) +\n  geom_vline(xintercept = median_sci, color = \"maroon\", linetype = \"dotdash\", size = 1) +\n  geom_text(aes(x = median_sci+100, y = 900, label = paste(\"Median:\", round(median_sci))), vjust = 2, color = \"maroon\",size=4) +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\n\n\n\n\n\n\nThe original visualisation at Section 4.1 (Performance by Gender) of the submission is reproduced below. Again, the plot on the average score (not specific subjects) is chosen.\n\n\n\n\n\nThe relevant observations made:\n\n“From the above violin plots, we can conclude:\n\nMale plot is more spread out, which indicates greater variability in scores within the male group\nFemale group and male group have similar average performance.\nAlmost all the distribution are left skewed, indicating a concentration of students with relatively higher scores, but a few students have much lower scores.”\n\n\n\n\n\nClarity\n\n😊😕 (Almost) Right Visualisation. The use of two violin plots shows the distributions of the average scores for female and male students. The use of the box plots in the middle of the violin plots shows the median average scores. However, as the two violin plots do not overlap, the comparison of average scores based on gender is more difficult to make visually. A density plot grouped by gender, with vertical lines added to show the median average scores, may be more appropriate for comparison purposes.\n😕 Missing Key Message in Title. The key message stated in the observation is not included in the title (which is factual). Stating the key message in the title would help the reader zoom in on the intended message of the plot.\n😕 Missing Source Caption. It would be useful to credit the source of the data directly in the plot to give the reader the appropriate context for interpreting plot.\n\nAesthetics\n\n😊 Simple Colours & Font Used. The plot uses an easy-to-read font and the stereotypical colours associated with each gender (pink for female; blue for male). However, this can be enhanced by using a simpler/minimalist theme for the background to keep the focus on the coloured density plots.\n😕 Redundant Legend. The legend is redundant given that the x-axis is labelled.\n😕 Unclear Labels for Mean and Median Values. The subtitle states the colours corresponding to the mean and median values for each gender which requires the reader to read and match accordingly, and reconfirm each time he/she views the plot. This is less intuitive than putting the labels together with the values.\n😕 Orientation of Label. The y-axis title can be rotated for easier reading.\n😕 Mis-titled Y-axis. The y-axis title was mistakenly labelled as “Reading Score” instead of “Average Score”.\n\n\n\n\nThe sketch of the alternative design is as follows:\n\n\n\n\n\n\n\n\nAn improved plot is made as follows:\n\nImproved PlotCode\n\n\n\n\n\n\n\n\n\n\nstu_df$Gender = ifelse(\n  stu_df$Gender == 1, \n  \"Female\", \"Male\")\n\nf = stu_df %&gt;%\n  filter(Gender == \"Female\")\n\nm = stu_df %&gt;%\n  filter(Gender == \"Male\")\n\nstu_df = stu_df %&gt;%\n  mutate(Gender = fct_relevel(Gender, \n                               \"Female\", \n                               \"Male\"))\n\nggplot(stu_df,\n       aes(x = Average_Score,\n           fill = Gender)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(aes(xintercept=median(f$Average_Score)),\n             color=\"red\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) +\n  geom_vline(aes(xintercept=median(m$Average_Score)),\n             color=\"blue\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) +\n  ggtitle(label = \"On Average, Boys and Girls Perform Similarly\",\n          subtitle= \"Distribution of Average Scores by Gender\") + \n  labs(caption = \"Source: PISA 2022\") +\n  ylab(\"Density\") + xlab(\"Average Score\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1)) +\n  theme_minimal() +\n  geom_vline(aes(xintercept=median(f$Average_Score)),\n             color=\"#CC3366\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) +\n  geom_vline(aes(xintercept=median(m$Average_Score)),\n             color=\"#033336\", \n             linetype=\"dotdash\", \n             linewidth=1,\n             alpha = 0.5) +\n  geom_text(aes(x = 690, y = 0.0045, label = paste(\"Median:\", round(median(m$Average_Score)))), vjust = 2, color = \"#033336\",size =4) +\n  geom_text(aes(x = 480, y = 0.0045, label = paste(\"Median:\", round(median(f$Average_Score)))), vjust = 2, color = \"#CC3366\",size=4) +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\n\nNote: The same makeover approach is also applicable to Section 4.3.3 (Performance Distribution by Internet Access) as it also uses violin plots.\n\n\n\n\n\n\n\nThe original visualisation at Section 4.3.2 (Distribution of by Number of Book in Students’ Home) of the submission is reproduced below. The plot on the reading score is chosen.\n\n\n\n\n\nThe observation made:\n\n“From the density plot, we can interpret that:\n\nFor students with no books in their homes, the density plot is right-skewed. This suggests that a lack of books is associated with a concentration of lower academic scores. The right skewness indicates that the majority of students in this group may have below-average scores.\nAs the number of books in the home increases, the density plot becomes more left-skewed. This trend suggests a positive correlation between the abundance of books and higher academic performance. The left skewness indicates a concentration of higher scores, with more students performing above the average.\nThe observation that the mean score of students increases as the number of books in the home increases aligns with the general trend of a left-skewed density plot. This indicates that, on average, students with access to a greater number of books tend to achieve higher academic scores.\nA notable deviation from the general trend occurs for students whose homes have “more than 500 books.” In this category, the mean score decreases, contrary to the overall positive relationship observed. This suggests that there may be diminishing returns in terms of academic performance when the number of books surpasses a certain threshold.\n\nIn summary, the density plot illustrates a positive association between the number of books in the home and student performance. However, the deviation observed for the “more than 500 books” category suggests a nuanced relationship, highlighting the need to consider optimal conditions for leveraging the positive influence of books on academic outcomes.”\n\n\n\n\nClarity\n\n😊😕 (Almost) Right Visualisation. The use of density plots, with vertical lines showing the median reading scores, is useful for comparing the distributions of reading scores based on the number of books at home. However, this can be further enhanced with the use of a ridgelines plot.\n😕 Missing Key Message in Title. The key message stated in the observation is not included in the title (which is factual). Stating the key message in the title would help the reader zoom in on the intended message of the plot.\n😕 Difficult to Compare Distributions. Given that there are many categories, it is difficult to compare the distributions. It may be useful to showcase the quartile cut-offs to allow for better comparisons across different density plots.\n😕 Missing Source Caption. It would be useful to credit the source of the data directly in the plot to give the reader the appropriate context for interpreting plot.\n\nAesthetics\n\n😕 Unclear Colours Used. The plot uses an easy-to-read font. However, the choice of gradient colours is not appropriate as the answers form a categorical, not continuous, variable. The use of gradient colours does not allow the reader to differentiate the distinct categories of answers to the question in the questionnaire on the number of books at home.\n😕 Lack of Labels for Categories. The categories representing different ranges of the number of books at home requires a separate table for interpretation. This inconveniences the reader. Hence, it would be better to label each density plot directly for easier identification.\n😕 No Labels for Mean Values. The values of the dashed vertical lines are not labelled. It may be useful to label some of them to allow for direct comparison. Alternatively, to prevent clutter, it may be useful to add just two vertical lines - overall mean and overall median for broad comparisons.\n😕 Orientation of Label. The y-axis title can be rotated for easier reading.\n\n\n\n\nThe sketch of the alternative design is as follows:\n\n\n\n\n\n\n\n\nAn improved plot is made as follows:\n\nImproved PlotCode\n\n\n\n\n\n\n\n\n\n\nstu_df$Possession_book = as.character(stu_df$Possession_book)\nstu_df = stu_df %&gt;% \n  mutate(Possession_book = recode(Possession_book,\n                        \"1\" = \"0 Book\",\n                        \"2\" = \"1-10 Books\",\n                        \"3\" = \"11-25 Books\",\n                        \"4\" = \"26-100 Books\",\n                        \"5\" = \"101-200 Books\",\n                        \"6\" = \"201-500 Books\",\n                        \"7\" = \"&gt;500 Books\"))\n\nr = stu_df %&gt;% \n  select(Possession_book, Reading_Average) %&gt;%\n  na.omit() %&gt;%\n  mutate(Books = fct_relevel(Possession_book, \n                              \"0 Book\", \n                              \"1-10 Books\",\n                              \"11-25 Books\",\n                              \"26-100 Books\",\n                              \"101-200 Books\",\n                              \"201-500 Books\",\n                              \"&gt;500 Books\"))\n                        \nggplot(r, \n       aes(x = Reading_Average, \n           y = Books,\n           fill = factor(after_stat(quantile)))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 2,\n    quantile_lines = TRUE) +\n  scale_fill_brewer(type = \"seq\", palette = \"Set3\") +\n  ggtitle(label = \"More Books at Home Generally Correspond to\\nHigher Reading Scores\",\n          subtitle = \"Distribution of Reading Scores vis-a-vis Number of Books at Home\") +\n  xlab(\"Score\") + ylab(\"No.of\\nBooks\") +\n  labs(caption = \"Source: PISA 2022\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1), \n        legend.position = \"none\") +\n  geom_vline(aes(xintercept=median(Reading_Average)),\n             color=\"red\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) + \n  annotate(\"text\", x=690, y=0.75, label=paste(\"Overall Median:\", round(median(r$Reading_Average))), color = \"red\") +\n  geom_vline(aes(xintercept=mean(Reading_Average)),\n             color=\"blue\", \n             linetype=\"dotdash\", \n             linewidth=1,\n             alpha = 0.5) + \n  annotate(\"text\", x=420, y=0.75, label=paste(\"Overall Mean:\", round(mean(r$Reading_Average))), color = \"blue\")\n\n\n\n\n\nNote: The same makeover approach is also applicable to Section 4.3.1 (Performance Distribution by Highest Level of Schooling of Parents) as it also uses density plots."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#conclusion",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "In conclusion, the ggplot2 package is a powerful package for exploratory data analysis through visualisation. However, it is insufficient if effort is not made to select, clean, categorise, and moderate the data properly, and the visualisation process does not take into account information design, visual encoding, and interface design.\nThe step-by-step process of a data visualisation makeover in this take-home exercise highlights the importance of paying attention to details when making a plot. A personal checklist of important points to consider that are elucidated from the makeover attempts above are summarised as follows:\n\nDoes the plot have a clear key message?\nIs the correct type of chart used?\nIs the plot telling a lie? Are the scales of the axes distorted?\nAre the components of the graph (titles, axes, labels, tick marks, legend, grid lines, caption) accurate, relevant, and easy-to-view, or are they erroneous, confusing or redundant?\nIs annotation required?\nAre colours and fonts used appropriately in the plot?\nTake note of data-ink by reducing non-data-ink, and enhancing data-ink. The use of an appropriate theme helps!\nAim for a Quadrant I plot that is clear and beautiful!"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#key-references",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#key-references",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "R for Visual Analytics.\nR for Data Science.\nFundamentals of Data Visualisation.\nPISA 2022 Database and Technical Report.\n\n~~~ End of Take-home Exercise 2 ~~~"
  }
]