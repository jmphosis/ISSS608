[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608: Visual Analytics and Applications",
    "section": "",
    "text": "Welcome to ISSS608 Visual Analytics and Applications!\nIn this webpage, I am going to share my visual analytics learning journey.\n\n\n\n\n\nCredit: Prima Consulting UK"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "Singapore has come a long way since gaining independence in 1965. Without natural resources, human capital development through a well-planned education system is a critical part of Singapore’s transformation from third world to first.\nDespite the success, there is still a correlation between socio-economic status and education achievement, as well as ingrained perceptions that some schools are better than others.\nHence, there is a need to use data to analyse the performance of Singapore students across different subjects, and identify any relationships between the performance in various subjects and factors such as gender, socioeconomic status, and type of school.\n\n\n\nIn this take-home exercise, the objective is to use the appropriate Exploratory Data Analysis (EDA) methods and ggplot2 functions (part of the amazing tidyverse ecosystem) to answer the following analytical questions:\n\nWhat is the distribution of Singapore students’ performance in Mathematics, Reading, and Science? What are the similarities and/or differences between the distributions for the three different subjects?\nAre there relationships between the students’ performance in the three subjects and factors such as their gender, socioeconomic status, and school? If yes, what kind of relationships are present?\n\nBased on the analysis and observations, this take-home exercise also briefly suggests the potential insights can be further studied in future research to better inform education policy planning.\n\n\n\n\n\n\nThe R packages used in this take-home exercise are:\n\nhaven for importing SAS files;\ntidyverse (i.e. readr, tidyr, dplyr, ggplot2) for performing data science tasks such as importing, tidying, and wrangling data, as well as creating graphics based on The Grammar of Graphics;\nreshape2 for transforming data between wide and long formats;\nggthemr for aesthetic themes created by user, Ciarán Tobin;\nggridges for creating ridgeline plots;\nggdist for visualising distributions and uncertainty; and\nggpubr for creating publication ready ggplot2 plots.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed in the computer. If yes, they are then loaded into the R environment. If no, they are installed, and then loaded into the R environment.\n\npacman::p_load(haven, tidyverse, reshape2,\n               ggthemr, ggridges, ggdist,\n               ggpubr)\n\nThe ggthemr() function in the ggthemr package is used to set the default theme of this take-home exercise as “solarized”.\n\nggthemr(\"solarized\")\n\n\n\n\nThe OECD Programme for International Student Assessment (PISA) measures how well 15-year-old students in different countries are “prepared to meet the challenges of today’s knowledge societies” by looking at “their ability to use their knowledge and skills to meet real-life challenges”. The PISA surveys take place every three years, the latest being conducted in 2022.\nThe PISA 2022 database contains the full set of responses from individual students, school principals, and parents. There are a total of five data files and their contents are as follows:\n\nStudent questionnaire data file;\nSchool questionnaire data file;\nTeacher questionnaire data file;\nCognitive item data file; and\nQuestionnaire timing data file.\n\nFor the purpose of this take-home exercise, the “Student questionnaire data file” is used.\n\n\n\n\n\n\nThe dataset used in this take-home exercise is the 2022 PISA student questionnaire data file, cy08msp_stu_qqq.sas7bdat, which is in the SAS file format.\nThe file is imported into the R environment using the read_sas() function in the haven package and stored as the R object, stu_qqq.\n\nstu_qqq = read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\nThe tibble data frame, stu_qqq, has 1,279 columns (variables) and 613,744 rows (observations).\n\n\n\nThere are 6,606 rows with the country code (i.e., CNT) value of “SGP”, which represents Singapore. This count is cross-verified by the information provided in the “CY08MSP_STU_QQQ” sheet in the codebook. The codebook also stated that Singapore students’ made up 1.0763% of the entire global student population who took part in the 2022 PISA.\nThe filter() function in the dplyr package is used to obtain these rows, and stored as the R object, stu_qqq_SG.\n\nstu_qqq_SG = stu_qqq %&gt;% filter(CNT == \"SGP\")\n\nThe tibble data frame, stu_qqq_SG, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(stu_qqq_SG, \"data/stu_qqq_SG.rds\")\n\n\nstu_qqq_SG = read_rds(\"data/stu_qqq_SG.rds\")\n\n\n\n\nOf the 1,279 variables (columns), the following 36 are preliminarily selected to answer the analytical questions:\n\nInternational School ID (“CNTSCHID”);\nInternational Student ID (“CNTSTUID”);\nType of School (“STRATUM”);\n\n“SGP01” is Public Secondary School.\n“SGP03” is Private Secondary School.\n\nStudent Gender (“ST004D01T”);\n\n“01” is Female.\n“02” is Male.\n\nIndex of Economic, Social, and Cultural Status (“ESCS”);\nNumber of Books at Home (“ST255Q01JA”);\n\n“01” is no books”.\n“02” is 1-10 books.\n“03” is 11-25 books.\n“04” is 26-100 books.\n“05” is 101-200 books.\n“06” is 201-500 books.\n“07” is more than 500 books.\n\nPlausible Values 1 to 10 in Mathematics (“PV1MATH” to “PV10MATH”);\nPlausible Values 1 to 10 in Reading (“PV1READ” to “PV10READ”); and\nPlausible Values 1 to 10 in Science (“PV1SCIE” to “PV10SCIE”).\n\nAgain, the select() function in the dplyr package is used to obtain these rows, and stored as the R object, stu_qqq_SG_final.\n\nstu_qqq_SG_final = stu_qqq_SG %&gt;% \n  select(\"CNTSCHID\", \"CNTSTUID\", \"STRATUM\", \"ST004D01T\", \"ESCS\", \"ST255Q01JA\",\n           \"PV1MATH\", \"PV2MATH\", \"PV3MATH\", \"PV4MATH\", \"PV5MATH\", \"PV6MATH\", \n           \"PV7MATH\", \"PV8MATH\", \"PV9MATH\", \"PV10MATH\", \"PV1READ\", \"PV2READ\", \n           \"PV3READ\", \"PV4READ\", \"PV5READ\", \"PV6READ\", \"PV7READ\", \"PV8READ\", \n           \"PV9READ\", \"PV10READ\", \"PV1SCIE\", \"PV2SCIE\", \"PV3SCIE\", \"PV4SCIE\", \n           \"PV5SCIE\", \"PV6SCIE\", \"PV7SCIE\", \"PV8SCIE\", \"PV9SCIE\", \"PV10SCIE\")\n\n\n\n\nThe dataset from PISA is expected to be relatively clean. Nevertheless, due diligence checks for duplicates and missing values are still made to confirm the assumption.\nThe duplicated() function in the base package is used to check for duplicates in stu_qqq_SG_final. There are no duplicates in the tibble data frame.\n\nstu_qqq_SG_final[duplicated(stu_qqq_SG_final), ]\n\n# A tibble: 0 × 36\n# ℹ 36 variables: CNTSCHID &lt;dbl&gt;, CNTSTUID &lt;dbl&gt;, STRATUM &lt;chr&gt;,\n#   ST004D01T &lt;dbl&gt;, ESCS &lt;dbl&gt;, ST255Q01JA &lt;dbl&gt;, PV1MATH &lt;dbl&gt;,\n#   PV2MATH &lt;dbl&gt;, PV3MATH &lt;dbl&gt;, PV4MATH &lt;dbl&gt;, PV5MATH &lt;dbl&gt;, PV6MATH &lt;dbl&gt;,\n#   PV7MATH &lt;dbl&gt;, PV8MATH &lt;dbl&gt;, PV9MATH &lt;dbl&gt;, PV10MATH &lt;dbl&gt;, PV1READ &lt;dbl&gt;,\n#   PV2READ &lt;dbl&gt;, PV3READ &lt;dbl&gt;, PV4READ &lt;dbl&gt;, PV5READ &lt;dbl&gt;, PV6READ &lt;dbl&gt;,\n#   PV7READ &lt;dbl&gt;, PV8READ &lt;dbl&gt;, PV9READ &lt;dbl&gt;, PV10READ &lt;dbl&gt;, PV1SCIE &lt;dbl&gt;,\n#   PV2SCIE &lt;dbl&gt;, PV3SCIE &lt;dbl&gt;, PV4SCIE &lt;dbl&gt;, PV5SCIE &lt;dbl&gt;, …\n\n\nThe colSums() function in the base package is used to check for missing values in stu_qqq_SG_final. There are no missing values in the tibble data frame for all most columns except for two:\n\nThe column with the Index of Economic, Social, and Cultural Status (“ESCS”) has 47 rows (observations) with NA values.\nThe column indicating the number of books at home (“ST255Q01JA” has 44 rows (observations) with NA values.\n\nIn total, there are 50 rows with one or more NA values. As this makes up only 0.757% of the 6,606 observations, we will remove them from the subsequent analysis. The na.omit() function in the stats package is used to remove them from stu_qqq_SG_final, which now has 6,556 observations and 36 variables. A confirmatory check is then made with the colSums() function in the base package.\n\ncolSums(is.na(stu_qqq_SG_final))\n\n  CNTSCHID   CNTSTUID    STRATUM  ST004D01T       ESCS ST255Q01JA    PV1MATH \n         0          0          0          0         47         44          0 \n   PV2MATH    PV3MATH    PV4MATH    PV5MATH    PV6MATH    PV7MATH    PV8MATH \n         0          0          0          0          0          0          0 \n   PV9MATH   PV10MATH    PV1READ    PV2READ    PV3READ    PV4READ    PV5READ \n         0          0          0          0          0          0          0 \n   PV6READ    PV7READ    PV8READ    PV9READ   PV10READ    PV1SCIE    PV2SCIE \n         0          0          0          0          0          0          0 \n   PV3SCIE    PV4SCIE    PV5SCIE    PV6SCIE    PV7SCIE    PV8SCIE    PV9SCIE \n         0          0          0          0          0          0          0 \n  PV10SCIE \n         0 \n\n\n\nstu_qqq_SG_final = stu_qqq_SG_final %&gt;%\n  na.omit()\n\n\ncolSums(is.na(stu_qqq_SG_final))\n\n  CNTSCHID   CNTSTUID    STRATUM  ST004D01T       ESCS ST255Q01JA    PV1MATH \n         0          0          0          0          0          0          0 \n   PV2MATH    PV3MATH    PV4MATH    PV5MATH    PV6MATH    PV7MATH    PV8MATH \n         0          0          0          0          0          0          0 \n   PV9MATH   PV10MATH    PV1READ    PV2READ    PV3READ    PV4READ    PV5READ \n         0          0          0          0          0          0          0 \n   PV6READ    PV7READ    PV8READ    PV9READ   PV10READ    PV1SCIE    PV2SCIE \n         0          0          0          0          0          0          0 \n   PV3SCIE    PV4SCIE    PV5SCIE    PV6SCIE    PV7SCIE    PV8SCIE    PV9SCIE \n         0          0          0          0          0          0          0 \n  PV10SCIE \n         0 \n\n\n\n\n\nFor ease of use, the columns are renamed accordingly using the rename() function in the dplyr package.\n\nstu_qqq_SG_final = stu_qqq_SG_final %&gt;%\n  rename(School = \"CNTSCHID\",\n         SchoolType = \"STRATUM\",\n         ID = \"CNTSTUID\",\n         Gender = \"ST004D01T\",\n         SocioeconStatus = \"ESCS\",\n         Books = \"ST255Q01JA\",\n         Math01 = \"PV1MATH\", Math02 = \"PV2MATH\", \n         Math03 = \"PV3MATH\", Math04 = \"PV4MATH\", \n         Math05 = \"PV5MATH\", Math06 = \"PV6MATH\", \n         Math07 = \"PV7MATH\", Math08 = \"PV8MATH\", \n         Math09 = \"PV9MATH\", Math10 = \"PV10MATH\", \n         Read01 = \"PV1READ\", Read02 = \"PV2READ\", \n         Read03 = \"PV3READ\", Read04 = \"PV4READ\", \n         Read05 = \"PV5READ\", Read06 = \"PV6READ\", \n         Read07 = \"PV7READ\", Read08 = \"PV8READ\", \n         Read09 = \"PV9READ\", Read10 = \"PV10READ\", \n         Sci01 = \"PV1SCIE\", Sci02 = \"PV2SCIE\", \n         Sci03 = \"PV3SCIE\", Sci04 = \"PV4SCIE\",\n         Sci05 = \"PV5SCIE\", Sci06 = \"PV6SCIE\", \n         Sci07 = \"PV7SCIE\", Sci08 = \"PV8SCIE\", \n         Sci09 = \"PV9SCIE\", Sci10 = \"PV10SCIE\")\n\nAlso, for the ease of use, the values for Gender, School Type, and Number of Books are replaced with characters using the ifelse() function in the base package.\n\nstu_qqq_SG_final$Gender = ifelse(\n  stu_qqq_SG_final$Gender == 01, \n  \"Female\", \"Male\")\nstu_qqq_SG_final$SchoolType = ifelse(\n  stu_qqq_SG_final$SchoolType == \"SGP01\", \n  \"Public\", \"Private\")\n\nstu_qqq_SG_final = stu_qqq_SG_final %&gt;%\n  mutate(Books = recode(Books, \n                        \"01\" = \"0 Book\",\n                        \"02\" = \"1-10 Books\",\n                        \"03\" = \"11-25 Books\",\n                        \"04\" = \"26-100 Books\",\n                        \"05\" = \"101-200 Books\",\n                        \"06\" = \"201-500 Books\",\n                        \"07\" = \"&gt;500 Books\"))\n\n\n\n\nThere are 10 Plausible Values (PVs) each for Mathematics, Reading, and Science. However, PISA cautions against averaging the PVs at the student level. Instead, it suggests that population statistics should be estimated using each PV separately - e.g., if one is interested in the correlation coefficient between the social index and the reading performance in PISA, 10 correlation coefficients should be computed and then averaged.\nA combination of half-density plot and box plot are plotted for the each of the 10 PVs for each subject. Firstly, the select() function in the dplyr package and the melt() function in the reshape2 package are used to select the relevant columns and transform them into a long table. The variables are then renamed using the rename() function in the dplyr package.\nThe ggplot(), geom_boxplot(), coord_flip() functions in the ggplot2 package, and the stat_halfeye() function in the ggdist package are used to create the plots. The ggtitle() and theme() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title, centre the plot title, and rotate the y-axis label.\n\nMathReadingScience\n\n\n\n\nCode\nmath = stu_qqq_SG_final %&gt;% \n  select(Math01, Math02, Math03, Math04, Math05,\n         Math06, Math07, Math08, Math09, Math10) %&gt;%\n  melt() %&gt;%\n  rename(\"PV\" = \"variable\", \"Score\" = \"value\")\n\nggplot(math, \n       aes(x = PV, \n           y = Score)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .15,\n               outlier.shape = NA) +\n  coord_flip() +\n  ggtitle(\"Distribution of Math Scores for Different PVs\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\n\n\n\n\n\nCode\nread = stu_qqq_SG_final %&gt;% \n  select(Read01, Read02, Read03, Read04, Read05,\n         Read06, Read07, Read08, Read09, Read10) %&gt;%\n  melt() %&gt;%\n  rename(\"PV\" = \"variable\", \"Score\" = \"value\")\n\nggplot(read, \n       aes(x = PV, \n           y = Score)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .15,\n               outlier.shape = NA) +\n  coord_flip() +\n  ggtitle(\"Distribution of Reading Scores for Different PVs\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\n\n\n\n\n\nCode\nsci = stu_qqq_SG_final %&gt;% \n  select(Sci01, Sci02, Sci03, Sci04, Sci05,\n         Sci06, Sci07, Sci08, Sci09, Sci10) %&gt;%\n  melt() %&gt;%\n  rename(\"PV\" = \"variable\", \"Score\" = \"value\")\n\nggplot(sci, \n       aes(x = PV, \n           y = Score)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .15,\n               outlier.shape = NA) +\n  coord_flip() +\n  ggtitle(\"Distribution of Science Scores for Different PVs\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\n\n\n\n\nBased on the plots, the 10 sets of PVs for each subject are broadly similar to one another (similar distributions and similar median values). Hence, for the purposes of this take-home exercise, the PV 1 values for each subject are used.\nThe select() and rename() functions in the dplyr package are used to further narrow down the number of variables chosen to 9 out of the preliminary 36 variables chosen previously, and rename some of the columns for easier identification.\n\nstu_qqq_SG_final = stu_qqq_SG_final %&gt;% \n  select(\"ID\", \"School\", \"SchoolType\", \n            \"Gender\", \"SocioeconStatus\", \n            \"Books\", \n            \"Math01\", \"Read01\", \"Sci01\") %&gt;%\n  rename(\"Mathematics\" = \"Math01\",\n         \"Reading\" = \"Read01\",\n         \"Science\" = \"Sci01\")\n\nThe finalised tibble data frame, stu_qqq_SG_final, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(stu_qqq_SG_final, \"data/stu_qqq_SG_final.rds\")\n\n\nstu_qqq_SG_final = read_rds(\"data/stu_qqq_SG_final.rds\")\n\n\n\n\n\n\n\nA ridgeline plot is created to visualise the distributions of the Singapore students’ performance for all three subjects within one plot.\nFirstly, the select() function in the dplyr package and the melt() function in the reshape2 package are used to select the relevant columns and transform them into a long table. The variables are then renamed using the rename() function in the dplyr package. The mutate() function in the dplyr package is then used with the “fct_relevel” argument to manually arrange the subjects’ order in the plot.\nThen, the ggplot() and scale_fill_viridis_d() functions in the ggplot2 package, and the stat_density_ridges() function in the ggridges package are used to create the plot. The ggtitle() and theme() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title and subtitle, centre the plot title, and rotate the y-axis label.\n\nPlot 1Code\n\n\n\n\n\n\n\n\n\n\ncomb = stu_qqq_SG_final %&gt;% \n  select(Mathematics, Reading, Science) %&gt;%\n  melt() %&gt;%\n  rename(\"Subject\" = \"variable\", \"Score\" = \"value\") %&gt;%\n  mutate(Subject = fct_relevel(Subject, \n                               \"Mathematics\", \n                               \"Science\", \n                               \"Reading\"))\n\nggplot(comb, \n       aes(x = Score, \n           y = Subject,\n           fill = factor(after_stat(quantile)))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\", alpha = 0.8) +\n  ggtitle(label = \"SG Students Generally Perform Better in\\nMathematics than Science & Reading\",\n          subtitle = \"Distribution of Mathematics, Science and Reading Scores\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\nObservation: Based on the plot, we can see that the scores for all three subjects resemble normal distribution. Also, based on the quartile lines, the scores for Mathematics tend to be higher than that for Science and Reading; while the scores of Science tend to be higher than that for Reading. This corresponds to general perceptions that Singapore students tend to be better at technical subjects (such as Mathematics and Science) as compared to subjects related to language skills (such as Reading).\nFuture research in this area may consider the historical trends in the differences between Singapore students’ performance in the three subjects and whether there are societal and economic forces behind such trends.\n\n\n\n\n\nThere is often a stereotype that men are better than women in the areas of science and technology (S&T). In this take-home exercise, we attempt to answer the question: “Are girls weaker than boys at Science in Singapore?” by creating a density plot of the Singapore students’ Science scores by gender.\nFirstly, the filter() function in the dplyr package is used to select the subsets of stu_qqq_SG_final containing the observations for each gender. The mutate() function in the dplyr package is then used with the “fct_relevel” argument to manually arrange the two gender’s order in the plot so that the Female density plot uses the stereotypical/associated colour of pink, while the Male density plot uses the stereotypical/associated colour of blue.\nThen, the ggplot(), geom_density() and geom_vline() functions in the ggplot2 package are used to create the plot and insert vertical lines indicating the median scores for each gender (again, pink for Female, and blue for Male). The ggtitle(), theme(), xlab(), and ylab() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title and subtitle, centre the plot title, rotate the y-axis label, and overwrite the axes’ labels.\n\nPlot 2Code\n\n\n\n\n\n\n\n\n\n\nf = stu_qqq_SG_final %&gt;%\n  filter(Gender == \"Female\")\n\nm = stu_qqq_SG_final %&gt;%\n  filter(Gender == \"Male\")\n\nstu_qqq_SG_final = stu_qqq_SG_final %&gt;%\n  mutate(Gender = fct_relevel(Gender, \n                               \"Male\", \n                               \"Female\"))\n\nggplot(stu_qqq_SG_final,\n       aes(x = Science,\n           fill = Gender)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(aes(xintercept=median(f$Science)),\n             color=\"red\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) +\n  geom_vline(aes(xintercept=median(m$Science)),\n             color=\"blue\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) +\n  ggtitle(label = \"Girls Are Not Necessarily Weaker in Science\",\n          subtitle= \"Distribution of Science Scores by Gender\") + \n  ylab(\"Density\") + xlab(\"Score\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\nObservation: Based on the plot, the distribution of Science scores for both Male and Female are broadly similar. With slightly more boys than girls on the two ends of the distribution (i.e., either poor or excellent scores), and conversely, more girls than boys in the centre of the distribution (i.e., close to median scores). The median scores for both gender are also very close to each other, with Male slightly higher than Female. This debunks the notion that girls are weaker in Science compared to boys.\nFuture research may consider why a relatively similar distribution in the Science performance of Singapore students at 15-year-old does not translate to a higher proportion of females undertaking S&T endeavours in college and beyond.\n\n\n\nParents in Singapore are often concerned about the type of schools their children enrol in as there is a strong, ingrained belief that some schools (i.e., Private) are better than others (i.e., Public). In this take-home exercise, we attempt to answer the question: “Do students in public schools perform poorer in Mathematics?” by creating a half-eye plot combined with a box plot for the Mathematics scores by type of school.\nFirstly, the filter() function in the dplyr package is used to select the subsets of stu_qqq_SG_final containing the observations for each gender. The mutate() function in the dplyr package is then used with the “fct_relevel” argument to manually arrange the two gender’s order in the plot so that the Female density plot uses the stereotypical/associated colour of pink, while the Male density plot uses the stereotypical/associated colour of blue.\nThe ggplot(), geom_boxplot() and coord_flip() functions in the ggplot2 package are used to create the plot, insert the box plots for each type of school, and change the orientation of the plot. The stat_halfeye() and stat_dots() functions from the ggdist package are used to show the distributions for each type of school. The ggtitle(), theme(), xlab(), and ylab() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title and subtitle, centre the plot title, rotate the y-axis label, and overwrite the axes’ labels.\n\nPlot 3Code\n\n\n\n\n\n\n\n\n\n\nggplot(stu_qqq_SG_final, \n       aes(x = SchoolType, \n           y = Mathematics)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.15,\n               .width = 0,\n               point_colour = NA,\n               scale= 0.55) +\n  geom_boxplot(width = .15,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = 3,\n            dotsize = 0.1) +\n  coord_flip() +\n  ggtitle(label = \"Public Schools Have Both Best and Worst Performing\\n Students in Mathematics\",\n          subtitle= \"Distribution of Mathematics Scores by School Type\") +\n  xlab(\"School\\nType\") + ylab(\"Score\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\nObservation: Based on the plot, the distribution of Mathematics scores for Singapore students in public and private schools differ in the following ways:\n\nThe range of scores in public schools is larger than that in private schools, i.e., public schools have a wider range of student capabilities for Mathematics;\nRelated to above, there is a greater density of students in private schools that perform moderately well (i.e., around the median score for private schools) compared to students from public schools.\nThe median Mathematics score for students in private schools is slightly higher than that for public schools.\n\nIt is important to note from the dot plots that there are a much larger number of students in public schools than private schools, which may mean that the results for private schools may not be representative.\nMore research would need to be conducted to uncover how and why the relationship between the type of school and Mathematics performance are as shown by the various observations. Also, there may be granular differences in Mathematics performance between different public schools.\n\n\n\nResearch has continued to show that socioeconomic status influence academic performance. In this take-home exercise, we attempt to answer the question: “Are Reading scores related to socioeconomic status?” by creating a scatter plot to show the distribution of Reading scores vis-a-vis the Index of Economic, Social, and Cultural Status values.\nThe ggplot(), geom_point() and geom_smooth() functions in the ggplot2 package are used to create the scatter plot, and add a smooth line showing the correlation between the two variables. The stat_cor() function in the ggpubr package is used to generate the correlation coefficient, R, which is 0.41. The ggtitle(), theme(), xlab(), and ylab() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title and subtitle, centre the plot title, rotate the y-axis label, and overwrite the axes’ labels.\n\nPlot 4Code\n\n\n\n\n\n\n\n\n\n\nggplot(data = stu_qqq_SG_final, \n            aes(x = Reading, \n                y = SocioeconStatus)) +   \n  geom_point(alpha = 0.1) +\n  geom_smooth(linewidth = 1, colour = \"pink\") + \n  stat_cor() +\n  ggtitle(label = \"Moderate Positive Correlation between\\nReading Scores and Socioeconomic Status\",\n          subtitle= \"Distribution of Reading Scores vis-a-vis Socioeconomic Status\") +\n  xlab(\"Score\") + ylab(\"Socioeconomic\\nStatus\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\nObservation: Based on the plot, there is no clear linear relationship between Reading scores and socioeconomic status given that the dots are generally well distributed, i.e., there are students with low socioeconomic status (below 0 value) with scores ranging from 200 to 800, as there are students with high socio economic status (above 0 value) with scores ranging from 200 to 800. Nevertheless, there is a higher concentration of students in the 400-600 score range with high socioeconomic status compared to students with low socioeconomic status in the same score range.\nFuture research may consider whether there are better variables that may explain differences in Reading performance.\nFor a start, we consider if a simpler variable such as the number of books may help to explain differences in Reading performance better than the Index of Economic, Social, and Cultural Status values. A ridgeline created to visualise the distributions of the Singapore students’ Reading performance depending on the self-reported number of books at home.\nFirstly, the select() function in the dplyr package and the melt() function in the reshape2 package are used to select the relevant columns and transform them into a long table. The mutate() function in the dplyr package is then used with the “fct_relevel” argument to manually arrange the number of books in ascending order in the plot.\nThen, the ggplot() and scale_fill_viridis_d() functions in the ggplot2 package, and the stat_density_ridges() function in the ggridges package are used to create the plot. The ggtitle() and theme() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title and subtitle, centre the plot title, rotate the y-axis label, and remove the legend.\n\nPlot 5Code\n\n\n\n\n\n\n\n\n\n\nr = stu_qqq_SG_final %&gt;% \n  select(Books, Reading) %&gt;%\n  mutate(Books = fct_relevel(Books, \n                              \"0 Book\", \n                              \"1-10 Books\",\n                              \"11-25 Books\",\n                              \"26-100 Books\",\n                              \"101-200 Books\",\n                              \"201-500 Books\",\n                              \"&gt;500 Books\"))\n                        \nggplot(r, \n       aes(x = Reading, \n           y = Books,\n           fill = factor(after_stat(quantile)))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 2,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(alpha = 0.8) +\n  ggtitle(label = \"More Books at Home Generally Correspond\\nto Higher Reading Scores\",\n          subtitle = \"Distribution of Reading Scores vis-a-vis Number of Books at Home\") +\n  xlab(\"Score\") + ylab(\"No.of\\nBooks\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1), \n        legend.position = \"none\")\n\n\n\n\n\n\n\nObservation: Based on the plot, there is a relationship between the number of books at home and the Reading scores. Focusing on the median values for each ridge (as shown by the divider line between the two colours), the values increase as the number of books at home increase. The only exception is the median value of Reading scores for students with more than 500 books at home, which is lower than that for 201-500 books.\nHence, we can see a clearer trend between Reading scores and number of books as compared to that between Reading scores and the Index of Economic, Social, and Cultural Status values. Further analysis can be conducted to determine the correlation coefficient of the relationship and whether the relationship is statistically significant.\n\n\n\n\n\nIn conclusion, the ggplot2 package is a powerful package for exploratory data analysis through visualisation. The PISA performance dataset is interesting and contains useful data for studying Singapore students’ academic performance in the three subjects at age 15. The insights gained from analysing the various questions posed in this take-home exercise provides a preview of the data analyses that can be conducted in further studies to better understand how various factors including gender, type of school, and socioeconomic status affect academic performance, so as to better inform education policy planning in Singapore.\n\n\n\n\nR for Visual Analytics.\nR for Data Science.\nFundamentals of Data Visualisation.\nPISA 2022 Database and Technical Report.\nOECD on Singapore: Rapid Improvement Followed by Strong Performance.\n\n~~~ End of Take-home Exercise 1 ~~~"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#introduction",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#introduction",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "Singapore has come a long way since gaining independence in 1965. Without natural resources, human capital development through a well-planned education system is a critical part of Singapore’s transformation from third world to first.\nDespite the success, there is still a correlation between socio-economic status and education achievement, as well as ingrained perceptions that some schools are better than others.\nHence, there is a need to use data to analyse the performance of Singapore students across different subjects, and identify any relationships between the performance in various subjects and factors such as gender, socioeconomic status, and type of school.\n\n\n\nIn this take-home exercise, the objective is to use the appropriate Exploratory Data Analysis (EDA) methods and ggplot2 functions (part of the amazing tidyverse ecosystem) to answer the following analytical questions:\n\nWhat is the distribution of Singapore students’ performance in Mathematics, Reading, and Science? What are the similarities and/or differences between the distributions for the three different subjects?\nAre there relationships between the students’ performance in the three subjects and factors such as their gender, socioeconomic status, and school? If yes, what kind of relationships are present?\n\nBased on the analysis and observations, this take-home exercise also briefly suggests the potential insights can be further studied in future research to better inform education policy planning."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#getting-started",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "The R packages used in this take-home exercise are:\n\nhaven for importing SAS files;\ntidyverse (i.e. readr, tidyr, dplyr, ggplot2) for performing data science tasks such as importing, tidying, and wrangling data, as well as creating graphics based on The Grammar of Graphics;\nreshape2 for transforming data between wide and long formats;\nggthemr for aesthetic themes created by user, Ciarán Tobin;\nggridges for creating ridgeline plots;\nggdist for visualising distributions and uncertainty; and\nggpubr for creating publication ready ggplot2 plots.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed in the computer. If yes, they are then loaded into the R environment. If no, they are installed, and then loaded into the R environment.\n\npacman::p_load(haven, tidyverse, reshape2,\n               ggthemr, ggridges, ggdist,\n               ggpubr)\n\nThe ggthemr() function in the ggthemr package is used to set the default theme of this take-home exercise as “solarized”.\n\nggthemr(\"solarized\")\n\n\n\n\nThe OECD Programme for International Student Assessment (PISA) measures how well 15-year-old students in different countries are “prepared to meet the challenges of today’s knowledge societies” by looking at “their ability to use their knowledge and skills to meet real-life challenges”. The PISA surveys take place every three years, the latest being conducted in 2022.\nThe PISA 2022 database contains the full set of responses from individual students, school principals, and parents. There are a total of five data files and their contents are as follows:\n\nStudent questionnaire data file;\nSchool questionnaire data file;\nTeacher questionnaire data file;\nCognitive item data file; and\nQuestionnaire timing data file.\n\nFor the purpose of this take-home exercise, the “Student questionnaire data file” is used."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-wrangling",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "The dataset used in this take-home exercise is the 2022 PISA student questionnaire data file, cy08msp_stu_qqq.sas7bdat, which is in the SAS file format.\nThe file is imported into the R environment using the read_sas() function in the haven package and stored as the R object, stu_qqq.\n\nstu_qqq = read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\nThe tibble data frame, stu_qqq, has 1,279 columns (variables) and 613,744 rows (observations).\n\n\n\nThere are 6,606 rows with the country code (i.e., CNT) value of “SGP”, which represents Singapore. This count is cross-verified by the information provided in the “CY08MSP_STU_QQQ” sheet in the codebook. The codebook also stated that Singapore students’ made up 1.0763% of the entire global student population who took part in the 2022 PISA.\nThe filter() function in the dplyr package is used to obtain these rows, and stored as the R object, stu_qqq_SG.\n\nstu_qqq_SG = stu_qqq %&gt;% filter(CNT == \"SGP\")\n\nThe tibble data frame, stu_qqq_SG, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(stu_qqq_SG, \"data/stu_qqq_SG.rds\")\n\n\nstu_qqq_SG = read_rds(\"data/stu_qqq_SG.rds\")\n\n\n\n\nOf the 1,279 variables (columns), the following 36 are preliminarily selected to answer the analytical questions:\n\nInternational School ID (“CNTSCHID”);\nInternational Student ID (“CNTSTUID”);\nType of School (“STRATUM”);\n\n“SGP01” is Public Secondary School.\n“SGP03” is Private Secondary School.\n\nStudent Gender (“ST004D01T”);\n\n“01” is Female.\n“02” is Male.\n\nIndex of Economic, Social, and Cultural Status (“ESCS”);\nNumber of Books at Home (“ST255Q01JA”);\n\n“01” is no books”.\n“02” is 1-10 books.\n“03” is 11-25 books.\n“04” is 26-100 books.\n“05” is 101-200 books.\n“06” is 201-500 books.\n“07” is more than 500 books.\n\nPlausible Values 1 to 10 in Mathematics (“PV1MATH” to “PV10MATH”);\nPlausible Values 1 to 10 in Reading (“PV1READ” to “PV10READ”); and\nPlausible Values 1 to 10 in Science (“PV1SCIE” to “PV10SCIE”).\n\nAgain, the select() function in the dplyr package is used to obtain these rows, and stored as the R object, stu_qqq_SG_final.\n\nstu_qqq_SG_final = stu_qqq_SG %&gt;% \n  select(\"CNTSCHID\", \"CNTSTUID\", \"STRATUM\", \"ST004D01T\", \"ESCS\", \"ST255Q01JA\",\n           \"PV1MATH\", \"PV2MATH\", \"PV3MATH\", \"PV4MATH\", \"PV5MATH\", \"PV6MATH\", \n           \"PV7MATH\", \"PV8MATH\", \"PV9MATH\", \"PV10MATH\", \"PV1READ\", \"PV2READ\", \n           \"PV3READ\", \"PV4READ\", \"PV5READ\", \"PV6READ\", \"PV7READ\", \"PV8READ\", \n           \"PV9READ\", \"PV10READ\", \"PV1SCIE\", \"PV2SCIE\", \"PV3SCIE\", \"PV4SCIE\", \n           \"PV5SCIE\", \"PV6SCIE\", \"PV7SCIE\", \"PV8SCIE\", \"PV9SCIE\", \"PV10SCIE\")\n\n\n\n\nThe dataset from PISA is expected to be relatively clean. Nevertheless, due diligence checks for duplicates and missing values are still made to confirm the assumption.\nThe duplicated() function in the base package is used to check for duplicates in stu_qqq_SG_final. There are no duplicates in the tibble data frame.\n\nstu_qqq_SG_final[duplicated(stu_qqq_SG_final), ]\n\n# A tibble: 0 × 36\n# ℹ 36 variables: CNTSCHID &lt;dbl&gt;, CNTSTUID &lt;dbl&gt;, STRATUM &lt;chr&gt;,\n#   ST004D01T &lt;dbl&gt;, ESCS &lt;dbl&gt;, ST255Q01JA &lt;dbl&gt;, PV1MATH &lt;dbl&gt;,\n#   PV2MATH &lt;dbl&gt;, PV3MATH &lt;dbl&gt;, PV4MATH &lt;dbl&gt;, PV5MATH &lt;dbl&gt;, PV6MATH &lt;dbl&gt;,\n#   PV7MATH &lt;dbl&gt;, PV8MATH &lt;dbl&gt;, PV9MATH &lt;dbl&gt;, PV10MATH &lt;dbl&gt;, PV1READ &lt;dbl&gt;,\n#   PV2READ &lt;dbl&gt;, PV3READ &lt;dbl&gt;, PV4READ &lt;dbl&gt;, PV5READ &lt;dbl&gt;, PV6READ &lt;dbl&gt;,\n#   PV7READ &lt;dbl&gt;, PV8READ &lt;dbl&gt;, PV9READ &lt;dbl&gt;, PV10READ &lt;dbl&gt;, PV1SCIE &lt;dbl&gt;,\n#   PV2SCIE &lt;dbl&gt;, PV3SCIE &lt;dbl&gt;, PV4SCIE &lt;dbl&gt;, PV5SCIE &lt;dbl&gt;, …\n\n\nThe colSums() function in the base package is used to check for missing values in stu_qqq_SG_final. There are no missing values in the tibble data frame for all most columns except for two:\n\nThe column with the Index of Economic, Social, and Cultural Status (“ESCS”) has 47 rows (observations) with NA values.\nThe column indicating the number of books at home (“ST255Q01JA” has 44 rows (observations) with NA values.\n\nIn total, there are 50 rows with one or more NA values. As this makes up only 0.757% of the 6,606 observations, we will remove them from the subsequent analysis. The na.omit() function in the stats package is used to remove them from stu_qqq_SG_final, which now has 6,556 observations and 36 variables. A confirmatory check is then made with the colSums() function in the base package.\n\ncolSums(is.na(stu_qqq_SG_final))\n\n  CNTSCHID   CNTSTUID    STRATUM  ST004D01T       ESCS ST255Q01JA    PV1MATH \n         0          0          0          0         47         44          0 \n   PV2MATH    PV3MATH    PV4MATH    PV5MATH    PV6MATH    PV7MATH    PV8MATH \n         0          0          0          0          0          0          0 \n   PV9MATH   PV10MATH    PV1READ    PV2READ    PV3READ    PV4READ    PV5READ \n         0          0          0          0          0          0          0 \n   PV6READ    PV7READ    PV8READ    PV9READ   PV10READ    PV1SCIE    PV2SCIE \n         0          0          0          0          0          0          0 \n   PV3SCIE    PV4SCIE    PV5SCIE    PV6SCIE    PV7SCIE    PV8SCIE    PV9SCIE \n         0          0          0          0          0          0          0 \n  PV10SCIE \n         0 \n\n\n\nstu_qqq_SG_final = stu_qqq_SG_final %&gt;%\n  na.omit()\n\n\ncolSums(is.na(stu_qqq_SG_final))\n\n  CNTSCHID   CNTSTUID    STRATUM  ST004D01T       ESCS ST255Q01JA    PV1MATH \n         0          0          0          0          0          0          0 \n   PV2MATH    PV3MATH    PV4MATH    PV5MATH    PV6MATH    PV7MATH    PV8MATH \n         0          0          0          0          0          0          0 \n   PV9MATH   PV10MATH    PV1READ    PV2READ    PV3READ    PV4READ    PV5READ \n         0          0          0          0          0          0          0 \n   PV6READ    PV7READ    PV8READ    PV9READ   PV10READ    PV1SCIE    PV2SCIE \n         0          0          0          0          0          0          0 \n   PV3SCIE    PV4SCIE    PV5SCIE    PV6SCIE    PV7SCIE    PV8SCIE    PV9SCIE \n         0          0          0          0          0          0          0 \n  PV10SCIE \n         0 \n\n\n\n\n\nFor ease of use, the columns are renamed accordingly using the rename() function in the dplyr package.\n\nstu_qqq_SG_final = stu_qqq_SG_final %&gt;%\n  rename(School = \"CNTSCHID\",\n         SchoolType = \"STRATUM\",\n         ID = \"CNTSTUID\",\n         Gender = \"ST004D01T\",\n         SocioeconStatus = \"ESCS\",\n         Books = \"ST255Q01JA\",\n         Math01 = \"PV1MATH\", Math02 = \"PV2MATH\", \n         Math03 = \"PV3MATH\", Math04 = \"PV4MATH\", \n         Math05 = \"PV5MATH\", Math06 = \"PV6MATH\", \n         Math07 = \"PV7MATH\", Math08 = \"PV8MATH\", \n         Math09 = \"PV9MATH\", Math10 = \"PV10MATH\", \n         Read01 = \"PV1READ\", Read02 = \"PV2READ\", \n         Read03 = \"PV3READ\", Read04 = \"PV4READ\", \n         Read05 = \"PV5READ\", Read06 = \"PV6READ\", \n         Read07 = \"PV7READ\", Read08 = \"PV8READ\", \n         Read09 = \"PV9READ\", Read10 = \"PV10READ\", \n         Sci01 = \"PV1SCIE\", Sci02 = \"PV2SCIE\", \n         Sci03 = \"PV3SCIE\", Sci04 = \"PV4SCIE\",\n         Sci05 = \"PV5SCIE\", Sci06 = \"PV6SCIE\", \n         Sci07 = \"PV7SCIE\", Sci08 = \"PV8SCIE\", \n         Sci09 = \"PV9SCIE\", Sci10 = \"PV10SCIE\")\n\nAlso, for the ease of use, the values for Gender, School Type, and Number of Books are replaced with characters using the ifelse() function in the base package.\n\nstu_qqq_SG_final$Gender = ifelse(\n  stu_qqq_SG_final$Gender == 01, \n  \"Female\", \"Male\")\nstu_qqq_SG_final$SchoolType = ifelse(\n  stu_qqq_SG_final$SchoolType == \"SGP01\", \n  \"Public\", \"Private\")\n\nstu_qqq_SG_final = stu_qqq_SG_final %&gt;%\n  mutate(Books = recode(Books, \n                        \"01\" = \"0 Book\",\n                        \"02\" = \"1-10 Books\",\n                        \"03\" = \"11-25 Books\",\n                        \"04\" = \"26-100 Books\",\n                        \"05\" = \"101-200 Books\",\n                        \"06\" = \"201-500 Books\",\n                        \"07\" = \"&gt;500 Books\"))\n\n\n\n\nThere are 10 Plausible Values (PVs) each for Mathematics, Reading, and Science. However, PISA cautions against averaging the PVs at the student level. Instead, it suggests that population statistics should be estimated using each PV separately - e.g., if one is interested in the correlation coefficient between the social index and the reading performance in PISA, 10 correlation coefficients should be computed and then averaged.\nA combination of half-density plot and box plot are plotted for the each of the 10 PVs for each subject. Firstly, the select() function in the dplyr package and the melt() function in the reshape2 package are used to select the relevant columns and transform them into a long table. The variables are then renamed using the rename() function in the dplyr package.\nThe ggplot(), geom_boxplot(), coord_flip() functions in the ggplot2 package, and the stat_halfeye() function in the ggdist package are used to create the plots. The ggtitle() and theme() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title, centre the plot title, and rotate the y-axis label.\n\nMathReadingScience\n\n\n\n\nCode\nmath = stu_qqq_SG_final %&gt;% \n  select(Math01, Math02, Math03, Math04, Math05,\n         Math06, Math07, Math08, Math09, Math10) %&gt;%\n  melt() %&gt;%\n  rename(\"PV\" = \"variable\", \"Score\" = \"value\")\n\nggplot(math, \n       aes(x = PV, \n           y = Score)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .15,\n               outlier.shape = NA) +\n  coord_flip() +\n  ggtitle(\"Distribution of Math Scores for Different PVs\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\n\n\n\n\n\nCode\nread = stu_qqq_SG_final %&gt;% \n  select(Read01, Read02, Read03, Read04, Read05,\n         Read06, Read07, Read08, Read09, Read10) %&gt;%\n  melt() %&gt;%\n  rename(\"PV\" = \"variable\", \"Score\" = \"value\")\n\nggplot(read, \n       aes(x = PV, \n           y = Score)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .15,\n               outlier.shape = NA) +\n  coord_flip() +\n  ggtitle(\"Distribution of Reading Scores for Different PVs\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\n\n\n\n\n\nCode\nsci = stu_qqq_SG_final %&gt;% \n  select(Sci01, Sci02, Sci03, Sci04, Sci05,\n         Sci06, Sci07, Sci08, Sci09, Sci10) %&gt;%\n  melt() %&gt;%\n  rename(\"PV\" = \"variable\", \"Score\" = \"value\")\n\nggplot(sci, \n       aes(x = PV, \n           y = Score)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .15,\n               outlier.shape = NA) +\n  coord_flip() +\n  ggtitle(\"Distribution of Science Scores for Different PVs\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\n\n\n\n\nBased on the plots, the 10 sets of PVs for each subject are broadly similar to one another (similar distributions and similar median values). Hence, for the purposes of this take-home exercise, the PV 1 values for each subject are used.\nThe select() and rename() functions in the dplyr package are used to further narrow down the number of variables chosen to 9 out of the preliminary 36 variables chosen previously, and rename some of the columns for easier identification.\n\nstu_qqq_SG_final = stu_qqq_SG_final %&gt;% \n  select(\"ID\", \"School\", \"SchoolType\", \n            \"Gender\", \"SocioeconStatus\", \n            \"Books\", \n            \"Math01\", \"Read01\", \"Sci01\") %&gt;%\n  rename(\"Mathematics\" = \"Math01\",\n         \"Reading\" = \"Read01\",\n         \"Science\" = \"Sci01\")\n\nThe finalised tibble data frame, stu_qqq_SG_final, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(stu_qqq_SG_final, \"data/stu_qqq_SG_final.rds\")\n\n\nstu_qqq_SG_final = read_rds(\"data/stu_qqq_SG_final.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#exploratory-data-analysis---computing-visualising-and-deriving-insights-on-singapores-pisa-performance",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#exploratory-data-analysis---computing-visualising-and-deriving-insights-on-singapores-pisa-performance",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "A ridgeline plot is created to visualise the distributions of the Singapore students’ performance for all three subjects within one plot.\nFirstly, the select() function in the dplyr package and the melt() function in the reshape2 package are used to select the relevant columns and transform them into a long table. The variables are then renamed using the rename() function in the dplyr package. The mutate() function in the dplyr package is then used with the “fct_relevel” argument to manually arrange the subjects’ order in the plot.\nThen, the ggplot() and scale_fill_viridis_d() functions in the ggplot2 package, and the stat_density_ridges() function in the ggridges package are used to create the plot. The ggtitle() and theme() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title and subtitle, centre the plot title, and rotate the y-axis label.\n\nPlot 1Code\n\n\n\n\n\n\n\n\n\n\ncomb = stu_qqq_SG_final %&gt;% \n  select(Mathematics, Reading, Science) %&gt;%\n  melt() %&gt;%\n  rename(\"Subject\" = \"variable\", \"Score\" = \"value\") %&gt;%\n  mutate(Subject = fct_relevel(Subject, \n                               \"Mathematics\", \n                               \"Science\", \n                               \"Reading\"))\n\nggplot(comb, \n       aes(x = Score, \n           y = Subject,\n           fill = factor(after_stat(quantile)))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\", alpha = 0.8) +\n  ggtitle(label = \"SG Students Generally Perform Better in\\nMathematics than Science & Reading\",\n          subtitle = \"Distribution of Mathematics, Science and Reading Scores\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\nObservation: Based on the plot, we can see that the scores for all three subjects resemble normal distribution. Also, based on the quartile lines, the scores for Mathematics tend to be higher than that for Science and Reading; while the scores of Science tend to be higher than that for Reading. This corresponds to general perceptions that Singapore students tend to be better at technical subjects (such as Mathematics and Science) as compared to subjects related to language skills (such as Reading).\nFuture research in this area may consider the historical trends in the differences between Singapore students’ performance in the three subjects and whether there are societal and economic forces behind such trends.\n\n\n\n\n\nThere is often a stereotype that men are better than women in the areas of science and technology (S&T). In this take-home exercise, we attempt to answer the question: “Are girls weaker than boys at Science in Singapore?” by creating a density plot of the Singapore students’ Science scores by gender.\nFirstly, the filter() function in the dplyr package is used to select the subsets of stu_qqq_SG_final containing the observations for each gender. The mutate() function in the dplyr package is then used with the “fct_relevel” argument to manually arrange the two gender’s order in the plot so that the Female density plot uses the stereotypical/associated colour of pink, while the Male density plot uses the stereotypical/associated colour of blue.\nThen, the ggplot(), geom_density() and geom_vline() functions in the ggplot2 package are used to create the plot and insert vertical lines indicating the median scores for each gender (again, pink for Female, and blue for Male). The ggtitle(), theme(), xlab(), and ylab() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title and subtitle, centre the plot title, rotate the y-axis label, and overwrite the axes’ labels.\n\nPlot 2Code\n\n\n\n\n\n\n\n\n\n\nf = stu_qqq_SG_final %&gt;%\n  filter(Gender == \"Female\")\n\nm = stu_qqq_SG_final %&gt;%\n  filter(Gender == \"Male\")\n\nstu_qqq_SG_final = stu_qqq_SG_final %&gt;%\n  mutate(Gender = fct_relevel(Gender, \n                               \"Male\", \n                               \"Female\"))\n\nggplot(stu_qqq_SG_final,\n       aes(x = Science,\n           fill = Gender)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(aes(xintercept=median(f$Science)),\n             color=\"red\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) +\n  geom_vline(aes(xintercept=median(m$Science)),\n             color=\"blue\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) +\n  ggtitle(label = \"Girls Are Not Necessarily Weaker in Science\",\n          subtitle= \"Distribution of Science Scores by Gender\") + \n  ylab(\"Density\") + xlab(\"Score\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\nObservation: Based on the plot, the distribution of Science scores for both Male and Female are broadly similar. With slightly more boys than girls on the two ends of the distribution (i.e., either poor or excellent scores), and conversely, more girls than boys in the centre of the distribution (i.e., close to median scores). The median scores for both gender are also very close to each other, with Male slightly higher than Female. This debunks the notion that girls are weaker in Science compared to boys.\nFuture research may consider why a relatively similar distribution in the Science performance of Singapore students at 15-year-old does not translate to a higher proportion of females undertaking S&T endeavours in college and beyond.\n\n\n\nParents in Singapore are often concerned about the type of schools their children enrol in as there is a strong, ingrained belief that some schools (i.e., Private) are better than others (i.e., Public). In this take-home exercise, we attempt to answer the question: “Do students in public schools perform poorer in Mathematics?” by creating a half-eye plot combined with a box plot for the Mathematics scores by type of school.\nFirstly, the filter() function in the dplyr package is used to select the subsets of stu_qqq_SG_final containing the observations for each gender. The mutate() function in the dplyr package is then used with the “fct_relevel” argument to manually arrange the two gender’s order in the plot so that the Female density plot uses the stereotypical/associated colour of pink, while the Male density plot uses the stereotypical/associated colour of blue.\nThe ggplot(), geom_boxplot() and coord_flip() functions in the ggplot2 package are used to create the plot, insert the box plots for each type of school, and change the orientation of the plot. The stat_halfeye() and stat_dots() functions from the ggdist package are used to show the distributions for each type of school. The ggtitle(), theme(), xlab(), and ylab() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title and subtitle, centre the plot title, rotate the y-axis label, and overwrite the axes’ labels.\n\nPlot 3Code\n\n\n\n\n\n\n\n\n\n\nggplot(stu_qqq_SG_final, \n       aes(x = SchoolType, \n           y = Mathematics)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.15,\n               .width = 0,\n               point_colour = NA,\n               scale= 0.55) +\n  geom_boxplot(width = .15,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = 3,\n            dotsize = 0.1) +\n  coord_flip() +\n  ggtitle(label = \"Public Schools Have Both Best and Worst Performing\\n Students in Mathematics\",\n          subtitle= \"Distribution of Mathematics Scores by School Type\") +\n  xlab(\"School\\nType\") + ylab(\"Score\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\nObservation: Based on the plot, the distribution of Mathematics scores for Singapore students in public and private schools differ in the following ways:\n\nThe range of scores in public schools is larger than that in private schools, i.e., public schools have a wider range of student capabilities for Mathematics;\nRelated to above, there is a greater density of students in private schools that perform moderately well (i.e., around the median score for private schools) compared to students from public schools.\nThe median Mathematics score for students in private schools is slightly higher than that for public schools.\n\nIt is important to note from the dot plots that there are a much larger number of students in public schools than private schools, which may mean that the results for private schools may not be representative.\nMore research would need to be conducted to uncover how and why the relationship between the type of school and Mathematics performance are as shown by the various observations. Also, there may be granular differences in Mathematics performance between different public schools.\n\n\n\nResearch has continued to show that socioeconomic status influence academic performance. In this take-home exercise, we attempt to answer the question: “Are Reading scores related to socioeconomic status?” by creating a scatter plot to show the distribution of Reading scores vis-a-vis the Index of Economic, Social, and Cultural Status values.\nThe ggplot(), geom_point() and geom_smooth() functions in the ggplot2 package are used to create the scatter plot, and add a smooth line showing the correlation between the two variables. The stat_cor() function in the ggpubr package is used to generate the correlation coefficient, R, which is 0.41. The ggtitle(), theme(), xlab(), and ylab() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title and subtitle, centre the plot title, rotate the y-axis label, and overwrite the axes’ labels.\n\nPlot 4Code\n\n\n\n\n\n\n\n\n\n\nggplot(data = stu_qqq_SG_final, \n            aes(x = Reading, \n                y = SocioeconStatus)) +   \n  geom_point(alpha = 0.1) +\n  geom_smooth(linewidth = 1, colour = \"pink\") + \n  stat_cor() +\n  ggtitle(label = \"Moderate Positive Correlation between\\nReading Scores and Socioeconomic Status\",\n          subtitle= \"Distribution of Reading Scores vis-a-vis Socioeconomic Status\") +\n  xlab(\"Score\") + ylab(\"Socioeconomic\\nStatus\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\nObservation: Based on the plot, there is no clear linear relationship between Reading scores and socioeconomic status given that the dots are generally well distributed, i.e., there are students with low socioeconomic status (below 0 value) with scores ranging from 200 to 800, as there are students with high socio economic status (above 0 value) with scores ranging from 200 to 800. Nevertheless, there is a higher concentration of students in the 400-600 score range with high socioeconomic status compared to students with low socioeconomic status in the same score range.\nFuture research may consider whether there are better variables that may explain differences in Reading performance.\nFor a start, we consider if a simpler variable such as the number of books may help to explain differences in Reading performance better than the Index of Economic, Social, and Cultural Status values. A ridgeline created to visualise the distributions of the Singapore students’ Reading performance depending on the self-reported number of books at home.\nFirstly, the select() function in the dplyr package and the melt() function in the reshape2 package are used to select the relevant columns and transform them into a long table. The mutate() function in the dplyr package is then used with the “fct_relevel” argument to manually arrange the number of books in ascending order in the plot.\nThen, the ggplot() and scale_fill_viridis_d() functions in the ggplot2 package, and the stat_density_ridges() function in the ggridges package are used to create the plot. The ggtitle() and theme() functions in the ggplot2 package are then used to make aesthetic adjustments to insert the plot title and subtitle, centre the plot title, rotate the y-axis label, and remove the legend.\n\nPlot 5Code\n\n\n\n\n\n\n\n\n\n\nr = stu_qqq_SG_final %&gt;% \n  select(Books, Reading) %&gt;%\n  mutate(Books = fct_relevel(Books, \n                              \"0 Book\", \n                              \"1-10 Books\",\n                              \"11-25 Books\",\n                              \"26-100 Books\",\n                              \"101-200 Books\",\n                              \"201-500 Books\",\n                              \"&gt;500 Books\"))\n                        \nggplot(r, \n       aes(x = Reading, \n           y = Books,\n           fill = factor(after_stat(quantile)))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 2,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(alpha = 0.8) +\n  ggtitle(label = \"More Books at Home Generally Correspond\\nto Higher Reading Scores\",\n          subtitle = \"Distribution of Reading Scores vis-a-vis Number of Books at Home\") +\n  xlab(\"Score\") + ylab(\"No.of\\nBooks\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1), \n        legend.position = \"none\")\n\n\n\n\n\n\n\nObservation: Based on the plot, there is a relationship between the number of books at home and the Reading scores. Focusing on the median values for each ridge (as shown by the divider line between the two colours), the values increase as the number of books at home increase. The only exception is the median value of Reading scores for students with more than 500 books at home, which is lower than that for 201-500 books.\nHence, we can see a clearer trend between Reading scores and number of books as compared to that between Reading scores and the Index of Economic, Social, and Cultural Status values. Further analysis can be conducted to determine the correlation coefficient of the relationship and whether the relationship is statistically significant."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#conclusion",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "In conclusion, the ggplot2 package is a powerful package for exploratory data analysis through visualisation. The PISA performance dataset is interesting and contains useful data for studying Singapore students’ academic performance in the three subjects at age 15. The insights gained from analysing the various questions posed in this take-home exercise provides a preview of the data analyses that can be conducted in further studies to better understand how various factors including gender, type of school, and socioeconomic status affect academic performance, so as to better inform education policy planning in Singapore."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#key-references",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#key-references",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "R for Visual Analytics.\nR for Data Science.\nFundamentals of Data Visualisation.\nPISA 2022 Database and Technical Report.\nOECD on Singapore: Rapid Improvement Followed by Strong Performance.\n\n~~~ End of Take-home Exercise 1 ~~~"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html",
    "title": "Take-home Exercise 4",
    "section": "",
    "text": "Singapore has come a long way since gaining independence in 1965. Without natural resources, human capital development through a well-planned, world-class education system was a critical part of Singapore’s transformation from third world to first. Despite the internationally lauded success, Singapore’s education system is far from perfect – there still exists gaps in education achievement among students. According to the latest OECD’s Programme for International Student Assessment (PISA) 2022, which measures 15-year-olds’ ability to use their reading, mathematics, and science knowledge and skills to meet real-life challenges, socioeconomic status accounted for 17% of the variation in mathematics performance in Singapore (compared to 15% on average across OECD countries). Clearly, Singapore’s success does not translate to success for every student. Why then do some students outperform others? And is socioeconomic status the only factor for success?\nOur team believes that knowledge is power. While causality cannot and should not be easily drawn between the various forces of influence and academic performance, a more detailed and nuanced understanding of these factors would highlight potential areas to focus on when engaging parents and students as well as when developing education and socioeconomic policies for a more inclusive and equitable society.\n\n\n\nThe PISA is treated as a platform for geopolitical competition, as shown by an overemphasis on the comparison of PISA scores between different countries. Scant attention is paid to the extensive data collected through the student, parent, and school questionnaires that are administered alongside the tests, which may reveal more a nuanced understanding of the individual lived experiences of students.\nResearch studies on the gaps in education attainment in Singapore tends to focus on traditional factors such as gender, race, and socioeconomic status. While these are important structural factors, they should be complemented by individual-level factors, such as psychological wellness, and social relations. Gen Alpha children are said to have reduced attention span, and less time for socialising. These could pose challenges in their education journeys, and deserve closer attention.\nTechnical knowledge is required to perform accurate and reliable data wrangling and analysis of the survey outcomes from PISA. Such valuable information should be made available to the public in an accessible and interactive manner for self-discovery.\n\n\n\nThe team will extract, analyse, and visualise the relationships between different factors and the PISA scores of Singapore students. Using various R packages, an interactive R shiny application with visual analytics techniques will be presented to enable users to interact with the data based on their personal interests and circumstances, and draw their own insights and conclusions regarding the state of learning in Singapore.\nUsers will be able to:\n\nCompare the PISA scores between different groups of students based on factors such as gender, socioeconomic status (e.g., employment status of parents, immigrant status), and psychological well-being (e.g., feeling of loneliness, resilience when faced with challenges).\nGain an understanding of the varying levels of importance and statistical significance of the various influences on PISA scores for different clusters of students.\n\nOverall, the research hopes to generate new knowledge on disparities in the everyday stress experience and psychological well-being between lower and higher SES students, and their spillover effects on academic performance and persistence. The broader implication of the study is that educational policies must actively address psychological challenges alongside resource challenges to better mitigate socioeconomic gaps in educational attainment.\n\n\n\nIn this take-home exercise, the objectives are to:\n\nEvaluate and determine if the necessary R packages needed for the team’s Shiny application are supported in R CRAN;\nPrepare and test the specific R codes to return the expected visualisation outputs;\nDetermine the parameters and outputs that will be provided in the Shiny application; and\nSelect the appropriate Shiny user interface (UI) components for displaying the parameters determined.\n\nIn this take-home exercise, I focus on prototyping the Cluster Analysis portion of the Shiny application, while my teammates would work on the Univariate & Bivariate Analysis, and Multivariate Analysis portions.\n\n\n\n\n\n\nThe R packages used for data wrangling are:\n\nhaven for importing SAS files;\ntidyverse (i.e. readr, tidyr, dplyr, ggplot2) for performing data science tasks such as importing, tidying, and wrangling data, as well as creating graphics based on The Grammar of Graphics;\njanitor for data exploration and cleaning;\nreshape2 for transforming data between wide and long formats; and\nDT and kableExtra for dynamic report generation.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed in the computer. If yes, they are then loaded into the R environment. If no, they are installed, and then loaded into the R environment.\n\npacman::p_load(haven, tidyverse, \n               janitor, reshape2,\n               DT, kableExtra)\n\n\n\n\nThe OECD Programme for International Student Assessment (PISA) measures how well 15-year-old students in different countries are “prepared to meet the challenges of today’s knowledge societies” by looking at “their ability to use their knowledge and skills to meet real-life challenges”. The PISA surveys take place very three years, the latest being conducted in 2022.\nThe PISA 2022 database contains the full set of responses from individual students, school principals, and parents. There are a total of five data files and their contents are as follows:\n\nStudent questionnaire data file;\nSchool questionnaire data file;\nTeacher questionnaire data file;\nCognitive item data file; and\nQuestionnaire timing data file.\n\nFor the purpose of this take-home exercise, the “Student questionnaire data file” is used.\n\n\n\n\n\n\nThe dataset used in this take-home exercise is the 2022 PISA student questionnaire data file, cy08msp_stu_qqq.sas7bdat, which is in the SAS file format.\nThe file is imported into the R environment using the read_sas() function in the haven package and stored as the R object, stu_qqq.\n\nstu_qqq = read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\nThe tibble data frame, stu_qqq, has 1,279 columns (variables) and 613,744 rows (observations).\n\n\n\nThere are 6,606 rows with the country code (i.e., CNT) value of “SGP”, which represents Singapore. This count is cross-verified by the information provided in the “CY08MSP_STU_QQQ” sheet in the codebook. The codebook also stated that Singapore students’ made up 1.0763% of the entire global student population who took part in the 2022 PISA.\nThe filter() function in the dplyr package is used to obtain these rows, and stored as the R object, stu_qqq_SG.\n\nstu_SG = stu_qqq %&gt;% filter(CNT == \"SGP\")\n\nThe tibble data frame, stu_SG, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(stu_SG, \"data/stu_SG.rds\")\n\n\nstu_SG = read_rds(\"data/stu_SG.rds\")\n\n\n\n\nAfter perusing the Codebook and Technical Report, the team narrowed down the questions from the survey that would yield insightful results. The names of the columns are stored in a vector, colname. To filter the raw dataset with the columns, the select() function in the readr package is used to identify all the variables listed out in the colname vector.\n\ncolname = c(\"ST034Q06TA\", \"ST265Q03JA\", \"ST270Q03JA\", \"ST004D01T\", \"ST296Q01JA\", \"ST296Q02JA\", \"ST296Q03JA\", \"STRATUM\", \"CNTSCHID\", \"HISCED\", \"IMMIG\", \"ST022Q01TA\", \"ST230Q01JA\", \"ST250D06JA\", \"ST250D07JA\", \"ST251Q01JA\", \"ST255Q01JA\", \"EXERPRAC\", \"ST250Q01JA\", \"WORKHOME\", \"ST268Q01JA\", \"ST268Q02JA\", \"ST268Q03JA\")\n\nThe relevant columns are then extracted using the following functions:\n\nselect() function to retain the following columns:\n\nVariables identified in colname and\nColumns that starts with “PV” and contains either “MATH”, “SCIE”, or “READ” to extract the plausible values of scores related to the subjects of Mathematics, Science, and Reading. This is performed using a combination of starts_with() and contains().\n\nstarts_with(): Matches the beginning of the column name with “PV”, and\ncontains(): Searches for columns containing three alternative subjects to be matched.\n\n\nmutate() to create 3 new variables to store the mean plausible values for each subject for each row using rowMeans() and across().\n\n\nstu_SG_filtered = stu_SG %&gt;% \n\n  # Retains desired variables\n  select(all_of(colname), starts_with(\"PV\") & contains(c(\"MATH\", \"READ\", \"SCIE\"))) %&gt;% \n\n  # Calculates the mean of plausible values for each subject per student\n  mutate(Math = rowMeans(across(starts_with(\"PV\") & contains(\"MATH\")), na.rm = TRUE),\n         Reading = rowMeans(across(starts_with(\"PV\") & contains(\"READ\")), na.rm = TRUE),\n         Science = rowMeans(across(starts_with(\"PV\") & contains(\"SCIE\")), na.rm = TRUE),\n         ) %&gt;% \n  \n  # Drops Plausible Values columns\n  select(-starts_with(\"PV\"))\n\nThe tibble data frame, stu_SG_filtered, contains 6,606 observations across 26 variables.\n\n\n\nThe columns are then renamed using the rename() function in the dplyr package.\n\nstu_SG_filtered =\n  stu_SG_filtered %&gt;% \n  rename(\n    \"Loneliness\" = \"ST034Q06TA\",\n    \"ClassroomSafety\" = \"ST265Q03JA\",\n    \"TeacherSupport\" = \"ST270Q03JA\",\n    \"Gender\" = \"ST004D01T\",\n    \"Homework_Math\" = \"ST296Q01JA\",\n    \"Homework_Reading\" = \"ST296Q02JA\",\n    \"Homework_Science\" = \"ST296Q03JA\",\n    \"SchoolType\" = \"STRATUM\",\n    \"SchoolID\" = \"CNTSCHID\",\n    \"ParentsEducation\" = \"HISCED\",\n    \"Immigration\" = \"IMMIG\",\n    \"HomeLanguage\" = \"ST022Q01TA\",\n    \"Sibling\" = \"ST230Q01JA\",\n    \"Aircon\" = \"ST250D06JA\",\n    \"Helper\" = \"ST250D07JA\",\n    \"Vehicle\" = \"ST251Q01JA\",\n    \"Books\" = \"ST255Q01JA\",\n    \"Exercise\" = \"EXERPRAC\",\n    \"OwnRoom\" = \"ST250Q01JA\",\n    \"FamilyCommitment\" = \"WORKHOME\",\n    \"Preference_Math\" = \"ST268Q01JA\",\n    \"Preference_Reading\" = \"ST268Q02JA\",\n    \"Preference_Science\" = \"ST268Q03JA\")\n\n\n\n\nThere are some responses which are marked as invalid, or missing, in the “Helper” and “Aircon” variables. These responses are replaced with “NA”.\n\nstu_SG_encode = stu_SG_filtered %&gt;% \n  mutate(Aircon = na_if(Aircon, \"9999999\"),\n         Helper = na_if(Helper, \"9999999\"))\n\n\n\n\nThere are several types of responses for the Student’s Questionnaire. We store all the response levels for each question in separate vectors and subsequently combine to create a global dictionary named dict.\n\nBooks = c('1' = \"0\",\n               '2' = \"1 - 10\",\n               '3' = \"11 - 25\",\n               '4' = \"26 - 100\",\n               '5' = \"101 - 200\",\n               '6' = \"201-500\",\n               '7' = \"&gt;500\")\n\nHomeLanguage = c('1' = \"English\",\n             '2' = \"Others\")\n\n# Likert Scales: Strong Disagree to Strongly Agree\nPreference_Math = c('1' = \"Strongly Disagree\",\n           '2' = \"Disagree\",\n           '3' = \"Agree\",\n           '4' = \"Strongly Agree\")\n\nPreference_Reading = c('1' = \"Strongly Disagree\",\n           '2' = \"Disagree\",\n           '3' = \"Agree\",\n           '4' = \"Strongly Agree\")\n\nPreference_Science = c('1' = \"Strongly Disagree\",\n           '2' = \"Disagree\",\n           '3' = \"Agree\",\n           '4' = \"Strongly Agree\")\n\n# Likert Scales: Strong Agree to Strongly Disagree\nLoneliness = c('1' = \"Strongly Agree\",\n           '2' = \"Agree\",\n           '3' = \"Disagree\",\n           '4' = \"Strongly Disagree\")\n\nClassroomSafety = c('1' = \"Strongly Agree\",\n           '2' = \"Agree\",\n           '3' = \"Disagree\",\n           '4' = \"Strongly Disagree\")\n\n# Binary\nSchoolType = c('SGP01' = \"Public\",\n           'SGP03' = \"Private\")\n\nOwnRoom = c('1' = \"Yes\", \n                '2' = \"No\")\n\nAircon = c('7020001' = \"Yes\",\n            '7020002' = \"No\",\n            .default = NA\n            )\n\nHelper = c('7020001' = \"Yes\",\n            '7020002' = \"No\",\n            .default = NA)\n\n# Frequency responses\nExercise = c('0' = \"0\",\n          '1' = \"1\", \n          '2' = \"2\",\n          '3' = \"3\",\n          '4' = \"4\",\n          '5' = \"5\",\n          '6' = \"6\",\n          '7' = \"7\",\n          '8' = \"8\",\n          '9' = \"9\",\n          '10' = \"10\")\n\nFamilyCommitment = c('0' = \"0\",\n          '1' = \"1\", \n          '2' = \"2\",\n          '3' = \"3\",\n          '4' = \"4\",\n          '5' = \"5\",\n          '6' = \"6\",\n          '7' = \"7\",\n          '8' = \"8\",\n          '9' = \"9\",\n          '10' = \"10\")\n\n# Time Periods\nHomework_Math = c('1' = \"≤ 0.5hr\",\n                '2' = \"0.5hr - 1hr\",\n                '3' = \"1hr - 2hr\",\n                '4' = \"2hr - 3hr\",\n                '5' = \"3 - 4 hr\",\n                '6' = \"&gt; 4hr\")\n\nHomework_Reading = c('1' = \"≤ 0.5hr\",\n                '2' = \"0.5hr - 1hr\",\n                '3' = \"1hr - 2hr\",\n                '4' = \"2hr - 3hr\",\n                '5' = \"3 - 4 hr\",\n                '6' = \"&gt; 4hr\")\n\nHomework_Science = c('1' = \"≤ 0.5hr\",\n                '2' = \"0.5hr - 1hr\",\n                '3' = \"1hr - 2hr\",\n                '4' = \"2hr - 3hr\",\n                '5' = \"3 - 4 hr\",\n                '6' = \"&gt; 4hr\")\n\n# Gender\nGender = c('1' = \"Female\",\n            '2' = \"Male\")\n\n\n# Immigrant Background\nImmigration = c('1' = \"Native\",\n           '2' = \"2nd Generation\",\n           '3' = \"3rd Generation\")\n\n# Education Level\nParentsEducation = c('1'=\"Pre-Primary\",   \n         '2'=\"Primary\", \n         '3'=\"Secondary\",\n         '4'='Secondary',\n         '6'=\"Post-Secondary\",\n         '7'=\"Post-Secondary\",\n         '8'=\"Tertiary\",\n         '9'=\"Tertiary\",\n         '10'=\"Tertiary\")\n\n# Posessions\nVehicle = c('1' = \"0\",\n            '2' = \"1\",\n            '3' = \"2\",\n            '4' = \"≥3\")\n\nSibling = c('1' = \"0\",\n            '2' = \"1\",\n            '3' = \"2\",\n            '4' = \"≥3\")\n\n# Support\nTeacherSupport = c('1' = \"Every lesson\",\n            '2' = \"Most lesson\",\n            '3' = \"Some lessons\",\n            '4' = \"Never or almost never\")\n\n# Global Dictionary\ndict = list(\n  \"Loneliness\" = Loneliness,\n  \"ClassroomSafety\" = ClassroomSafety,\n  \"TeacherSupport\" = TeacherSupport,\n  \"Gender\" = Gender,\n  \"Homework_Math\" = Homework_Math,\n  \"Homework_Reading\" = Homework_Reading,\n  \"Homework_Science\" = Homework_Science,\n  \"SchoolType\" = SchoolType,\n  \"ParentsEducation\" = ParentsEducation,\n  \"Immigration\" = Immigration,\n  \"HomeLanguage\" = HomeLanguage,\n  \"Sibling\" = Sibling,\n  \"Aircon\" = Aircon,\n  \"Helper\" = Helper,\n  \"Vehicle\" = Vehicle,\n  \"Books\" = Books,\n  \"Exercise\" = Exercise,\n  \"OwnRoom\" = OwnRoom,\n  \"FamilyCommitment\" = FamilyCommitment,\n  \"Preference_Math\" = Preference_Math,\n  \"Preference_Reading\" = Preference_Reading,\n  \"Preference_Science\" = Preference_Science)\n\nThe helper function below attempts to recode all of the columns based on the global recode dictionary, dict, using functions from the base R, tidyr, and rlang packages:\n\nThe names(x) function retrieves the column names of the input dataframe\nThe recode() function helps to recode values in the columns using dicts\nThe !!sym(x_nm) function unquotes and evaluates the column name that matches the names of the dictionaries, while the !!!dicts[[x_nm]] function unquotes and splices the global recoding dictionary corresponding to the column name.\n\n\nrcd = function(x) {\n  x_nm = names(x)\n  mutate(x, !! x_nm := recode(!! sym(x_nm), !!! dict[[x_nm]]))\n}\n\nThe lmap_at() function in the purrr package applies the helper function to the column in the data frame where the column name matches the keys of the dictionaries.\n\nstu_SG_rcd = lmap_at(stu_SG_encode, \n        names(dict),\n        rcd)\n\nThe get_dupes() function in the janitor package is used to hunt for duplicate records. The results show that there are no duplicated rows.\n\nget_dupes(stu_SG_rcd)\n\n [1] Loneliness         ClassroomSafety    TeacherSupport     Gender            \n [5] Homework_Math      Homework_Reading   Homework_Science   SchoolType        \n [9] SchoolID           ParentsEducation   Immigration        HomeLanguage      \n[13] Sibling            Aircon             Helper             Vehicle           \n[17] Books              Exercise           OwnRoom            FamilyCommitment  \n[21] Preference_Math    Preference_Reading Preference_Science Math              \n[25] Reading            Science            dupe_count        \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nThe mutate() function in the dplyr package and the fct_relevel() function in the forcats package are then used to set the order for ordinal variables.\n\nstu_SG_rcd = stu_SG_rcd %&gt;%\n    mutate(Books = fct_relevel(Books,\n                               \"0\",\n                               \"1 - 10\",\n                               \"11 - 25\",\n                               \"26 - 100\",\n                               \"101 - 200\",\n                               \"201-500\",\n                               \"&gt;500\"),\n           Preference_Math = fct_relevel(Preference_Math,\n                                         \"Strongly Disagree\",\n                                         \"Disagree\",\n                                         \"Agree\",\n                                         \"Strongly Agree\"),\n           Preference_Reading = fct_relevel(Preference_Math,\n                                         \"Strongly Disagree\",\n                                         \"Disagree\",\n                                         \"Agree\",\n                                         \"Strongly Agree\"),\n           Preference_Science = fct_relevel(Preference_Math,\n                                         \"Strongly Disagree\",\n                                         \"Disagree\",\n                                         \"Agree\",\n                                         \"Strongly Agree\"),\n           Loneliness = fct_relevel(Loneliness,\n                                    \"Strongly Agree\",\n                                    \"Agree\",\n                                    \"Disagree\",\n                                    \"Strongly Disagree\"),\n           ClassroomSafety = fct_relevel(ClassroomSafety,\n                                    \"Strongly Disagree\",\n                                    \"Disagree\",\n                                    \"Agree\",\n                                    \"Strongly Agree\"),\n           Exercise = fct_relevel(Exercise,\n                                  \"0\",\n                                  \"1\", \n                                  \"2\",\n                                  \"3\",\n                                  \"4\",\n                                  \"5\",\n                                  \"6\",\n                                  \"7\",\n                                  \"8\",\n                                  \"9\",\n                                  \"10\"),\n           FamilyCommitment = fct_relevel(FamilyCommitment,\n                                  \"0\",\n                                  \"1\", \n                                  \"2\",\n                                  \"3\",\n                                  \"4\",\n                                  \"5\",\n                                  \"6\",\n                                  \"7\",\n                                  \"8\",\n                                  \"9\",\n                                  \"10\"),\n           Homework_Math = fct_relevel(Homework_Math, \n                                       \"≤ 0.5hr\",\n                                       \"0.5hr - 1hr\",\n                                       \"1hr - 2hr\",\n                                       \"2hr - 3hr\",\n                                       \"3 - 4 hr\",\n                                       \"&gt; 4hr\"),\n           Homework_Reading = fct_relevel(Homework_Reading, \n                                       \"≤ 0.5hr\",\n                                       \"0.5hr - 1hr\",\n                                       \"1hr - 2hr\",\n                                       \"2hr - 3hr\",\n                                       \"3 - 4 hr\",\n                                       \"&gt; 4hr\"),\n           Homework_Science = fct_relevel(Homework_Reading, \n                                       \"≤ 0.5hr\",\n                                       \"0.5hr - 1hr\",\n                                       \"1hr - 2hr\",\n                                       \"2hr - 3hr\",\n                                       \"3 - 4 hr\",\n                                       \"&gt; 4hr\"),\n           Immigration = fct_relevel(Immigration,\n                                     \"Native\",\n                                     \"2nd Generation\",\n                                     \"3rd Generation\"),\n           ParentsEducation = fct_relevel(ParentsEducation,\n                                          \"Pre-Primary\",\n                                          \"Primary\", \n                                          \"Secondary\",\n                                          \"Post-Secondary\",\n                                          \"Tertiary\"),\n           Vehicle = fct_relevel(Vehicle,\n                                 \"0\",\n                                 \"1\",\n                                 \"2\",\n                                 \"≥3\"),\n           Sibling = fct_relevel(Sibling,\n                                 \"0\",\n                                 \"1\",\n                                 \"2\",\n                                 \"≥3\"),\n           TeacherSupport = fct_relevel(TeacherSupport,\n                                        \"Never or almost never\",\n                                        \"Some lessons\",\n                                        \"Most lesson\",\n                                        \"Every lesson\"))\n\nstu_SG_rcd$SchoolID = as.character(stu_SG_rcd$SchoolID)\n\nThe tibble data frame, stu_SG_rcd, is then saved in the rds file format and imported into the R environment as cdata.\n\nwrite_rds(stu_SG_rcd, \"data/stu_SG_rcd.rds\")\n\n\ncluster_data = read_rds(\"data/stu_SG_rcd.rds\")\n\n\n\n\n\n\n\n\n\n\n\n\nUnder the Cluster Analysis sub-module, users will be able to conduct cluster analysis using either hierarchical or k-means clustering approaches. Users may build two cluster models and compare them against each other based on a variety of validation criteria (e.g., Hubert’s gamme coefficient, Dunn index, corrected rand index).\nMore specifically, we will utilise the following graphs:\n\nHeatmap: When the user chooses the hierarchical clustering approach, a heatmap is generated to allow the user to visualise the patterns of similarity or dissimilarity between subject scores and the various variables based on the colours gradients displayed. The user can then pick up underlying patterns and associations.\nScatter Plot: When the user chooses the k-means (partitioning) clustering approach, a scatter plot is created to show the different clusters based on subject scores and variables (e.g., high-high, high-low, low-high, low-low quadrants), with clear demarcation by colours between each cluster. This provides an intuitive bird’s-eye view of the clustering outcomes. [Note: The scatter plot will not be utilised afterall given the large number of categorical variables in the dataset.]\n\n\n\n\n\n\n\n\n\n\n\nCluster heatmap is a powerful tool for exploring multivariate data. They are valuable when used appropriately in conjunction with domain expertise and an understanding of the underlying data characteristics.\n\nHelp in identifying patterns and structures within complex datasets.\nClusters and dendrograms provide insights into similarities and differences among data points.\nEnable simultaneous exploration of multiple variables, allowing users to grasp relationships within the data comprehensively.\nUsers can customise clustering parameters, data transformation methods, and other aspects to tailor the visualisation to their needs.\nInteractive cluster heatmaps allow users to explore the data dynamically, adjusting parameters and observing the immediate impact on the visualisation.\nUseful for condensing large datasets into a manageable visual representation, highlighting key features.\n\nHowever, some words of caution would need to be taken into consideration as well:\n\nInterpretation can be complex, especially for large datasets, and may require domain knowledge to make sense of the patterns.\nChoice of clustering algorithms and parameters introduces subjectivity. Different methods may yield different results.\nGenerating cluster heatmaps for large datasets can be computationally intensive, affecting performance and responsiveness.\n\nThe R packages used for plotting interactive cluster heatmaps are heatmaply and dendextend.\n\npacman::p_load(heatmaply, dendextend)\n\n\n\n\nAs interactive cluster heatmaps cannot be easily generated for large datasets, a representative sample is taken for the purpose of data visualisation.\nThe na.omit() function in the stats package is used to omit the rows with missing values in any of the variables. Then, the dataset is sampled by “SchoolID” using the slice_sample() function in the dplyr package, with the proportion of 10%. This would provide a data subset that allows for easier data visualisation.\n\n# User Input: seed value\nset.seed(123)\n\n# User Choice: group_by value\n# User Input: prop value\nheatmap_data = cluster_data %&gt;%\n  na.omit() %&gt;%\n  group_by(SchoolID)  %&gt;%\n  slice_sample(prop = 0.1, replace = FALSE) %&gt;%\n  ungroup()\n\n\n\n\nBefore plotting the heatmap, the tibble data frame, heatmap_data, is converted to a data matrix.\n\nheatmap_data = data.matrix(heatmap_data)\n\n\n\n\nWhen analysing a multivariate dataset, it is very common that the variables include values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation is commonly used before clustering. The three main data transformation methods are supported by the heatmaply() function are: scale, normalise, and percentise.\n\n\nWhen all variables are (or assumed to be) from some normal distribution, then scaling (i.e., subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution. In such a case, each value would reflect the distance from the mean in units of standard deviation. The “scale” argument in the heatmaply() function supports column and row scaling.\nAn interactive scaled basic cluster heatmap is then plotted using the heatmaply() function in the heatmaply package. The plot below uses the “scale” argument to scale column-wise.\n\n# User Choice: scale method\nheatmaply(heatmap_data,\n          scale = \"column\")\n\n\n\n\n\n\n\n\nWhen variables in the dataset comes from possibly different (and non-normal) distributions, the normalise function can be used to bring the values to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations. This preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”. Different from scaling, the normalise method is performed on the input dataset.\nAn interactive normalised basic cluster heatmap is then plotted using the heatmaply() function in the heatmaply package.\n\nheatmaply(normalize(heatmap_data))\n\n\n\n\n\n\n\n\nThis is similar to ranking the variables, but instead of keeping the rank values, they are divided by the maximal rank. This is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile. The benefit of the percentise method is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it. Similar to the normalise method, the percentise method is also performed on the input dataset.\nAn interactive percentised basic cluster heatmap is then plotted using the heatmaply() function in the heatmaply package.\n\nheatmaply(percentize(heatmap_data))\n\n\n\n\n\n\n\n\n\nThe heatmaply package supports a variety of hierarchical clustering algorithm. The main arguments are:\n\n“dist_method” whose default is NULL (which results in “euclidean” to be used). Can accept alternative character strings indicating the method to be passed to distfun. By default distfun is dist hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\n“hclust_method” whose default is NULL (which results in “complete” to be used). Can accept alternative character strings indicating the method to be passed to hclustfun By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC). Specifying hclust_method=NA causes heatmaply to use find_dend() to find the “optimal” dendrogram for the data.\n“k_row” is an integer scalar with the desired number of groups by which to color the dendrogram’s branches in the rows (uses color_branches()) If NA then find_k() is used to deduce the optimal number of clusters.\n\n\n# User Choice: dist_method\nheatmaply(normalize(heatmap_data),\n          dist_method = \"euclidean\",\n          hclust_method = NA,\n          k_row = NA)\n\n\n\n\n\nFor ease of use, the cluster heatmap will be designed to allow the user to calibrate the cluster heatmap statistically, with the relevant data table on the best clustering method and line chart on the number of clusters displayed as supplementary visualisations alongside the cluster heatmap to allow the user to experiment. In order to determine the best clustering method and number of clusters, the dend_expend() and find_k() functions in the dendextend package is used.\n\n# Supplementary Visualisation 1 - Best Clustering Method:\n# User Choice: dist()'s method\ndend_expend(dist(normalize(heatmap_data),\n          method = \"minkowski\"))[[3]] %&gt;% \n  select(2:3)\n\n  hclust_methods     optim\n1         ward.D 0.3716380\n2        ward.D2 0.3873639\n3         single 0.5676544\n4       complete 0.4216517\n5        average 0.6141891\n6       mcquitty 0.4155008\n7         median 0.4392578\n8       centroid 0.5498881\n\n# Supplementary Visualisation 2 - Optimal No. of Clusters:\n# User Choice: dist()'s method and hclust()'s method\nplot(find_k(hclust(dist(normalize(heatmap_data),\n          method = \"euclidean\"), \n          method = \"average\")))\n\n\n\n\n\n\n\nOne of the problems with hierarchical clustering is that it does not actually place the rows in a definite order; it merely constrains the space of possible orderings. The heatmaply() function uses the “seriate” argument to find an optimal ordering of rows and columns. It is of a character indicating the method of matrix sorting (default: “OLO”). Implemented options include:\n\n“OLO”: Optimal leaf ordering, optimizes the Hamiltonian path length that is restricted by the dendrogram structure - works in O(n^4);\n“mean”: sorts the matrix based on the reorderfun using marginal means of the matrix. This is the default used by heatmap.2;\n“none”: the default order produced by the dendrogram; and\n“GW”: Gruvaeus and Wainer heuristic to optimize the Hamiltonian path length that is restricted by the dendrogram structure.\n\n\n\nThe seriation algorithm of Optimal Leaf Ordering (OLO) is used in the plot. This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimise the sum of dissimilarities between adjacent leaves.\n\nheatmaply(normalize(heatmap_data),           \n          seriate = \"OLO\")\n\n\n\n\n\n\n\n\nThe default option is “OLO” (Optimal leaf ordering) which optimises the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(heatmap_data),\n          seriate = \"GW\")\n\n\n\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as heatmap.2.\n\nheatmaply(normalize(heatmap_data),\n          seriate = \"mean\")\n\n\n\n\n\n\n\n\nThe option “none” gives us dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(heatmap_data),\n          seriate = \"none\")\n\n\n\n\n\n\n\n\n\nTo summarise, the relevant codes would be used to generate the cluster heatmap based on the user’s choices for three main categories of decisions.\n\n# Step 1: Choice of Data Sampling Approach\n# - User Input: seed value\n# set.seed(&lt;value&gt;)\n# \n# - User Choice: group_by value\n# - User Input: prop value\n# heatmap_data = cluster_data %&gt;%\n#   na.omit() %&gt;%\n#   group_by(&lt;variable name&gt;)  %&gt;%\n#   slice_sample(prop = &lt;value&gt;, replace = &lt;TRUE/FALSE&gt;) %&gt;%\n#   ungroup()\n#   \n# Step 2: Choice of Data Transformation Method\n# - If Choice is Scale method: \n# heatmaply(heatmap_data,           \n#           scale = \"&lt;column/row&gt;\")  \n# \n# - If Choice is Normalise method: \n# heatmaply(normalize(heatmap_data),           \n#           scale = \"none\")  \n# \n# - If Choice is Percentise method: \n# heatmaply(percentize(heatmap_data),           \n#           scale = \"none\")\n# \n# Step 3: Choice of Clustering Algorithm\n# - User Choice: dist_method\n# heatmaply(normalize(heatmap_data),\n#           dist_method = \"&lt;euclidean/maximum/manhattan/canberra/binary/minkowski&gt;\",\n#           hclust_method = NA,\n#           k_row = NA)\n# \n# Supplementary Visualisation - Best Clustering Method:\n# - User Choice: dist() method\n# dend_expend(dist(&lt;normalize/percentize&gt;(heatmap_data),\n#           &lt;scale = \"&lt;column/row&gt;\",&gt;\n#           method = \"&lt;euclidean/maximum/manhattan/canberra/binary/minkowski&gt;\"))[[3]] %&gt;% \n#   select(2:3)\n# \n# Supplementary Visualisation - Optimal No. of Clusters:\n# - User Choice: dist() method and hclust() method\n# plot(find_k(hclust(dist(&lt;normalize/percentize&gt;(heatmap_data),\n#           &lt;scale = \"&lt;column/row&gt;\",&gt;\n#           method = \"&lt;euclidean/maximum/manhattan/canberra/binary/minkowski&gt;\"), \n#           method = \"&lt;complete/ward.D/ward.D2/single/average/mcquitty/median/centroid&gt;\")))\n# \n# Step 4: Choice of Seriation Algorithm\n# - User Choice: seriate\n# heatmaply(&lt;normalize/percentize&gt;(heatmap_data),\n#           &lt;scale = \"&lt;column/row&gt;\",&gt;\n#           seriate = \"&lt;OLO/mean/GW/none&gt;\",\n#           k_row = NA)\n\n\n\n\n\n\n\nParallel plot is an effective tool for exploring relationships among multiple variables simultaneously, providing a holistic view of the data.\n\nFacilitate the identification of patterns, trends, and anomalies in the dataset by visually connecting related variables.\nSuitable for comparing patterns between different groups or categories within the dataset, aiding in group-wise analysis.\nComplement clustering analyses by visually representing the similarities and differences among clusters.\nOutliers can be easily identified as deviations from the typical pattern along the parallel axes.\nThe relative importance of variables can be assessed based on their impact on the overall pattern in the parallel plot.\nUsers can customise parallel plots by selecting specific variables, adjusting color schemes, and modifying the appearance for better interpretability.\nInteractive parallel plots allow users to dynamically explore the data, emphasising certain variables or groups for deeper investigation.\n\nHowever, some words of caution would need to be taken into consideration as well:\n\nHandling high-dimensional data can be challenging as the complexity of the plot increases with the number of variables.\nWith many variables, lines on the plot may overlap, making it difficult to discern individual patterns and affecting interpretability.\nThe way variables are connected can introduce subjectivity, and different connections may lead to different interpretations.\nParallel plots are most suitable for numerical data, and their effectiveness may diminish with categorical or ordinal variables. [Note: However, parallel plot is still a better choice than Sankey diagram in dealing with a mix of continuous and categorical variables within a dataset.]\n\nThe R package used for plotting interactive parallel plots is parallelPlot.\n\npacman::p_load(parallelPlot)\n\n\n\n\nAs interactive parallel plots cannot be easily generated for large datasets, a representative sample is taken for the purpose of data visualisation.\nThe na.omit() function in the stats package is used to omit the rows with missing values in any of the variables. Then, the dataset is sampled by “SchoolID” using the slice_sample() function in the dplyr package, with the proportion of 10%. This would provide a data subset that allows for easier data visualisation. The “SchoolID” variable is then dropped since there are too many IDs, which may not yield useful insights in parallel plots.\n\n# User Input: seed value \nset.seed(123)  \n\n# User Choice: group_by value \n# User Input: prop value \nparallelplot_data = cluster_data %&gt;%   \n  na.omit() %&gt;%   \n  group_by(SchoolID)  %&gt;%   \n  slice_sample(prop = 0.1, replace = FALSE) %&gt;%   \n  ungroup() %&gt;%\n  select(-c(\"SchoolID\"))\n\n\n\n\nThe list of categories for the categorical variables are first prepared.\n\n# Preparing List of Categories for Each Categorical Variable in Specific Order\nLoneliness = list(\"Strongly Agree\",\n                  \"Agree\",\n                  \"Disagree\",\n                  \"Strongly Disagree\")\n\nClassroomSafety = list(\"Strongly Disagree\",\n                \"Disagree\",\n                \"Agree\",\n                \"Strongly Agree\")\n\nPreference_Math = list(\"Strongly Disagree\",\n                \"Disagree\",\n                \"Agree\",\n                \"Strongly Agree\")\n\nPreference_Reading = list(\"Strongly Disagree\",\n                \"Disagree\",\n                \"Agree\",\n                \"Strongly Agree\")\n\nPreference_Science = list(\"Strongly Disagree\",\n                \"Disagree\",\n                \"Agree\",\n                \"Strongly Agree\")\n\nBooks = list(\"0\",\n             \"1 - 10\",\n             \"11 - 25\",\n             \"26 - 100\",\n             \"101 - 200\",\n             \"201-500\",\n             \"&gt;500\")\n\nExercise = list(\"0\",\n                \"1\", \n                \"2\",\n                \"3\",\n                \"4\",\n                \"5\",\n                \"6\",\n                \"7\",\n                \"8\",\n                \"9\",\n                \"10\")\n\nFamilyCommitment = list(\"0\",\n                \"1\", \n                \"2\",\n                \"3\",\n                \"4\",\n                \"5\",\n                \"6\",\n                \"7\",\n                \"8\",\n                \"9\",\n                \"10\")\n\nHomework_Math = list(\"≤ 0.5hr\",\n             \"0.5hr - 1hr\",\n             \"1hr - 2hr\",\n             \"2hr - 3hr\",\n             \"3 - 4 hr\",\n             \"&gt; 4hr\")\n\nHomework_Reading = list(\"≤ 0.5hr\",\n             \"0.5hr - 1hr\",\n             \"1hr - 2hr\",\n             \"2hr - 3hr\",\n             \"3 - 4 hr\",\n             \"&gt; 4hr\")\n\nHomework_Science = list(\"≤ 0.5hr\",\n             \"0.5hr - 1hr\",\n             \"1hr - 2hr\",\n             \"2hr - 3hr\",\n             \"3 - 4 hr\",\n             \"&gt; 4hr\")\n\nImmigration = list(\"Native\",\n                   \"2nd Generation\",\n                   \"3rd Generation\")\n\nParentsEducation = list(\"Pre-Primary\",\n                        \"Primary\", \n                        \"Secondary\",\n                        \"Post-Secondary\",\n                        \"Tertiary\")\n\nSibling = list(\"0\",\n             \"1\",\n             \"2\",\n             \"≥3\")\n\nVehicle = list(\"0\",\n             \"1\",\n             \"2\",\n             \"≥3\")\n\nTeacherSupport = list(\"Never or almost never\",\n                      \"Some lessons\",\n                      \"Most lesson\",\n                      \"Every lesson\")\n\nHomeLanguage = list(\"English\",\n                    \"Others\")\n\nSchoolType = list(\"Public\",\n                  \"Private\")\n\nAircon = list(\"No\", \"Yes\")\n\nHelper = list(\"No\", \"Yes\")\n\nGender = list(\"Female\",\n              \"Male\")\n\nThe parallelPlot() function is then used to plot an interactive parallel plot.\n\n# Full Set of Variables & Lists of Categories\n# parallelPlot(parallelplot_data[,c(1:25)],\n#              categorical = list(\n#                Loneliness,\n#                ClassroomSafety,\n#                TeacherSupport,\n#                Gender,\n#                Homework_Math,\n#                Homework_Reading,\n#                Homework_Science,\n#                SchoolType,\n#                ParentsEducation,\n#                Immigration,\n#                HomeLanguage,\n#                Sibling,\n#                Aircon,\n#                Helper,\n#                Vehicle,\n#                Books,\n#                Exercise,\n#                OwnRoom,\n#                FamilyCommitment,\n#                Preference_Math,\n#                Preference_Reading,\n#                Preference_Science,\n#                NULL,\n#                NULL,\n#                NULL))\n\n\n# User Input: Chosen Columns (Recommended: Up to 8)\nparallelPlot(parallelplot_data[,c(4, 5, 8, 9, 10, 17, 20, 23)],\n             categorical = list(\n               Gender,\n               Homework_Math,\n               SchoolType,\n               ParentsEducation,\n               Immigration,\n               Exercise,\n               Preference_Math,\n               NULL))\n\n\n\n\n\n\n\n\nThe aesthetics of the parallel plot is then added. This allows the user to customise the plot’s colours.\nAlso, the “rotateTitle” argument is set to “TRUE” for readability.\n\n# User Input: Chosen Columns (Recommended: Up to 8)\n# User Input: Name of Reference Column (to determine colour to attribute to a row)\n# User Input: Colour Scale for Continuous Data\n# User Choice: Colour Scale for Categorical Data\nparallelPlot(parallelplot_data[,c(4, 5, 8, 9, 10, 17, 20, 23)],\n             categorical = list(\n               Gender,\n               Homework_Math,\n               SchoolType,\n               ParentsEducation,\n               Immigration,\n               Exercise,\n               Preference_Math,\n               NULL),\n             rotateTitle = TRUE,\n             refColumnDim = \"Gender\",\n             continuousCS = \"Blues\",\n             categoricalCS = \"Accent\")\n\n\n\n\n\n\n\n\n\n# - User Input: Chosen Columns (Recommended: Up to 8)\n# parallelPlot(parallelplot_data[,c(&lt;chosen columns&gt;)],\n#              categorical = list(&lt;of lists of categories for each chosen column&gt;))\n# \n# - User Input: Name of Reference Column (to determine colour to attribute to a row)\n# - User Input: Colour Scale for Continuous Data\n# - User Choice: Colour Scale for Categorical Data\n# parallelPlot(parallelplot_data[,c(&lt;chosen columns&gt;)],\n#              categorical = list(&lt;of lists of categories for each chosen column&gt;),\n#              rotateTitle = TRUE,\n#              refColumnDim = \"&lt;variable name&gt;\",\n#              continuousCS = \"&lt;Viridis/Inferno/Magma/Plasma/Warm/Cool/Rainbow/CubehelixDefault/Blues/Greens/Greys/Oranges/Purples/Reds/BuGn/BuPu/GnBu/OrRd/PuBuGn/PuBu/PuRd/RdBu/RdPu/YlGnBu/YlGn/YlOrBr/YlOrRd&gt;\",\n#              categoricalCS = \"&lt;Category10/Accent/Dark2/Paired/Set1&gt;\")\n\n\n\n\n\nAfter working through the details of the two types of plots (cluster heatmap and parallel plot) in the preceeding sections, the storyboard approach is now used to prototype the Cluster Analysis portion of the Shiny application. The two storyboards below (one for each type of plot) are used to illustrate the different components of the UI for the proposed design.\n\n\nTo summarise, the input/choices that the user is required to make are:\n\nData Sampling Approach: seed value, group_by() value, slice_sample() prop value and replace/do not replace.\nData Transformation Method: scale by column/row, normalise, or percentise.\nClustering Algorithm:\n\ndist_method in heatmaply: euclidean, maximum, manhattan, canberra, binary, or minkowski.\ndist in dend_expend: euclidean, maximum, manhattan, canberra, binary, or minkowski.\nhclust in find_k: complete, ward.D, ward.D2, single, average, mcquitty, median, or centroid.\nseriation algorithm in heatmaply: OLO, mean, GW, or none.\n\n\nAs mentioned, the supplementary data table and visualisation will help the user who may not be very familiar with cluster heatmap to choose the suitable clustering method and number of clusters.\n\n\n\nStoryboard 1: Cluster Heatmap\n\n\n\n\n\nStoryboard 2: Annotated Cluster Heatmap\n\n\n\n# Step 1: Choice of Data Sampling Approach\n# - User Input: seed value\n# set.seed(&lt;value&gt;)\n# \n# - User Choice: group_by value\n# - User Input: prop value\n# heatmap_data = cluster_data %&gt;%\n#   na.omit() %&gt;%\n#   group_by(&lt;variable name&gt;)  %&gt;%\n#   slice_sample(prop = &lt;value&gt;, replace = &lt;TRUE/FALSE&gt;) %&gt;%\n#   ungroup()\n#   \n# Step 2: Choice of Data Transformation Method\n# - If Choice is Scale method: \n# heatmaply(heatmap_data,           \n#           scale = \"&lt;column/row&gt;\")  \n# \n# - If Choice is Normalise method: \n# heatmaply(normalize(heatmap_data),           \n#           scale = \"none\")  \n# \n# - If Choice is Percentise method: \n# heatmaply(percentize(heatmap_data),           \n#           scale = \"none\")\n# \n# Step 3: Choice of Clustering Algorithm\n# - User Choice: dist_method\n# heatmaply(normalize(heatmap_data),\n#           dist_method = \"&lt;euclidean/maximum/manhattan/canberra/binary/minkowski&gt;\",\n#           hclust_method = NA,\n#           k_row = NA)\n# \n# Supplementary Visualisation - Best Clustering Method:\n# - User Choice: dist() method\n# dend_expend(dist(&lt;normalize/percentize&gt;(heatmap_data),\n#           &lt;scale = \"&lt;column/row&gt;\",&gt;\n#           method = \"&lt;euclidean/maximum/manhattan/canberra/binary/minkowski&gt;\"))[[3]] %&gt;% \n#   select(2:3)\n# \n# Supplementary Visualisation - Optimal No. of Clusters:\n# - User Choice: dist() method and hclust() method\n# plot(find_k(hclust(dist(&lt;normalize/percentize&gt;(heatmap_data),\n#           &lt;scale = \"&lt;column/row&gt;\",&gt;\n#           method = \"&lt;euclidean/maximum/manhattan/canberra/binary/minkowski&gt;\"), \n#           method = \"&lt;complete/ward.D/ward.D2/single/average/mcquitty/median/centroid&gt;\")))\n# \n# Step 4: Choice of Seriation Algorithm\n# - User Choice: seriate\n# heatmaply(&lt;normalize/percentize&gt;(heatmap_data),\n#           &lt;scale = \"&lt;column/row&gt;\",&gt;\n#           seriate = \"&lt;OLO/mean/GW/none&gt;\")\n\n\n\n\nTo summarise, the input/choices that the user is required to make are:\n\nChosen Variables (columns).\n\nThe list of lists of categories in each categorical variable chosen have been pre-ordered for the dataset for ease of use.\n\nReference Column: to determine colour to attribute to a row.\nColour Scale for Continuous Data: Viridis, Inferno, Magma, Plasma, Warm, Cool, Rainbow, CubehelixDefault, Blues, Greens, Greys, Oranges, Purples, Reds, BuGn, BuPu, GnBu, OrRd, PuBuGn, PuBu, PuRd, RdBu, RdPu, YlGnBu, YlGn, YlOrBr, or YlOrRd.\nColour Scale for Categorical Data: Category10, Accent, Dark2, Paired, or Set1.\n\n\n\n\nStoryboard 3: Parallel Plot\n\n\n\n\n\nStoryboard 4: Annotated Parallel Plot\n\n\n\n# - User Input: Chosen Columns (Recommended: Up to 8)\n# parallelPlot(parallelplot_data[,c(&lt;chosen columns&gt;)],\n#              categorical = list(&lt;of lists of categories for each chosen column&gt;))\n# \n# - User Input: Name of Reference Column (to determine colour to attribute to a row)\n# - User Input: Colour Scale for Continuous Data\n# - User Choice: Colour Scale for Categorical Data\n# parallelPlot(parallelplot_data[,c(&lt;chosen columns&gt;)],\n#              categorical = list(&lt;of lists of categories for each chosen column&gt;),\n#              rotateTitle = TRUE,\n#              refColumnDim = \"&lt;variable name&gt;\",\n#              continuousCS = \"&lt;Viridis/Inferno/Magma/Plasma/Warm/Cool/Rainbow/CubehelixDefault/Blues/Greens/Greys/Oranges/Purples/Reds/BuGn/BuPu/GnBu/OrRd/PuBuGn/PuBu/PuRd/RdBu/RdPu/YlGnBu/YlGn/YlOrBr/YlOrRd&gt;\",\n#              categoricalCS = \"&lt;Category10/Accent/Dark2/Paired/Set1&gt;\")\n\n\n\n\n\nIn conclusion, the Shiny application powerful tool for exploratory and confirmatory data analysis through visualisation. The PISA performance dataset is interesting and contains useful data for studying Singapore students’ academic performance in the three subjects at age 15. The prototyping of the Shiny application for the cluster analysis of the PISA dataset in this take-home exercise has provided an invaluable opportunity to apply various skills learnt in this module for data wrangling, data visualisation, and design. Our team hopes that the final product would provide a useful interface for users to explore the wonderful wealth of information within the PISA dataset, to better understand how various factors including gender, type of school, and socioeconomic status affect academic performance in Singapore.\n\n\n\n\nR for Visual Analytics.\nR for Data Science.\nFundamentals of Data Visualisation.\nStoryboard for Shiny application.\n\n~~~ End of Take-home Exercise 4 ~~~"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#introduction",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#introduction",
    "title": "Take-home Exercise 4",
    "section": "",
    "text": "Singapore has come a long way since gaining independence in 1965. Without natural resources, human capital development through a well-planned, world-class education system was a critical part of Singapore’s transformation from third world to first. Despite the internationally lauded success, Singapore’s education system is far from perfect – there still exists gaps in education achievement among students. According to the latest OECD’s Programme for International Student Assessment (PISA) 2022, which measures 15-year-olds’ ability to use their reading, mathematics, and science knowledge and skills to meet real-life challenges, socioeconomic status accounted for 17% of the variation in mathematics performance in Singapore (compared to 15% on average across OECD countries). Clearly, Singapore’s success does not translate to success for every student. Why then do some students outperform others? And is socioeconomic status the only factor for success?\nOur team believes that knowledge is power. While causality cannot and should not be easily drawn between the various forces of influence and academic performance, a more detailed and nuanced understanding of these factors would highlight potential areas to focus on when engaging parents and students as well as when developing education and socioeconomic policies for a more inclusive and equitable society.\n\n\n\nThe PISA is treated as a platform for geopolitical competition, as shown by an overemphasis on the comparison of PISA scores between different countries. Scant attention is paid to the extensive data collected through the student, parent, and school questionnaires that are administered alongside the tests, which may reveal more a nuanced understanding of the individual lived experiences of students.\nResearch studies on the gaps in education attainment in Singapore tends to focus on traditional factors such as gender, race, and socioeconomic status. While these are important structural factors, they should be complemented by individual-level factors, such as psychological wellness, and social relations. Gen Alpha children are said to have reduced attention span, and less time for socialising. These could pose challenges in their education journeys, and deserve closer attention.\nTechnical knowledge is required to perform accurate and reliable data wrangling and analysis of the survey outcomes from PISA. Such valuable information should be made available to the public in an accessible and interactive manner for self-discovery.\n\n\n\nThe team will extract, analyse, and visualise the relationships between different factors and the PISA scores of Singapore students. Using various R packages, an interactive R shiny application with visual analytics techniques will be presented to enable users to interact with the data based on their personal interests and circumstances, and draw their own insights and conclusions regarding the state of learning in Singapore.\nUsers will be able to:\n\nCompare the PISA scores between different groups of students based on factors such as gender, socioeconomic status (e.g., employment status of parents, immigrant status), and psychological well-being (e.g., feeling of loneliness, resilience when faced with challenges).\nGain an understanding of the varying levels of importance and statistical significance of the various influences on PISA scores for different clusters of students.\n\nOverall, the research hopes to generate new knowledge on disparities in the everyday stress experience and psychological well-being between lower and higher SES students, and their spillover effects on academic performance and persistence. The broader implication of the study is that educational policies must actively address psychological challenges alongside resource challenges to better mitigate socioeconomic gaps in educational attainment.\n\n\n\nIn this take-home exercise, the objectives are to:\n\nEvaluate and determine if the necessary R packages needed for the team’s Shiny application are supported in R CRAN;\nPrepare and test the specific R codes to return the expected visualisation outputs;\nDetermine the parameters and outputs that will be provided in the Shiny application; and\nSelect the appropriate Shiny user interface (UI) components for displaying the parameters determined.\n\nIn this take-home exercise, I focus on prototyping the Cluster Analysis portion of the Shiny application, while my teammates would work on the Univariate & Bivariate Analysis, and Multivariate Analysis portions."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#getting-started",
    "title": "Take-home Exercise 4",
    "section": "",
    "text": "The R packages used for data wrangling are:\n\nhaven for importing SAS files;\ntidyverse (i.e. readr, tidyr, dplyr, ggplot2) for performing data science tasks such as importing, tidying, and wrangling data, as well as creating graphics based on The Grammar of Graphics;\njanitor for data exploration and cleaning;\nreshape2 for transforming data between wide and long formats; and\nDT and kableExtra for dynamic report generation.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed in the computer. If yes, they are then loaded into the R environment. If no, they are installed, and then loaded into the R environment.\n\npacman::p_load(haven, tidyverse, \n               janitor, reshape2,\n               DT, kableExtra)\n\n\n\n\nThe OECD Programme for International Student Assessment (PISA) measures how well 15-year-old students in different countries are “prepared to meet the challenges of today’s knowledge societies” by looking at “their ability to use their knowledge and skills to meet real-life challenges”. The PISA surveys take place very three years, the latest being conducted in 2022.\nThe PISA 2022 database contains the full set of responses from individual students, school principals, and parents. There are a total of five data files and their contents are as follows:\n\nStudent questionnaire data file;\nSchool questionnaire data file;\nTeacher questionnaire data file;\nCognitive item data file; and\nQuestionnaire timing data file.\n\nFor the purpose of this take-home exercise, the “Student questionnaire data file” is used."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#data-wrangling",
    "title": "Take-home Exercise 4",
    "section": "",
    "text": "The dataset used in this take-home exercise is the 2022 PISA student questionnaire data file, cy08msp_stu_qqq.sas7bdat, which is in the SAS file format.\nThe file is imported into the R environment using the read_sas() function in the haven package and stored as the R object, stu_qqq.\n\nstu_qqq = read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\nThe tibble data frame, stu_qqq, has 1,279 columns (variables) and 613,744 rows (observations).\n\n\n\nThere are 6,606 rows with the country code (i.e., CNT) value of “SGP”, which represents Singapore. This count is cross-verified by the information provided in the “CY08MSP_STU_QQQ” sheet in the codebook. The codebook also stated that Singapore students’ made up 1.0763% of the entire global student population who took part in the 2022 PISA.\nThe filter() function in the dplyr package is used to obtain these rows, and stored as the R object, stu_qqq_SG.\n\nstu_SG = stu_qqq %&gt;% filter(CNT == \"SGP\")\n\nThe tibble data frame, stu_SG, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(stu_SG, \"data/stu_SG.rds\")\n\n\nstu_SG = read_rds(\"data/stu_SG.rds\")\n\n\n\n\nAfter perusing the Codebook and Technical Report, the team narrowed down the questions from the survey that would yield insightful results. The names of the columns are stored in a vector, colname. To filter the raw dataset with the columns, the select() function in the readr package is used to identify all the variables listed out in the colname vector.\n\ncolname = c(\"ST034Q06TA\", \"ST265Q03JA\", \"ST270Q03JA\", \"ST004D01T\", \"ST296Q01JA\", \"ST296Q02JA\", \"ST296Q03JA\", \"STRATUM\", \"CNTSCHID\", \"HISCED\", \"IMMIG\", \"ST022Q01TA\", \"ST230Q01JA\", \"ST250D06JA\", \"ST250D07JA\", \"ST251Q01JA\", \"ST255Q01JA\", \"EXERPRAC\", \"ST250Q01JA\", \"WORKHOME\", \"ST268Q01JA\", \"ST268Q02JA\", \"ST268Q03JA\")\n\nThe relevant columns are then extracted using the following functions:\n\nselect() function to retain the following columns:\n\nVariables identified in colname and\nColumns that starts with “PV” and contains either “MATH”, “SCIE”, or “READ” to extract the plausible values of scores related to the subjects of Mathematics, Science, and Reading. This is performed using a combination of starts_with() and contains().\n\nstarts_with(): Matches the beginning of the column name with “PV”, and\ncontains(): Searches for columns containing three alternative subjects to be matched.\n\n\nmutate() to create 3 new variables to store the mean plausible values for each subject for each row using rowMeans() and across().\n\n\nstu_SG_filtered = stu_SG %&gt;% \n\n  # Retains desired variables\n  select(all_of(colname), starts_with(\"PV\") & contains(c(\"MATH\", \"READ\", \"SCIE\"))) %&gt;% \n\n  # Calculates the mean of plausible values for each subject per student\n  mutate(Math = rowMeans(across(starts_with(\"PV\") & contains(\"MATH\")), na.rm = TRUE),\n         Reading = rowMeans(across(starts_with(\"PV\") & contains(\"READ\")), na.rm = TRUE),\n         Science = rowMeans(across(starts_with(\"PV\") & contains(\"SCIE\")), na.rm = TRUE),\n         ) %&gt;% \n  \n  # Drops Plausible Values columns\n  select(-starts_with(\"PV\"))\n\nThe tibble data frame, stu_SG_filtered, contains 6,606 observations across 26 variables.\n\n\n\nThe columns are then renamed using the rename() function in the dplyr package.\n\nstu_SG_filtered =\n  stu_SG_filtered %&gt;% \n  rename(\n    \"Loneliness\" = \"ST034Q06TA\",\n    \"ClassroomSafety\" = \"ST265Q03JA\",\n    \"TeacherSupport\" = \"ST270Q03JA\",\n    \"Gender\" = \"ST004D01T\",\n    \"Homework_Math\" = \"ST296Q01JA\",\n    \"Homework_Reading\" = \"ST296Q02JA\",\n    \"Homework_Science\" = \"ST296Q03JA\",\n    \"SchoolType\" = \"STRATUM\",\n    \"SchoolID\" = \"CNTSCHID\",\n    \"ParentsEducation\" = \"HISCED\",\n    \"Immigration\" = \"IMMIG\",\n    \"HomeLanguage\" = \"ST022Q01TA\",\n    \"Sibling\" = \"ST230Q01JA\",\n    \"Aircon\" = \"ST250D06JA\",\n    \"Helper\" = \"ST250D07JA\",\n    \"Vehicle\" = \"ST251Q01JA\",\n    \"Books\" = \"ST255Q01JA\",\n    \"Exercise\" = \"EXERPRAC\",\n    \"OwnRoom\" = \"ST250Q01JA\",\n    \"FamilyCommitment\" = \"WORKHOME\",\n    \"Preference_Math\" = \"ST268Q01JA\",\n    \"Preference_Reading\" = \"ST268Q02JA\",\n    \"Preference_Science\" = \"ST268Q03JA\")\n\n\n\n\nThere are some responses which are marked as invalid, or missing, in the “Helper” and “Aircon” variables. These responses are replaced with “NA”.\n\nstu_SG_encode = stu_SG_filtered %&gt;% \n  mutate(Aircon = na_if(Aircon, \"9999999\"),\n         Helper = na_if(Helper, \"9999999\"))\n\n\n\n\nThere are several types of responses for the Student’s Questionnaire. We store all the response levels for each question in separate vectors and subsequently combine to create a global dictionary named dict.\n\nBooks = c('1' = \"0\",\n               '2' = \"1 - 10\",\n               '3' = \"11 - 25\",\n               '4' = \"26 - 100\",\n               '5' = \"101 - 200\",\n               '6' = \"201-500\",\n               '7' = \"&gt;500\")\n\nHomeLanguage = c('1' = \"English\",\n             '2' = \"Others\")\n\n# Likert Scales: Strong Disagree to Strongly Agree\nPreference_Math = c('1' = \"Strongly Disagree\",\n           '2' = \"Disagree\",\n           '3' = \"Agree\",\n           '4' = \"Strongly Agree\")\n\nPreference_Reading = c('1' = \"Strongly Disagree\",\n           '2' = \"Disagree\",\n           '3' = \"Agree\",\n           '4' = \"Strongly Agree\")\n\nPreference_Science = c('1' = \"Strongly Disagree\",\n           '2' = \"Disagree\",\n           '3' = \"Agree\",\n           '4' = \"Strongly Agree\")\n\n# Likert Scales: Strong Agree to Strongly Disagree\nLoneliness = c('1' = \"Strongly Agree\",\n           '2' = \"Agree\",\n           '3' = \"Disagree\",\n           '4' = \"Strongly Disagree\")\n\nClassroomSafety = c('1' = \"Strongly Agree\",\n           '2' = \"Agree\",\n           '3' = \"Disagree\",\n           '4' = \"Strongly Disagree\")\n\n# Binary\nSchoolType = c('SGP01' = \"Public\",\n           'SGP03' = \"Private\")\n\nOwnRoom = c('1' = \"Yes\", \n                '2' = \"No\")\n\nAircon = c('7020001' = \"Yes\",\n            '7020002' = \"No\",\n            .default = NA\n            )\n\nHelper = c('7020001' = \"Yes\",\n            '7020002' = \"No\",\n            .default = NA)\n\n# Frequency responses\nExercise = c('0' = \"0\",\n          '1' = \"1\", \n          '2' = \"2\",\n          '3' = \"3\",\n          '4' = \"4\",\n          '5' = \"5\",\n          '6' = \"6\",\n          '7' = \"7\",\n          '8' = \"8\",\n          '9' = \"9\",\n          '10' = \"10\")\n\nFamilyCommitment = c('0' = \"0\",\n          '1' = \"1\", \n          '2' = \"2\",\n          '3' = \"3\",\n          '4' = \"4\",\n          '5' = \"5\",\n          '6' = \"6\",\n          '7' = \"7\",\n          '8' = \"8\",\n          '9' = \"9\",\n          '10' = \"10\")\n\n# Time Periods\nHomework_Math = c('1' = \"≤ 0.5hr\",\n                '2' = \"0.5hr - 1hr\",\n                '3' = \"1hr - 2hr\",\n                '4' = \"2hr - 3hr\",\n                '5' = \"3 - 4 hr\",\n                '6' = \"&gt; 4hr\")\n\nHomework_Reading = c('1' = \"≤ 0.5hr\",\n                '2' = \"0.5hr - 1hr\",\n                '3' = \"1hr - 2hr\",\n                '4' = \"2hr - 3hr\",\n                '5' = \"3 - 4 hr\",\n                '6' = \"&gt; 4hr\")\n\nHomework_Science = c('1' = \"≤ 0.5hr\",\n                '2' = \"0.5hr - 1hr\",\n                '3' = \"1hr - 2hr\",\n                '4' = \"2hr - 3hr\",\n                '5' = \"3 - 4 hr\",\n                '6' = \"&gt; 4hr\")\n\n# Gender\nGender = c('1' = \"Female\",\n            '2' = \"Male\")\n\n\n# Immigrant Background\nImmigration = c('1' = \"Native\",\n           '2' = \"2nd Generation\",\n           '3' = \"3rd Generation\")\n\n# Education Level\nParentsEducation = c('1'=\"Pre-Primary\",   \n         '2'=\"Primary\", \n         '3'=\"Secondary\",\n         '4'='Secondary',\n         '6'=\"Post-Secondary\",\n         '7'=\"Post-Secondary\",\n         '8'=\"Tertiary\",\n         '9'=\"Tertiary\",\n         '10'=\"Tertiary\")\n\n# Posessions\nVehicle = c('1' = \"0\",\n            '2' = \"1\",\n            '3' = \"2\",\n            '4' = \"≥3\")\n\nSibling = c('1' = \"0\",\n            '2' = \"1\",\n            '3' = \"2\",\n            '4' = \"≥3\")\n\n# Support\nTeacherSupport = c('1' = \"Every lesson\",\n            '2' = \"Most lesson\",\n            '3' = \"Some lessons\",\n            '4' = \"Never or almost never\")\n\n# Global Dictionary\ndict = list(\n  \"Loneliness\" = Loneliness,\n  \"ClassroomSafety\" = ClassroomSafety,\n  \"TeacherSupport\" = TeacherSupport,\n  \"Gender\" = Gender,\n  \"Homework_Math\" = Homework_Math,\n  \"Homework_Reading\" = Homework_Reading,\n  \"Homework_Science\" = Homework_Science,\n  \"SchoolType\" = SchoolType,\n  \"ParentsEducation\" = ParentsEducation,\n  \"Immigration\" = Immigration,\n  \"HomeLanguage\" = HomeLanguage,\n  \"Sibling\" = Sibling,\n  \"Aircon\" = Aircon,\n  \"Helper\" = Helper,\n  \"Vehicle\" = Vehicle,\n  \"Books\" = Books,\n  \"Exercise\" = Exercise,\n  \"OwnRoom\" = OwnRoom,\n  \"FamilyCommitment\" = FamilyCommitment,\n  \"Preference_Math\" = Preference_Math,\n  \"Preference_Reading\" = Preference_Reading,\n  \"Preference_Science\" = Preference_Science)\n\nThe helper function below attempts to recode all of the columns based on the global recode dictionary, dict, using functions from the base R, tidyr, and rlang packages:\n\nThe names(x) function retrieves the column names of the input dataframe\nThe recode() function helps to recode values in the columns using dicts\nThe !!sym(x_nm) function unquotes and evaluates the column name that matches the names of the dictionaries, while the !!!dicts[[x_nm]] function unquotes and splices the global recoding dictionary corresponding to the column name.\n\n\nrcd = function(x) {\n  x_nm = names(x)\n  mutate(x, !! x_nm := recode(!! sym(x_nm), !!! dict[[x_nm]]))\n}\n\nThe lmap_at() function in the purrr package applies the helper function to the column in the data frame where the column name matches the keys of the dictionaries.\n\nstu_SG_rcd = lmap_at(stu_SG_encode, \n        names(dict),\n        rcd)\n\nThe get_dupes() function in the janitor package is used to hunt for duplicate records. The results show that there are no duplicated rows.\n\nget_dupes(stu_SG_rcd)\n\n [1] Loneliness         ClassroomSafety    TeacherSupport     Gender            \n [5] Homework_Math      Homework_Reading   Homework_Science   SchoolType        \n [9] SchoolID           ParentsEducation   Immigration        HomeLanguage      \n[13] Sibling            Aircon             Helper             Vehicle           \n[17] Books              Exercise           OwnRoom            FamilyCommitment  \n[21] Preference_Math    Preference_Reading Preference_Science Math              \n[25] Reading            Science            dupe_count        \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nThe mutate() function in the dplyr package and the fct_relevel() function in the forcats package are then used to set the order for ordinal variables.\n\nstu_SG_rcd = stu_SG_rcd %&gt;%\n    mutate(Books = fct_relevel(Books,\n                               \"0\",\n                               \"1 - 10\",\n                               \"11 - 25\",\n                               \"26 - 100\",\n                               \"101 - 200\",\n                               \"201-500\",\n                               \"&gt;500\"),\n           Preference_Math = fct_relevel(Preference_Math,\n                                         \"Strongly Disagree\",\n                                         \"Disagree\",\n                                         \"Agree\",\n                                         \"Strongly Agree\"),\n           Preference_Reading = fct_relevel(Preference_Math,\n                                         \"Strongly Disagree\",\n                                         \"Disagree\",\n                                         \"Agree\",\n                                         \"Strongly Agree\"),\n           Preference_Science = fct_relevel(Preference_Math,\n                                         \"Strongly Disagree\",\n                                         \"Disagree\",\n                                         \"Agree\",\n                                         \"Strongly Agree\"),\n           Loneliness = fct_relevel(Loneliness,\n                                    \"Strongly Agree\",\n                                    \"Agree\",\n                                    \"Disagree\",\n                                    \"Strongly Disagree\"),\n           ClassroomSafety = fct_relevel(ClassroomSafety,\n                                    \"Strongly Disagree\",\n                                    \"Disagree\",\n                                    \"Agree\",\n                                    \"Strongly Agree\"),\n           Exercise = fct_relevel(Exercise,\n                                  \"0\",\n                                  \"1\", \n                                  \"2\",\n                                  \"3\",\n                                  \"4\",\n                                  \"5\",\n                                  \"6\",\n                                  \"7\",\n                                  \"8\",\n                                  \"9\",\n                                  \"10\"),\n           FamilyCommitment = fct_relevel(FamilyCommitment,\n                                  \"0\",\n                                  \"1\", \n                                  \"2\",\n                                  \"3\",\n                                  \"4\",\n                                  \"5\",\n                                  \"6\",\n                                  \"7\",\n                                  \"8\",\n                                  \"9\",\n                                  \"10\"),\n           Homework_Math = fct_relevel(Homework_Math, \n                                       \"≤ 0.5hr\",\n                                       \"0.5hr - 1hr\",\n                                       \"1hr - 2hr\",\n                                       \"2hr - 3hr\",\n                                       \"3 - 4 hr\",\n                                       \"&gt; 4hr\"),\n           Homework_Reading = fct_relevel(Homework_Reading, \n                                       \"≤ 0.5hr\",\n                                       \"0.5hr - 1hr\",\n                                       \"1hr - 2hr\",\n                                       \"2hr - 3hr\",\n                                       \"3 - 4 hr\",\n                                       \"&gt; 4hr\"),\n           Homework_Science = fct_relevel(Homework_Reading, \n                                       \"≤ 0.5hr\",\n                                       \"0.5hr - 1hr\",\n                                       \"1hr - 2hr\",\n                                       \"2hr - 3hr\",\n                                       \"3 - 4 hr\",\n                                       \"&gt; 4hr\"),\n           Immigration = fct_relevel(Immigration,\n                                     \"Native\",\n                                     \"2nd Generation\",\n                                     \"3rd Generation\"),\n           ParentsEducation = fct_relevel(ParentsEducation,\n                                          \"Pre-Primary\",\n                                          \"Primary\", \n                                          \"Secondary\",\n                                          \"Post-Secondary\",\n                                          \"Tertiary\"),\n           Vehicle = fct_relevel(Vehicle,\n                                 \"0\",\n                                 \"1\",\n                                 \"2\",\n                                 \"≥3\"),\n           Sibling = fct_relevel(Sibling,\n                                 \"0\",\n                                 \"1\",\n                                 \"2\",\n                                 \"≥3\"),\n           TeacherSupport = fct_relevel(TeacherSupport,\n                                        \"Never or almost never\",\n                                        \"Some lessons\",\n                                        \"Most lesson\",\n                                        \"Every lesson\"))\n\nstu_SG_rcd$SchoolID = as.character(stu_SG_rcd$SchoolID)\n\nThe tibble data frame, stu_SG_rcd, is then saved in the rds file format and imported into the R environment as cdata.\n\nwrite_rds(stu_SG_rcd, \"data/stu_SG_rcd.rds\")\n\n\ncluster_data = read_rds(\"data/stu_SG_rcd.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#cluster-analysis-initial-proposal",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#cluster-analysis-initial-proposal",
    "title": "Take-home Exercise 4",
    "section": "",
    "text": "Under the Cluster Analysis sub-module, users will be able to conduct cluster analysis using either hierarchical or k-means clustering approaches. Users may build two cluster models and compare them against each other based on a variety of validation criteria (e.g., Hubert’s gamme coefficient, Dunn index, corrected rand index).\nMore specifically, we will utilise the following graphs:\n\nHeatmap: When the user chooses the hierarchical clustering approach, a heatmap is generated to allow the user to visualise the patterns of similarity or dissimilarity between subject scores and the various variables based on the colours gradients displayed. The user can then pick up underlying patterns and associations.\nScatter Plot: When the user chooses the k-means (partitioning) clustering approach, a scatter plot is created to show the different clusters based on subject scores and variables (e.g., high-high, high-low, low-high, low-low quadrants), with clear demarcation by colours between each cluster. This provides an intuitive bird’s-eye view of the clustering outcomes. [Note: The scatter plot will not be utilised afterall given the large number of categorical variables in the dataset.]"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#cluster-heatmap",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#cluster-heatmap",
    "title": "Take-home Exercise 4",
    "section": "",
    "text": "Cluster heatmap is a powerful tool for exploring multivariate data. They are valuable when used appropriately in conjunction with domain expertise and an understanding of the underlying data characteristics.\n\nHelp in identifying patterns and structures within complex datasets.\nClusters and dendrograms provide insights into similarities and differences among data points.\nEnable simultaneous exploration of multiple variables, allowing users to grasp relationships within the data comprehensively.\nUsers can customise clustering parameters, data transformation methods, and other aspects to tailor the visualisation to their needs.\nInteractive cluster heatmaps allow users to explore the data dynamically, adjusting parameters and observing the immediate impact on the visualisation.\nUseful for condensing large datasets into a manageable visual representation, highlighting key features.\n\nHowever, some words of caution would need to be taken into consideration as well:\n\nInterpretation can be complex, especially for large datasets, and may require domain knowledge to make sense of the patterns.\nChoice of clustering algorithms and parameters introduces subjectivity. Different methods may yield different results.\nGenerating cluster heatmaps for large datasets can be computationally intensive, affecting performance and responsiveness.\n\nThe R packages used for plotting interactive cluster heatmaps are heatmaply and dendextend.\n\npacman::p_load(heatmaply, dendextend)\n\n\n\n\nAs interactive cluster heatmaps cannot be easily generated for large datasets, a representative sample is taken for the purpose of data visualisation.\nThe na.omit() function in the stats package is used to omit the rows with missing values in any of the variables. Then, the dataset is sampled by “SchoolID” using the slice_sample() function in the dplyr package, with the proportion of 10%. This would provide a data subset that allows for easier data visualisation.\n\n# User Input: seed value\nset.seed(123)\n\n# User Choice: group_by value\n# User Input: prop value\nheatmap_data = cluster_data %&gt;%\n  na.omit() %&gt;%\n  group_by(SchoolID)  %&gt;%\n  slice_sample(prop = 0.1, replace = FALSE) %&gt;%\n  ungroup()\n\n\n\n\nBefore plotting the heatmap, the tibble data frame, heatmap_data, is converted to a data matrix.\n\nheatmap_data = data.matrix(heatmap_data)\n\n\n\n\nWhen analysing a multivariate dataset, it is very common that the variables include values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation is commonly used before clustering. The three main data transformation methods are supported by the heatmaply() function are: scale, normalise, and percentise.\n\n\nWhen all variables are (or assumed to be) from some normal distribution, then scaling (i.e., subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution. In such a case, each value would reflect the distance from the mean in units of standard deviation. The “scale” argument in the heatmaply() function supports column and row scaling.\nAn interactive scaled basic cluster heatmap is then plotted using the heatmaply() function in the heatmaply package. The plot below uses the “scale” argument to scale column-wise.\n\n# User Choice: scale method\nheatmaply(heatmap_data,\n          scale = \"column\")\n\n\n\n\n\n\n\n\nWhen variables in the dataset comes from possibly different (and non-normal) distributions, the normalise function can be used to bring the values to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations. This preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”. Different from scaling, the normalise method is performed on the input dataset.\nAn interactive normalised basic cluster heatmap is then plotted using the heatmaply() function in the heatmaply package.\n\nheatmaply(normalize(heatmap_data))\n\n\n\n\n\n\n\n\nThis is similar to ranking the variables, but instead of keeping the rank values, they are divided by the maximal rank. This is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile. The benefit of the percentise method is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it. Similar to the normalise method, the percentise method is also performed on the input dataset.\nAn interactive percentised basic cluster heatmap is then plotted using the heatmaply() function in the heatmaply package.\n\nheatmaply(percentize(heatmap_data))\n\n\n\n\n\n\n\n\n\nThe heatmaply package supports a variety of hierarchical clustering algorithm. The main arguments are:\n\n“dist_method” whose default is NULL (which results in “euclidean” to be used). Can accept alternative character strings indicating the method to be passed to distfun. By default distfun is dist hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\n“hclust_method” whose default is NULL (which results in “complete” to be used). Can accept alternative character strings indicating the method to be passed to hclustfun By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC). Specifying hclust_method=NA causes heatmaply to use find_dend() to find the “optimal” dendrogram for the data.\n“k_row” is an integer scalar with the desired number of groups by which to color the dendrogram’s branches in the rows (uses color_branches()) If NA then find_k() is used to deduce the optimal number of clusters.\n\n\n# User Choice: dist_method\nheatmaply(normalize(heatmap_data),\n          dist_method = \"euclidean\",\n          hclust_method = NA,\n          k_row = NA)\n\n\n\n\n\nFor ease of use, the cluster heatmap will be designed to allow the user to calibrate the cluster heatmap statistically, with the relevant data table on the best clustering method and line chart on the number of clusters displayed as supplementary visualisations alongside the cluster heatmap to allow the user to experiment. In order to determine the best clustering method and number of clusters, the dend_expend() and find_k() functions in the dendextend package is used.\n\n# Supplementary Visualisation 1 - Best Clustering Method:\n# User Choice: dist()'s method\ndend_expend(dist(normalize(heatmap_data),\n          method = \"minkowski\"))[[3]] %&gt;% \n  select(2:3)\n\n  hclust_methods     optim\n1         ward.D 0.3716380\n2        ward.D2 0.3873639\n3         single 0.5676544\n4       complete 0.4216517\n5        average 0.6141891\n6       mcquitty 0.4155008\n7         median 0.4392578\n8       centroid 0.5498881\n\n# Supplementary Visualisation 2 - Optimal No. of Clusters:\n# User Choice: dist()'s method and hclust()'s method\nplot(find_k(hclust(dist(normalize(heatmap_data),\n          method = \"euclidean\"), \n          method = \"average\")))\n\n\n\n\n\n\n\nOne of the problems with hierarchical clustering is that it does not actually place the rows in a definite order; it merely constrains the space of possible orderings. The heatmaply() function uses the “seriate” argument to find an optimal ordering of rows and columns. It is of a character indicating the method of matrix sorting (default: “OLO”). Implemented options include:\n\n“OLO”: Optimal leaf ordering, optimizes the Hamiltonian path length that is restricted by the dendrogram structure - works in O(n^4);\n“mean”: sorts the matrix based on the reorderfun using marginal means of the matrix. This is the default used by heatmap.2;\n“none”: the default order produced by the dendrogram; and\n“GW”: Gruvaeus and Wainer heuristic to optimize the Hamiltonian path length that is restricted by the dendrogram structure.\n\n\n\nThe seriation algorithm of Optimal Leaf Ordering (OLO) is used in the plot. This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimise the sum of dissimilarities between adjacent leaves.\n\nheatmaply(normalize(heatmap_data),           \n          seriate = \"OLO\")\n\n\n\n\n\n\n\n\nThe default option is “OLO” (Optimal leaf ordering) which optimises the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(heatmap_data),\n          seriate = \"GW\")\n\n\n\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as heatmap.2.\n\nheatmaply(normalize(heatmap_data),\n          seriate = \"mean\")\n\n\n\n\n\n\n\n\nThe option “none” gives us dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(heatmap_data),\n          seriate = \"none\")\n\n\n\n\n\n\n\n\n\nTo summarise, the relevant codes would be used to generate the cluster heatmap based on the user’s choices for three main categories of decisions.\n\n# Step 1: Choice of Data Sampling Approach\n# - User Input: seed value\n# set.seed(&lt;value&gt;)\n# \n# - User Choice: group_by value\n# - User Input: prop value\n# heatmap_data = cluster_data %&gt;%\n#   na.omit() %&gt;%\n#   group_by(&lt;variable name&gt;)  %&gt;%\n#   slice_sample(prop = &lt;value&gt;, replace = &lt;TRUE/FALSE&gt;) %&gt;%\n#   ungroup()\n#   \n# Step 2: Choice of Data Transformation Method\n# - If Choice is Scale method: \n# heatmaply(heatmap_data,           \n#           scale = \"&lt;column/row&gt;\")  \n# \n# - If Choice is Normalise method: \n# heatmaply(normalize(heatmap_data),           \n#           scale = \"none\")  \n# \n# - If Choice is Percentise method: \n# heatmaply(percentize(heatmap_data),           \n#           scale = \"none\")\n# \n# Step 3: Choice of Clustering Algorithm\n# - User Choice: dist_method\n# heatmaply(normalize(heatmap_data),\n#           dist_method = \"&lt;euclidean/maximum/manhattan/canberra/binary/minkowski&gt;\",\n#           hclust_method = NA,\n#           k_row = NA)\n# \n# Supplementary Visualisation - Best Clustering Method:\n# - User Choice: dist() method\n# dend_expend(dist(&lt;normalize/percentize&gt;(heatmap_data),\n#           &lt;scale = \"&lt;column/row&gt;\",&gt;\n#           method = \"&lt;euclidean/maximum/manhattan/canberra/binary/minkowski&gt;\"))[[3]] %&gt;% \n#   select(2:3)\n# \n# Supplementary Visualisation - Optimal No. of Clusters:\n# - User Choice: dist() method and hclust() method\n# plot(find_k(hclust(dist(&lt;normalize/percentize&gt;(heatmap_data),\n#           &lt;scale = \"&lt;column/row&gt;\",&gt;\n#           method = \"&lt;euclidean/maximum/manhattan/canberra/binary/minkowski&gt;\"), \n#           method = \"&lt;complete/ward.D/ward.D2/single/average/mcquitty/median/centroid&gt;\")))\n# \n# Step 4: Choice of Seriation Algorithm\n# - User Choice: seriate\n# heatmaply(&lt;normalize/percentize&gt;(heatmap_data),\n#           &lt;scale = \"&lt;column/row&gt;\",&gt;\n#           seriate = \"&lt;OLO/mean/GW/none&gt;\",\n#           k_row = NA)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#parallel-plot",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#parallel-plot",
    "title": "Take-home Exercise 4",
    "section": "",
    "text": "Parallel plot is an effective tool for exploring relationships among multiple variables simultaneously, providing a holistic view of the data.\n\nFacilitate the identification of patterns, trends, and anomalies in the dataset by visually connecting related variables.\nSuitable for comparing patterns between different groups or categories within the dataset, aiding in group-wise analysis.\nComplement clustering analyses by visually representing the similarities and differences among clusters.\nOutliers can be easily identified as deviations from the typical pattern along the parallel axes.\nThe relative importance of variables can be assessed based on their impact on the overall pattern in the parallel plot.\nUsers can customise parallel plots by selecting specific variables, adjusting color schemes, and modifying the appearance for better interpretability.\nInteractive parallel plots allow users to dynamically explore the data, emphasising certain variables or groups for deeper investigation.\n\nHowever, some words of caution would need to be taken into consideration as well:\n\nHandling high-dimensional data can be challenging as the complexity of the plot increases with the number of variables.\nWith many variables, lines on the plot may overlap, making it difficult to discern individual patterns and affecting interpretability.\nThe way variables are connected can introduce subjectivity, and different connections may lead to different interpretations.\nParallel plots are most suitable for numerical data, and their effectiveness may diminish with categorical or ordinal variables. [Note: However, parallel plot is still a better choice than Sankey diagram in dealing with a mix of continuous and categorical variables within a dataset.]\n\nThe R package used for plotting interactive parallel plots is parallelPlot.\n\npacman::p_load(parallelPlot)\n\n\n\n\nAs interactive parallel plots cannot be easily generated for large datasets, a representative sample is taken for the purpose of data visualisation.\nThe na.omit() function in the stats package is used to omit the rows with missing values in any of the variables. Then, the dataset is sampled by “SchoolID” using the slice_sample() function in the dplyr package, with the proportion of 10%. This would provide a data subset that allows for easier data visualisation. The “SchoolID” variable is then dropped since there are too many IDs, which may not yield useful insights in parallel plots.\n\n# User Input: seed value \nset.seed(123)  \n\n# User Choice: group_by value \n# User Input: prop value \nparallelplot_data = cluster_data %&gt;%   \n  na.omit() %&gt;%   \n  group_by(SchoolID)  %&gt;%   \n  slice_sample(prop = 0.1, replace = FALSE) %&gt;%   \n  ungroup() %&gt;%\n  select(-c(\"SchoolID\"))\n\n\n\n\nThe list of categories for the categorical variables are first prepared.\n\n# Preparing List of Categories for Each Categorical Variable in Specific Order\nLoneliness = list(\"Strongly Agree\",\n                  \"Agree\",\n                  \"Disagree\",\n                  \"Strongly Disagree\")\n\nClassroomSafety = list(\"Strongly Disagree\",\n                \"Disagree\",\n                \"Agree\",\n                \"Strongly Agree\")\n\nPreference_Math = list(\"Strongly Disagree\",\n                \"Disagree\",\n                \"Agree\",\n                \"Strongly Agree\")\n\nPreference_Reading = list(\"Strongly Disagree\",\n                \"Disagree\",\n                \"Agree\",\n                \"Strongly Agree\")\n\nPreference_Science = list(\"Strongly Disagree\",\n                \"Disagree\",\n                \"Agree\",\n                \"Strongly Agree\")\n\nBooks = list(\"0\",\n             \"1 - 10\",\n             \"11 - 25\",\n             \"26 - 100\",\n             \"101 - 200\",\n             \"201-500\",\n             \"&gt;500\")\n\nExercise = list(\"0\",\n                \"1\", \n                \"2\",\n                \"3\",\n                \"4\",\n                \"5\",\n                \"6\",\n                \"7\",\n                \"8\",\n                \"9\",\n                \"10\")\n\nFamilyCommitment = list(\"0\",\n                \"1\", \n                \"2\",\n                \"3\",\n                \"4\",\n                \"5\",\n                \"6\",\n                \"7\",\n                \"8\",\n                \"9\",\n                \"10\")\n\nHomework_Math = list(\"≤ 0.5hr\",\n             \"0.5hr - 1hr\",\n             \"1hr - 2hr\",\n             \"2hr - 3hr\",\n             \"3 - 4 hr\",\n             \"&gt; 4hr\")\n\nHomework_Reading = list(\"≤ 0.5hr\",\n             \"0.5hr - 1hr\",\n             \"1hr - 2hr\",\n             \"2hr - 3hr\",\n             \"3 - 4 hr\",\n             \"&gt; 4hr\")\n\nHomework_Science = list(\"≤ 0.5hr\",\n             \"0.5hr - 1hr\",\n             \"1hr - 2hr\",\n             \"2hr - 3hr\",\n             \"3 - 4 hr\",\n             \"&gt; 4hr\")\n\nImmigration = list(\"Native\",\n                   \"2nd Generation\",\n                   \"3rd Generation\")\n\nParentsEducation = list(\"Pre-Primary\",\n                        \"Primary\", \n                        \"Secondary\",\n                        \"Post-Secondary\",\n                        \"Tertiary\")\n\nSibling = list(\"0\",\n             \"1\",\n             \"2\",\n             \"≥3\")\n\nVehicle = list(\"0\",\n             \"1\",\n             \"2\",\n             \"≥3\")\n\nTeacherSupport = list(\"Never or almost never\",\n                      \"Some lessons\",\n                      \"Most lesson\",\n                      \"Every lesson\")\n\nHomeLanguage = list(\"English\",\n                    \"Others\")\n\nSchoolType = list(\"Public\",\n                  \"Private\")\n\nAircon = list(\"No\", \"Yes\")\n\nHelper = list(\"No\", \"Yes\")\n\nGender = list(\"Female\",\n              \"Male\")\n\nThe parallelPlot() function is then used to plot an interactive parallel plot.\n\n# Full Set of Variables & Lists of Categories\n# parallelPlot(parallelplot_data[,c(1:25)],\n#              categorical = list(\n#                Loneliness,\n#                ClassroomSafety,\n#                TeacherSupport,\n#                Gender,\n#                Homework_Math,\n#                Homework_Reading,\n#                Homework_Science,\n#                SchoolType,\n#                ParentsEducation,\n#                Immigration,\n#                HomeLanguage,\n#                Sibling,\n#                Aircon,\n#                Helper,\n#                Vehicle,\n#                Books,\n#                Exercise,\n#                OwnRoom,\n#                FamilyCommitment,\n#                Preference_Math,\n#                Preference_Reading,\n#                Preference_Science,\n#                NULL,\n#                NULL,\n#                NULL))\n\n\n# User Input: Chosen Columns (Recommended: Up to 8)\nparallelPlot(parallelplot_data[,c(4, 5, 8, 9, 10, 17, 20, 23)],\n             categorical = list(\n               Gender,\n               Homework_Math,\n               SchoolType,\n               ParentsEducation,\n               Immigration,\n               Exercise,\n               Preference_Math,\n               NULL))\n\n\n\n\n\n\n\n\nThe aesthetics of the parallel plot is then added. This allows the user to customise the plot’s colours.\nAlso, the “rotateTitle” argument is set to “TRUE” for readability.\n\n# User Input: Chosen Columns (Recommended: Up to 8)\n# User Input: Name of Reference Column (to determine colour to attribute to a row)\n# User Input: Colour Scale for Continuous Data\n# User Choice: Colour Scale for Categorical Data\nparallelPlot(parallelplot_data[,c(4, 5, 8, 9, 10, 17, 20, 23)],\n             categorical = list(\n               Gender,\n               Homework_Math,\n               SchoolType,\n               ParentsEducation,\n               Immigration,\n               Exercise,\n               Preference_Math,\n               NULL),\n             rotateTitle = TRUE,\n             refColumnDim = \"Gender\",\n             continuousCS = \"Blues\",\n             categoricalCS = \"Accent\")\n\n\n\n\n\n\n\n\n\n# - User Input: Chosen Columns (Recommended: Up to 8)\n# parallelPlot(parallelplot_data[,c(&lt;chosen columns&gt;)],\n#              categorical = list(&lt;of lists of categories for each chosen column&gt;))\n# \n# - User Input: Name of Reference Column (to determine colour to attribute to a row)\n# - User Input: Colour Scale for Continuous Data\n# - User Choice: Colour Scale for Categorical Data\n# parallelPlot(parallelplot_data[,c(&lt;chosen columns&gt;)],\n#              categorical = list(&lt;of lists of categories for each chosen column&gt;),\n#              rotateTitle = TRUE,\n#              refColumnDim = \"&lt;variable name&gt;\",\n#              continuousCS = \"&lt;Viridis/Inferno/Magma/Plasma/Warm/Cool/Rainbow/CubehelixDefault/Blues/Greens/Greys/Oranges/Purples/Reds/BuGn/BuPu/GnBu/OrRd/PuBuGn/PuBu/PuRd/RdBu/RdPu/YlGnBu/YlGn/YlOrBr/YlOrRd&gt;\",\n#              categoricalCS = \"&lt;Category10/Accent/Dark2/Paired/Set1&gt;\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#user-interface-design",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#user-interface-design",
    "title": "Take-home Exercise 4",
    "section": "",
    "text": "After working through the details of the two types of plots (cluster heatmap and parallel plot) in the preceeding sections, the storyboard approach is now used to prototype the Cluster Analysis portion of the Shiny application. The two storyboards below (one for each type of plot) are used to illustrate the different components of the UI for the proposed design.\n\n\nTo summarise, the input/choices that the user is required to make are:\n\nData Sampling Approach: seed value, group_by() value, slice_sample() prop value and replace/do not replace.\nData Transformation Method: scale by column/row, normalise, or percentise.\nClustering Algorithm:\n\ndist_method in heatmaply: euclidean, maximum, manhattan, canberra, binary, or minkowski.\ndist in dend_expend: euclidean, maximum, manhattan, canberra, binary, or minkowski.\nhclust in find_k: complete, ward.D, ward.D2, single, average, mcquitty, median, or centroid.\nseriation algorithm in heatmaply: OLO, mean, GW, or none.\n\n\nAs mentioned, the supplementary data table and visualisation will help the user who may not be very familiar with cluster heatmap to choose the suitable clustering method and number of clusters.\n\n\n\nStoryboard 1: Cluster Heatmap\n\n\n\n\n\nStoryboard 2: Annotated Cluster Heatmap\n\n\n\n# Step 1: Choice of Data Sampling Approach\n# - User Input: seed value\n# set.seed(&lt;value&gt;)\n# \n# - User Choice: group_by value\n# - User Input: prop value\n# heatmap_data = cluster_data %&gt;%\n#   na.omit() %&gt;%\n#   group_by(&lt;variable name&gt;)  %&gt;%\n#   slice_sample(prop = &lt;value&gt;, replace = &lt;TRUE/FALSE&gt;) %&gt;%\n#   ungroup()\n#   \n# Step 2: Choice of Data Transformation Method\n# - If Choice is Scale method: \n# heatmaply(heatmap_data,           \n#           scale = \"&lt;column/row&gt;\")  \n# \n# - If Choice is Normalise method: \n# heatmaply(normalize(heatmap_data),           \n#           scale = \"none\")  \n# \n# - If Choice is Percentise method: \n# heatmaply(percentize(heatmap_data),           \n#           scale = \"none\")\n# \n# Step 3: Choice of Clustering Algorithm\n# - User Choice: dist_method\n# heatmaply(normalize(heatmap_data),\n#           dist_method = \"&lt;euclidean/maximum/manhattan/canberra/binary/minkowski&gt;\",\n#           hclust_method = NA,\n#           k_row = NA)\n# \n# Supplementary Visualisation - Best Clustering Method:\n# - User Choice: dist() method\n# dend_expend(dist(&lt;normalize/percentize&gt;(heatmap_data),\n#           &lt;scale = \"&lt;column/row&gt;\",&gt;\n#           method = \"&lt;euclidean/maximum/manhattan/canberra/binary/minkowski&gt;\"))[[3]] %&gt;% \n#   select(2:3)\n# \n# Supplementary Visualisation - Optimal No. of Clusters:\n# - User Choice: dist() method and hclust() method\n# plot(find_k(hclust(dist(&lt;normalize/percentize&gt;(heatmap_data),\n#           &lt;scale = \"&lt;column/row&gt;\",&gt;\n#           method = \"&lt;euclidean/maximum/manhattan/canberra/binary/minkowski&gt;\"), \n#           method = \"&lt;complete/ward.D/ward.D2/single/average/mcquitty/median/centroid&gt;\")))\n# \n# Step 4: Choice of Seriation Algorithm\n# - User Choice: seriate\n# heatmaply(&lt;normalize/percentize&gt;(heatmap_data),\n#           &lt;scale = \"&lt;column/row&gt;\",&gt;\n#           seriate = \"&lt;OLO/mean/GW/none&gt;\")\n\n\n\n\nTo summarise, the input/choices that the user is required to make are:\n\nChosen Variables (columns).\n\nThe list of lists of categories in each categorical variable chosen have been pre-ordered for the dataset for ease of use.\n\nReference Column: to determine colour to attribute to a row.\nColour Scale for Continuous Data: Viridis, Inferno, Magma, Plasma, Warm, Cool, Rainbow, CubehelixDefault, Blues, Greens, Greys, Oranges, Purples, Reds, BuGn, BuPu, GnBu, OrRd, PuBuGn, PuBu, PuRd, RdBu, RdPu, YlGnBu, YlGn, YlOrBr, or YlOrRd.\nColour Scale for Categorical Data: Category10, Accent, Dark2, Paired, or Set1.\n\n\n\n\nStoryboard 3: Parallel Plot\n\n\n\n\n\nStoryboard 4: Annotated Parallel Plot\n\n\n\n# - User Input: Chosen Columns (Recommended: Up to 8)\n# parallelPlot(parallelplot_data[,c(&lt;chosen columns&gt;)],\n#              categorical = list(&lt;of lists of categories for each chosen column&gt;))\n# \n# - User Input: Name of Reference Column (to determine colour to attribute to a row)\n# - User Input: Colour Scale for Continuous Data\n# - User Choice: Colour Scale for Categorical Data\n# parallelPlot(parallelplot_data[,c(&lt;chosen columns&gt;)],\n#              categorical = list(&lt;of lists of categories for each chosen column&gt;),\n#              rotateTitle = TRUE,\n#              refColumnDim = \"&lt;variable name&gt;\",\n#              continuousCS = \"&lt;Viridis/Inferno/Magma/Plasma/Warm/Cool/Rainbow/CubehelixDefault/Blues/Greens/Greys/Oranges/Purples/Reds/BuGn/BuPu/GnBu/OrRd/PuBuGn/PuBu/PuRd/RdBu/RdPu/YlGnBu/YlGn/YlOrBr/YlOrRd&gt;\",\n#              categoricalCS = \"&lt;Category10/Accent/Dark2/Paired/Set1&gt;\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#conclusion",
    "title": "Take-home Exercise 4",
    "section": "",
    "text": "In conclusion, the Shiny application powerful tool for exploratory and confirmatory data analysis through visualisation. The PISA performance dataset is interesting and contains useful data for studying Singapore students’ academic performance in the three subjects at age 15. The prototyping of the Shiny application for the cluster analysis of the PISA dataset in this take-home exercise has provided an invaluable opportunity to apply various skills learnt in this module for data wrangling, data visualisation, and design. Our team hopes that the final product would provide a useful interface for users to explore the wonderful wealth of information within the PISA dataset, to better understand how various factors including gender, type of school, and socioeconomic status affect academic performance in Singapore."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#key-references",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#key-references",
    "title": "Take-home Exercise 4",
    "section": "",
    "text": "R for Visual Analytics.\nR for Data Science.\nFundamentals of Data Visualisation.\nStoryboard for Shiny application.\n\n~~~ End of Take-home Exercise 4 ~~~"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "Singapore has come a long way since gaining independence in 1965. Without natural resources, human capital development through a well-planned education system is a critical part of Singapore’s transformation from third world to first.\nDespite the success, there is still a correlation between socio-economic status and education achievement, as well as ingrained perceptions that some schools are better than others.\nHence, there is a need to use data to analyse the performance of Singapore students across different subjects, and identify any relationships between the performance in various subjects and factors such as gender, socioeconomic status, and type of school.\n\n\n\nIn this take-home exercise, the objective is to improve on an original visualisation by a fellow course-mate by focusing on what works, what does not work, why they do not work, and how to make it better.\nThe specific tasks are:\n\nSelect a Take-home Exercise 1 prepared by a course-mate;\nCritic the submission in terms of clarity and aesthetics;\nPrepare a sketch for the alternative design using the data visualisation design principles and best practices from Lesson 1 and Lesson 2; and\nRemake the original design using ggplot2, ggplot2 extensions, and tidyverse packages.\n\n\n\n\n\n\n\nThe R packages used in this take-home exercise are:\n\nhaven for importing SAS files;\ntidyverse (i.e. readr, tidyr, dplyr, ggplot2) for performing data science tasks such as importing, tidying, and wrangling data, as well as creating graphics based on The Grammar of Graphics;\nreshape2 for transforming data between wide and long formats;\nggthemes for extra themes, geoms, and scales for ggplot2;\nggridges for creating ridgeline plots;\nggdist for visualising distributions and uncertainty; and\nggpubr for creating publication ready ggplot2 plots.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed in the computer. If yes, they are then loaded into the R environment. If no, they are installed, and then loaded into the R environment.\n\npacman::p_load(haven, tidyverse, reshape2,\n               ggthemes, ggridges, ggdist,\n               ggpubr)\n\n\n\n\nThe OECD Programme for International Student Assessment (PISA) measures how well 15-year-old students in different countries are “prepared to meet the challenges of today’s knowledge societies” by looking at “their ability to use their knowledge and skills to meet real-life challenges”. The PISA surveys take place very three years, the latest being conducted in 2022.\nThe PISA 2022 database contains the full set of responses from individual students, school principals, and parents. There are a total of five data files and their contents are as follows:\n\nStudent questionnaire data file;\nSchool questionnaire data file;\nTeacher questionnaire data file;\nCognitive item data file; and\nQuestionnaire timing data file.\n\nFor the purpose of this take-home exercise, the “Student questionnaire data file” is used.\n\n\n\n\n\n\nThe dataset used in this take-home exercise is the 2022 PISA student questionnaire data file, cy08msp_stu_qqq.sas7bdat, which is in the SAS file format.\nThe file is imported into the R environment using the read_sas() function in the haven package and stored as the R object, stu_qqq.\n\nstu_qqq = read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\nThe tibble data frame, stu_qqq, has 1,279 columns (variables) and 613,744 rows (observations).\n\n\n\nThere are 6,606 rows with the country code (i.e., CNT) value of “SGP”, which represents Singapore. This count is cross-verified by the information provided in the “CY08MSP_STU_QQQ” sheet in the codebook. The codebook also stated that Singapore students’ made up 1.0763% of the entire global student population who took part in the 2022 PISA.\nThe filter() function in the dplyr package is used to obtain these rows, and stored as the R object, stu_qqq_SG.\n\nstu_qqq_SG = stu_qqq %&gt;% filter(CNT == \"SGP\")\n\nThe tibble data frame, stu_qqq_SG, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(stu_qqq_SG, \"data/stu_qqq_SG.rds\")\n\n\nstu_qqq_SG = read_rds(\"data/stu_qqq_SG.rds\")\n\n\n\n\nUsing the chosen submission, the dataset is prepared according to the original submission.\n\n\nCode\n#  PREPARE MAIN DATA-FRAME\nstudent_columns = \"CNTSTUID\"\ngender_columns = \"ST004D01T\"\nschool_columns = \"CNTSCHID\"\n\neducation_column_mother = \"ST005Q01JA\"\neducation_column_father = \"ST007Q01JA\"\n\ntraining_column_mother = \"ST006Q01JA\"\ntraining_column_father = \"ST008Q01JA\"\npossession_room_column = \"ST250Q01JA\"\npossession_computer_column = \"ST250Q02JA\"\npossession_software_column = \"ST250Q03JA\"\npossession_phone_column = \"ST250Q04JA\"\npossession_internet_column = \"ST250Q05JA\"\npossession_book_column = \"ST255Q01JA\"\n\nmath_columns = c(\"PV1MATH\", \"PV2MATH\", \"PV3MATH\", \"PV4MATH\", \"PV5MATH\", \"PV6MATH\", \"PV7MATH\", \"PV8MATH\", \"PV9MATH\", \"PV10MATH\")\nreading_columns = c(\"PV1READ\", \"PV2READ\", \"PV3READ\", \"PV4READ\", \"PV5READ\", \"PV6READ\", \"PV7READ\", \"PV8READ\", \"PV9READ\", \"PV10READ\")\nscience_columns = c(\"PV1SCIE\", \"PV2SCIE\", \"PV3SCIE\", \"PV4SCIE\", \"PV5SCIE\", \"PV6SCIE\", \"PV7SCIE\", \"PV8SCIE\", \"PV9SCIE\", \"PV10SCIE\")\n\nstudent_ID = stu_qqq_SG[, student_columns, drop = FALSE]\ngender = stu_qqq_SG[, gender_columns, drop = FALSE]\nschool_ID = stu_qqq_SG[, school_columns, drop = FALSE]\neducation_mother = stu_qqq_SG[, education_column_mother, drop = FALSE]\neducation_father = stu_qqq_SG[, education_column_father, drop = FALSE]\n\ntraining_mother = stu_qqq_SG[, training_column_mother, drop = FALSE]\ntraining_father = stu_qqq_SG[, training_column_father, drop = FALSE]\npossession_room = stu_qqq_SG[, possession_room_column, drop = FALSE]\npossession_computer = stu_qqq_SG[, possession_computer_column, drop = FALSE]\npossession_software = stu_qqq_SG[, possession_software_column, drop = FALSE]\npossession_phone = stu_qqq_SG[, possession_phone_column, drop = FALSE]\npossession_internet = stu_qqq_SG[, possession_internet_column, drop = FALSE]\npossession_book = stu_qqq_SG[, possession_book_column, drop = FALSE]\n\nmath_avg = rowMeans(stu_qqq_SG[, math_columns, drop = FALSE])\nreading_avg = rowMeans(stu_qqq_SG[, reading_columns, drop = FALSE])\nscience_avg = rowMeans(stu_qqq_SG[, science_columns, drop = FALSE])\n\nstu_df = data.frame(Student_ID = student_ID,\n  Gender = gender,\n  School_ID = school_ID,\n  Education_mother = education_mother,\n  Education_father = education_father,\n \n#  Training_mother = training_mother,\n#  Training_father = training_father,\n  Possession_room = possession_room,\n  Possession_computer = possession_computer,\n  Possession_software = possession_software,\n  Possession_phone = possession_phone,\n  Possession_internet = possession_internet,\n  Possession_book = possession_book,\n  \n  Math_Average = round(math_avg,digits=2),\n  Reading_Average = round(reading_avg,digits=2),\n  Science_Average = round(science_avg,digits=2),\nAverage_score=round(((math_avg+reading_avg+science_avg)/3),digits=2))\n\nnames(stu_df) = c(\"Student_ID\",\"Gender\",\"School_ID\",\"Education_mother\",\n                   \"Education_father\",\"Possession_room\",\"Possession_computer\",\n                   \"Possession_software\",\"Possession_phone\",\n                   \"Possession_internet\",\"Possession_book\",\"Math_Average\",\n                   \"Reading_Average\",\"Science_Average\",\"Average_Score\")\n\n\nThe finalised tibble data frames, stu_df is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(stu_df, \"data/stu_df.rds\")\n\n\nstu_df = read_rds(\"data/stu_df.rds\")\n\n\n\n\n\n\n\n\n\nThe original visualisation at Section 3 (Distribution of Singapore Students’ Performance) of the submission is reproduced below. The plot on the average score (not specific subjects) is chosen.\n\n\n\n\n\nThe observation made:\n\n“In this case, all the histogram are left skewed. The left-skewed (negative skewness) nature of the histogram indicates that there is a concentration of scores towards the higher end, with a tail extending towards lower scores. The mean is influenced by these lower scores, pulling it towards the left. The median, being less sensitive to extreme values, is positioned higher in the distribution.”\n\n\n\n\nClarity\n\n😊 Right Visualisation. The use of a histogram provides a clear representation of the distribution of the average scores among the students, and the use of the mean and median vertical dashed lines shows the left-skewness of the distribution.\n😕 Missing Key Message in Title. The key message stated in the observation is not included in the title (which is factual). Stating the key message in the title would help the reader zoom in on the intended message of the plot.\n😕 Missing Source Caption. It would be useful to credit the source of the data directly in the plot to give the reader the appropriate context for interpreting plot.\n\nAesthetics\n\n😊 Graphical Integrity Maintained. The y-axis starts from zero, ensuring that the histogram is not distorted.\n😊 Simple Colours & Font Used. The plot uses an easy-to-read font and colour. However, this can be enhanced by using a professional theme.\n😕 Overcrowding of Details & Labels. The x-axis labels use values that are too detailed (not rounded up). Also, each column of the histogram is labelled with the corresponding frequency for that range of scores. These details are overwhelming and distract from the intended message. The placement of the mean and median values can also be better adjusted to avoid overlapping with the histogram. The decimal values in the mean and median may also be removed for simplicity to improve readability given that the difference in the two values is more than 1 point.\n😕 Orientation of Label. The y-axis title can be rotated for easier reading.\n\n\n\n\nThe sketch of the alternative design is as follows:\n\n\n\n\n\n\n\n\nAn improved plot is made as follows:\n\nImproved PlotCode\n\n\n\n\n\n\n\n\n\n\naverage_sci = mean(stu_df$Average_Score, na.rm = TRUE)\nmedian_sci = median(stu_df$Average_Score, na.rm = TRUE)\n\nggplot(stu_df, aes(x = Average_Score)) +\n  geom_histogram(bins = 20, \n                 fill = \"cadetblue3\", \n                 color = \"grey95\") +\n  labs(title = \"More Students Score Above Mean Average Score\",\n       subtitle = \"Distribution of Average Scores\",\n       x = \"Average Score\",\n       y = \"Frequency\", \n       caption = \"Source: PISA 2022\") +\n  theme_minimal() +\n  scale_x_continuous(n.breaks = 10) +\n  scale_y_continuous(n.breaks = 6) +\n  geom_vline(xintercept = average_sci, color = \"navy\", linetype = \"dashed\", size = 1) +\n  geom_text(aes(x = average_sci-60, y = 900, label = paste(\"Mean:\", round(average_sci))), vjust = 2, color = \"navy\",size =4) +\n  geom_vline(xintercept = median_sci, color = \"maroon\", linetype = \"dotdash\", size = 1) +\n  geom_text(aes(x = median_sci+100, y = 900, label = paste(\"Median:\", round(median_sci))), vjust = 2, color = \"maroon\",size=4) +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\n\n\n\n\n\n\nThe original visualisation at Section 4.1 (Performance by Gender) of the submission is reproduced below. Again, the plot on the average score (not specific subjects) is chosen.\n\n\n\n\n\nThe relevant observations made:\n\n“From the above violin plots, we can conclude:\n\nMale plot is more spread out, which indicates greater variability in scores within the male group\nFemale group and male group have similar average performance.\nAlmost all the distribution are left skewed, indicating a concentration of students with relatively higher scores, but a few students have much lower scores.”\n\n\n\n\n\nClarity\n\n😊😕 (Almost) Right Visualisation. The use of two violin plots shows the distributions of the average scores for female and male students. The use of the box plots in the middle of the violin plots shows the median average scores. However, as the two violin plots do not overlap, the comparison of average scores based on gender is more difficult to make visually. A density plot grouped by gender, with vertical lines added to show the median average scores, may be more appropriate for comparison purposes.\n😕 Missing Key Message in Title. The key message stated in the observation is not included in the title (which is factual). Stating the key message in the title would help the reader zoom in on the intended message of the plot.\n😕 Missing Source Caption. It would be useful to credit the source of the data directly in the plot to give the reader the appropriate context for interpreting plot.\n\nAesthetics\n\n😊 Simple Colours & Font Used. The plot uses an easy-to-read font and the stereotypical colours associated with each gender (pink for female; blue for male). However, this can be enhanced by using a simpler/minimalist theme for the background to keep the focus on the coloured density plots.\n😕 Redundant Legend. The legend is redundant given that the x-axis is labelled.\n😕 Unclear Labels for Mean and Median Values. The subtitle states the colours corresponding to the mean and median values for each gender which requires the reader to read and match accordingly, and reconfirm each time he/she views the plot. This is less intuitive than putting the labels together with the values.\n😕 Orientation of Label. The y-axis title can be rotated for easier reading.\n😕 Mis-titled Y-axis. The y-axis title was mistakenly labelled as “Reading Score” instead of “Average Score”.\n\n\n\n\nThe sketch of the alternative design is as follows:\n\n\n\n\n\n\n\n\nAn improved plot is made as follows:\n\nImproved PlotCode\n\n\n\n\n\n\n\n\n\n\nstu_df$Gender = ifelse(\n  stu_df$Gender == 1, \n  \"Female\", \"Male\")\n\nf = stu_df %&gt;%\n  filter(Gender == \"Female\")\n\nm = stu_df %&gt;%\n  filter(Gender == \"Male\")\n\nstu_df = stu_df %&gt;%\n  mutate(Gender = fct_relevel(Gender, \n                               \"Female\", \n                               \"Male\"))\n\nggplot(stu_df,\n       aes(x = Average_Score,\n           fill = Gender)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(aes(xintercept=median(f$Average_Score)),\n             color=\"red\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) +\n  geom_vline(aes(xintercept=median(m$Average_Score)),\n             color=\"blue\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) +\n  ggtitle(label = \"On Average, Boys and Girls Perform Similarly\",\n          subtitle= \"Distribution of Average Scores by Gender\") + \n  labs(caption = \"Source: PISA 2022\") +\n  ylab(\"Density\") + xlab(\"Average Score\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1)) +\n  theme_minimal() +\n  geom_vline(aes(xintercept=median(f$Average_Score)),\n             color=\"#CC3366\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) +\n  geom_vline(aes(xintercept=median(m$Average_Score)),\n             color=\"#033336\", \n             linetype=\"dotdash\", \n             linewidth=1,\n             alpha = 0.5) +\n  geom_text(aes(x = 690, y = 0.0045, label = paste(\"Median:\", round(median(m$Average_Score)))), vjust = 2, color = \"#033336\",size =4) +\n  geom_text(aes(x = 480, y = 0.0045, label = paste(\"Median:\", round(median(f$Average_Score)))), vjust = 2, color = \"#CC3366\",size=4) +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\n\nNote: The same makeover approach is also applicable to Section 4.3.3 (Performance Distribution by Internet Access) as it also uses violin plots.\n\n\n\n\n\n\n\nThe original visualisation at Section 4.3.2 (Distribution of by Number of Book in Students’ Home) of the submission is reproduced below. The plot on the reading score is chosen.\n\n\n\n\n\nThe observation made:\n\n“From the density plot, we can interpret that:\n\nFor students with no books in their homes, the density plot is right-skewed. This suggests that a lack of books is associated with a concentration of lower academic scores. The right skewness indicates that the majority of students in this group may have below-average scores.\nAs the number of books in the home increases, the density plot becomes more left-skewed. This trend suggests a positive correlation between the abundance of books and higher academic performance. The left skewness indicates a concentration of higher scores, with more students performing above the average.\nThe observation that the mean score of students increases as the number of books in the home increases aligns with the general trend of a left-skewed density plot. This indicates that, on average, students with access to a greater number of books tend to achieve higher academic scores.\nA notable deviation from the general trend occurs for students whose homes have “more than 500 books.” In this category, the mean score decreases, contrary to the overall positive relationship observed. This suggests that there may be diminishing returns in terms of academic performance when the number of books surpasses a certain threshold.\n\nIn summary, the density plot illustrates a positive association between the number of books in the home and student performance. However, the deviation observed for the “more than 500 books” category suggests a nuanced relationship, highlighting the need to consider optimal conditions for leveraging the positive influence of books on academic outcomes.”\n\n\n\n\nClarity\n\n😊😕 (Almost) Right Visualisation. The use of density plots, with vertical lines showing the median reading scores, is useful for comparing the distributions of reading scores based on the number of books at home. However, this can be further enhanced with the use of a ridgelines plot.\n😕 Missing Key Message in Title. The key message stated in the observation is not included in the title (which is factual). Stating the key message in the title would help the reader zoom in on the intended message of the plot.\n😕 Difficult to Compare Distributions. Given that there are many categories, it is difficult to compare the distributions. It may be useful to showcase the quartile cut-offs to allow for better comparisons across different density plots.\n😕 Missing Source Caption. It would be useful to credit the source of the data directly in the plot to give the reader the appropriate context for interpreting plot.\n\nAesthetics\n\n😕 Unclear Colours Used. The plot uses an easy-to-read font. However, the choice of gradient colours is not appropriate as the answers form a categorical, not continuous, variable. The use of gradient colours does not allow the reader to differentiate the distinct categories of answers to the question in the questionnaire on the number of books at home.\n😕 Lack of Labels for Categories. The categories representing different ranges of the number of books at home requires a separate table for interpretation. This inconveniences the reader. Hence, it would be better to label each density plot directly for easier identification.\n😕 No Labels for Mean Values. The values of the dashed vertical lines are not labelled. It may be useful to label some of them to allow for direct comparison. Alternatively, to prevent clutter, it may be useful to add just two vertical lines - overall mean and overall median for broad comparisons.\n😕 Orientation of Label. The y-axis title can be rotated for easier reading.\n\n\n\n\nThe sketch of the alternative design is as follows:\n\n\n\n\n\n\n\n\nAn improved plot is made as follows:\n\nImproved PlotCode\n\n\n\n\n\n\n\n\n\n\nstu_df$Possession_book = as.character(stu_df$Possession_book)\nstu_df = stu_df %&gt;% \n  mutate(Possession_book = recode(Possession_book,\n                        \"1\" = \"0 Book\",\n                        \"2\" = \"1-10 Books\",\n                        \"3\" = \"11-25 Books\",\n                        \"4\" = \"26-100 Books\",\n                        \"5\" = \"101-200 Books\",\n                        \"6\" = \"201-500 Books\",\n                        \"7\" = \"&gt;500 Books\"))\n\nr = stu_df %&gt;% \n  select(Possession_book, Reading_Average) %&gt;%\n  na.omit() %&gt;%\n  mutate(Books = fct_relevel(Possession_book, \n                              \"0 Book\", \n                              \"1-10 Books\",\n                              \"11-25 Books\",\n                              \"26-100 Books\",\n                              \"101-200 Books\",\n                              \"201-500 Books\",\n                              \"&gt;500 Books\"))\n                        \nggplot(r, \n       aes(x = Reading_Average, \n           y = Books,\n           fill = factor(after_stat(quantile)))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 2,\n    quantile_lines = TRUE) +\n  scale_fill_brewer(type = \"seq\", palette = \"Set3\") +\n  ggtitle(label = \"More Books at Home Generally Correspond to\\nHigher Reading Scores\",\n          subtitle = \"Distribution of Reading Scores vis-a-vis Number of Books at Home\") +\n  xlab(\"Score\") + ylab(\"No.of\\nBooks\") +\n  labs(caption = \"Source: PISA 2022\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1), \n        legend.position = \"none\") +\n  geom_vline(aes(xintercept=median(Reading_Average)),\n             color=\"red\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) + \n  annotate(\"text\", x=690, y=0.75, label=paste(\"Overall Median:\", round(median(r$Reading_Average))), color = \"red\") +\n  geom_vline(aes(xintercept=mean(Reading_Average)),\n             color=\"blue\", \n             linetype=\"dotdash\", \n             linewidth=1,\n             alpha = 0.5) + \n  annotate(\"text\", x=420, y=0.75, label=paste(\"Overall Mean:\", round(mean(r$Reading_Average))), color = \"blue\")\n\n\n\n\n\nNote: The same makeover approach is also applicable to Section 4.3.1 (Performance Distribution by Highest Level of Schooling of Parents) as it also uses density plots.\n\n\n\n\n\n\nIn conclusion, the ggplot2 package is a powerful package for exploratory data analysis through visualisation. However, it is insufficient if effort is not made to select, clean, categorise, and moderate the data properly, and the visualisation process does not take into account information design, visual encoding, and interface design.\nThe step-by-step process of a data visualisation makeover in this take-home exercise highlights the importance of paying attention to details when making a plot. A personal checklist of important points to consider that are elucidated from the makeover attempts above are summarised as follows:\n\nDoes the plot have a clear key message?\nIs the correct type of chart used?\nIs the plot telling a lie? Are the scales of the axes distorted?\nAre the components of the graph (titles, axes, labels, tick marks, legend, grid lines, caption) accurate, relevant, and easy-to-view, or are they erroneous, confusing or redundant?\nIs annotation required?\nAre colours and fonts used appropriately in the plot?\nTake note of data-ink by reducing non-data-ink, and enhancing data-ink. The use of an appropriate theme helps!\nAim for a Quadrant I plot that is clear and beautiful!\n\n\n\n\n\nR for Visual Analytics.\nR for Data Science.\nFundamentals of Data Visualisation.\nPISA 2022 Database and Technical Report.\n\n~~~ End of Take-home Exercise 2 ~~~"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#introduction",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#introduction",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "Singapore has come a long way since gaining independence in 1965. Without natural resources, human capital development through a well-planned education system is a critical part of Singapore’s transformation from third world to first.\nDespite the success, there is still a correlation between socio-economic status and education achievement, as well as ingrained perceptions that some schools are better than others.\nHence, there is a need to use data to analyse the performance of Singapore students across different subjects, and identify any relationships between the performance in various subjects and factors such as gender, socioeconomic status, and type of school.\n\n\n\nIn this take-home exercise, the objective is to improve on an original visualisation by a fellow course-mate by focusing on what works, what does not work, why they do not work, and how to make it better.\nThe specific tasks are:\n\nSelect a Take-home Exercise 1 prepared by a course-mate;\nCritic the submission in terms of clarity and aesthetics;\nPrepare a sketch for the alternative design using the data visualisation design principles and best practices from Lesson 1 and Lesson 2; and\nRemake the original design using ggplot2, ggplot2 extensions, and tidyverse packages."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#getting-started",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "The R packages used in this take-home exercise are:\n\nhaven for importing SAS files;\ntidyverse (i.e. readr, tidyr, dplyr, ggplot2) for performing data science tasks such as importing, tidying, and wrangling data, as well as creating graphics based on The Grammar of Graphics;\nreshape2 for transforming data between wide and long formats;\nggthemes for extra themes, geoms, and scales for ggplot2;\nggridges for creating ridgeline plots;\nggdist for visualising distributions and uncertainty; and\nggpubr for creating publication ready ggplot2 plots.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed in the computer. If yes, they are then loaded into the R environment. If no, they are installed, and then loaded into the R environment.\n\npacman::p_load(haven, tidyverse, reshape2,\n               ggthemes, ggridges, ggdist,\n               ggpubr)\n\n\n\n\nThe OECD Programme for International Student Assessment (PISA) measures how well 15-year-old students in different countries are “prepared to meet the challenges of today’s knowledge societies” by looking at “their ability to use their knowledge and skills to meet real-life challenges”. The PISA surveys take place very three years, the latest being conducted in 2022.\nThe PISA 2022 database contains the full set of responses from individual students, school principals, and parents. There are a total of five data files and their contents are as follows:\n\nStudent questionnaire data file;\nSchool questionnaire data file;\nTeacher questionnaire data file;\nCognitive item data file; and\nQuestionnaire timing data file.\n\nFor the purpose of this take-home exercise, the “Student questionnaire data file” is used."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-wrangling",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "The dataset used in this take-home exercise is the 2022 PISA student questionnaire data file, cy08msp_stu_qqq.sas7bdat, which is in the SAS file format.\nThe file is imported into the R environment using the read_sas() function in the haven package and stored as the R object, stu_qqq.\n\nstu_qqq = read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\nThe tibble data frame, stu_qqq, has 1,279 columns (variables) and 613,744 rows (observations).\n\n\n\nThere are 6,606 rows with the country code (i.e., CNT) value of “SGP”, which represents Singapore. This count is cross-verified by the information provided in the “CY08MSP_STU_QQQ” sheet in the codebook. The codebook also stated that Singapore students’ made up 1.0763% of the entire global student population who took part in the 2022 PISA.\nThe filter() function in the dplyr package is used to obtain these rows, and stored as the R object, stu_qqq_SG.\n\nstu_qqq_SG = stu_qqq %&gt;% filter(CNT == \"SGP\")\n\nThe tibble data frame, stu_qqq_SG, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(stu_qqq_SG, \"data/stu_qqq_SG.rds\")\n\n\nstu_qqq_SG = read_rds(\"data/stu_qqq_SG.rds\")\n\n\n\n\nUsing the chosen submission, the dataset is prepared according to the original submission.\n\n\nCode\n#  PREPARE MAIN DATA-FRAME\nstudent_columns = \"CNTSTUID\"\ngender_columns = \"ST004D01T\"\nschool_columns = \"CNTSCHID\"\n\neducation_column_mother = \"ST005Q01JA\"\neducation_column_father = \"ST007Q01JA\"\n\ntraining_column_mother = \"ST006Q01JA\"\ntraining_column_father = \"ST008Q01JA\"\npossession_room_column = \"ST250Q01JA\"\npossession_computer_column = \"ST250Q02JA\"\npossession_software_column = \"ST250Q03JA\"\npossession_phone_column = \"ST250Q04JA\"\npossession_internet_column = \"ST250Q05JA\"\npossession_book_column = \"ST255Q01JA\"\n\nmath_columns = c(\"PV1MATH\", \"PV2MATH\", \"PV3MATH\", \"PV4MATH\", \"PV5MATH\", \"PV6MATH\", \"PV7MATH\", \"PV8MATH\", \"PV9MATH\", \"PV10MATH\")\nreading_columns = c(\"PV1READ\", \"PV2READ\", \"PV3READ\", \"PV4READ\", \"PV5READ\", \"PV6READ\", \"PV7READ\", \"PV8READ\", \"PV9READ\", \"PV10READ\")\nscience_columns = c(\"PV1SCIE\", \"PV2SCIE\", \"PV3SCIE\", \"PV4SCIE\", \"PV5SCIE\", \"PV6SCIE\", \"PV7SCIE\", \"PV8SCIE\", \"PV9SCIE\", \"PV10SCIE\")\n\nstudent_ID = stu_qqq_SG[, student_columns, drop = FALSE]\ngender = stu_qqq_SG[, gender_columns, drop = FALSE]\nschool_ID = stu_qqq_SG[, school_columns, drop = FALSE]\neducation_mother = stu_qqq_SG[, education_column_mother, drop = FALSE]\neducation_father = stu_qqq_SG[, education_column_father, drop = FALSE]\n\ntraining_mother = stu_qqq_SG[, training_column_mother, drop = FALSE]\ntraining_father = stu_qqq_SG[, training_column_father, drop = FALSE]\npossession_room = stu_qqq_SG[, possession_room_column, drop = FALSE]\npossession_computer = stu_qqq_SG[, possession_computer_column, drop = FALSE]\npossession_software = stu_qqq_SG[, possession_software_column, drop = FALSE]\npossession_phone = stu_qqq_SG[, possession_phone_column, drop = FALSE]\npossession_internet = stu_qqq_SG[, possession_internet_column, drop = FALSE]\npossession_book = stu_qqq_SG[, possession_book_column, drop = FALSE]\n\nmath_avg = rowMeans(stu_qqq_SG[, math_columns, drop = FALSE])\nreading_avg = rowMeans(stu_qqq_SG[, reading_columns, drop = FALSE])\nscience_avg = rowMeans(stu_qqq_SG[, science_columns, drop = FALSE])\n\nstu_df = data.frame(Student_ID = student_ID,\n  Gender = gender,\n  School_ID = school_ID,\n  Education_mother = education_mother,\n  Education_father = education_father,\n \n#  Training_mother = training_mother,\n#  Training_father = training_father,\n  Possession_room = possession_room,\n  Possession_computer = possession_computer,\n  Possession_software = possession_software,\n  Possession_phone = possession_phone,\n  Possession_internet = possession_internet,\n  Possession_book = possession_book,\n  \n  Math_Average = round(math_avg,digits=2),\n  Reading_Average = round(reading_avg,digits=2),\n  Science_Average = round(science_avg,digits=2),\nAverage_score=round(((math_avg+reading_avg+science_avg)/3),digits=2))\n\nnames(stu_df) = c(\"Student_ID\",\"Gender\",\"School_ID\",\"Education_mother\",\n                   \"Education_father\",\"Possession_room\",\"Possession_computer\",\n                   \"Possession_software\",\"Possession_phone\",\n                   \"Possession_internet\",\"Possession_book\",\"Math_Average\",\n                   \"Reading_Average\",\"Science_Average\",\"Average_Score\")\n\n\nThe finalised tibble data frames, stu_df is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(stu_df, \"data/stu_df.rds\")\n\n\nstu_df = read_rds(\"data/stu_df.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-visualisation-makeover",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-visualisation-makeover",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "The original visualisation at Section 3 (Distribution of Singapore Students’ Performance) of the submission is reproduced below. The plot on the average score (not specific subjects) is chosen.\n\n\n\n\n\nThe observation made:\n\n“In this case, all the histogram are left skewed. The left-skewed (negative skewness) nature of the histogram indicates that there is a concentration of scores towards the higher end, with a tail extending towards lower scores. The mean is influenced by these lower scores, pulling it towards the left. The median, being less sensitive to extreme values, is positioned higher in the distribution.”\n\n\n\n\nClarity\n\n😊 Right Visualisation. The use of a histogram provides a clear representation of the distribution of the average scores among the students, and the use of the mean and median vertical dashed lines shows the left-skewness of the distribution.\n😕 Missing Key Message in Title. The key message stated in the observation is not included in the title (which is factual). Stating the key message in the title would help the reader zoom in on the intended message of the plot.\n😕 Missing Source Caption. It would be useful to credit the source of the data directly in the plot to give the reader the appropriate context for interpreting plot.\n\nAesthetics\n\n😊 Graphical Integrity Maintained. The y-axis starts from zero, ensuring that the histogram is not distorted.\n😊 Simple Colours & Font Used. The plot uses an easy-to-read font and colour. However, this can be enhanced by using a professional theme.\n😕 Overcrowding of Details & Labels. The x-axis labels use values that are too detailed (not rounded up). Also, each column of the histogram is labelled with the corresponding frequency for that range of scores. These details are overwhelming and distract from the intended message. The placement of the mean and median values can also be better adjusted to avoid overlapping with the histogram. The decimal values in the mean and median may also be removed for simplicity to improve readability given that the difference in the two values is more than 1 point.\n😕 Orientation of Label. The y-axis title can be rotated for easier reading.\n\n\n\n\nThe sketch of the alternative design is as follows:\n\n\n\n\n\n\n\n\nAn improved plot is made as follows:\n\nImproved PlotCode\n\n\n\n\n\n\n\n\n\n\naverage_sci = mean(stu_df$Average_Score, na.rm = TRUE)\nmedian_sci = median(stu_df$Average_Score, na.rm = TRUE)\n\nggplot(stu_df, aes(x = Average_Score)) +\n  geom_histogram(bins = 20, \n                 fill = \"cadetblue3\", \n                 color = \"grey95\") +\n  labs(title = \"More Students Score Above Mean Average Score\",\n       subtitle = \"Distribution of Average Scores\",\n       x = \"Average Score\",\n       y = \"Frequency\", \n       caption = \"Source: PISA 2022\") +\n  theme_minimal() +\n  scale_x_continuous(n.breaks = 10) +\n  scale_y_continuous(n.breaks = 6) +\n  geom_vline(xintercept = average_sci, color = \"navy\", linetype = \"dashed\", size = 1) +\n  geom_text(aes(x = average_sci-60, y = 900, label = paste(\"Mean:\", round(average_sci))), vjust = 2, color = \"navy\",size =4) +\n  geom_vline(xintercept = median_sci, color = \"maroon\", linetype = \"dotdash\", size = 1) +\n  geom_text(aes(x = median_sci+100, y = 900, label = paste(\"Median:\", round(median_sci))), vjust = 2, color = \"maroon\",size=4) +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\n\n\n\n\n\n\nThe original visualisation at Section 4.1 (Performance by Gender) of the submission is reproduced below. Again, the plot on the average score (not specific subjects) is chosen.\n\n\n\n\n\nThe relevant observations made:\n\n“From the above violin plots, we can conclude:\n\nMale plot is more spread out, which indicates greater variability in scores within the male group\nFemale group and male group have similar average performance.\nAlmost all the distribution are left skewed, indicating a concentration of students with relatively higher scores, but a few students have much lower scores.”\n\n\n\n\n\nClarity\n\n😊😕 (Almost) Right Visualisation. The use of two violin plots shows the distributions of the average scores for female and male students. The use of the box plots in the middle of the violin plots shows the median average scores. However, as the two violin plots do not overlap, the comparison of average scores based on gender is more difficult to make visually. A density plot grouped by gender, with vertical lines added to show the median average scores, may be more appropriate for comparison purposes.\n😕 Missing Key Message in Title. The key message stated in the observation is not included in the title (which is factual). Stating the key message in the title would help the reader zoom in on the intended message of the plot.\n😕 Missing Source Caption. It would be useful to credit the source of the data directly in the plot to give the reader the appropriate context for interpreting plot.\n\nAesthetics\n\n😊 Simple Colours & Font Used. The plot uses an easy-to-read font and the stereotypical colours associated with each gender (pink for female; blue for male). However, this can be enhanced by using a simpler/minimalist theme for the background to keep the focus on the coloured density plots.\n😕 Redundant Legend. The legend is redundant given that the x-axis is labelled.\n😕 Unclear Labels for Mean and Median Values. The subtitle states the colours corresponding to the mean and median values for each gender which requires the reader to read and match accordingly, and reconfirm each time he/she views the plot. This is less intuitive than putting the labels together with the values.\n😕 Orientation of Label. The y-axis title can be rotated for easier reading.\n😕 Mis-titled Y-axis. The y-axis title was mistakenly labelled as “Reading Score” instead of “Average Score”.\n\n\n\n\nThe sketch of the alternative design is as follows:\n\n\n\n\n\n\n\n\nAn improved plot is made as follows:\n\nImproved PlotCode\n\n\n\n\n\n\n\n\n\n\nstu_df$Gender = ifelse(\n  stu_df$Gender == 1, \n  \"Female\", \"Male\")\n\nf = stu_df %&gt;%\n  filter(Gender == \"Female\")\n\nm = stu_df %&gt;%\n  filter(Gender == \"Male\")\n\nstu_df = stu_df %&gt;%\n  mutate(Gender = fct_relevel(Gender, \n                               \"Female\", \n                               \"Male\"))\n\nggplot(stu_df,\n       aes(x = Average_Score,\n           fill = Gender)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(aes(xintercept=median(f$Average_Score)),\n             color=\"red\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) +\n  geom_vline(aes(xintercept=median(m$Average_Score)),\n             color=\"blue\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) +\n  ggtitle(label = \"On Average, Boys and Girls Perform Similarly\",\n          subtitle= \"Distribution of Average Scores by Gender\") + \n  labs(caption = \"Source: PISA 2022\") +\n  ylab(\"Density\") + xlab(\"Average Score\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1)) +\n  theme_minimal() +\n  geom_vline(aes(xintercept=median(f$Average_Score)),\n             color=\"#CC3366\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) +\n  geom_vline(aes(xintercept=median(m$Average_Score)),\n             color=\"#033336\", \n             linetype=\"dotdash\", \n             linewidth=1,\n             alpha = 0.5) +\n  geom_text(aes(x = 690, y = 0.0045, label = paste(\"Median:\", round(median(m$Average_Score)))), vjust = 2, color = \"#033336\",size =4) +\n  geom_text(aes(x = 480, y = 0.0045, label = paste(\"Median:\", round(median(f$Average_Score)))), vjust = 2, color = \"#CC3366\",size=4) +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1))\n\n\n\n\n\nNote: The same makeover approach is also applicable to Section 4.3.3 (Performance Distribution by Internet Access) as it also uses violin plots.\n\n\n\n\n\n\n\nThe original visualisation at Section 4.3.2 (Distribution of by Number of Book in Students’ Home) of the submission is reproduced below. The plot on the reading score is chosen.\n\n\n\n\n\nThe observation made:\n\n“From the density plot, we can interpret that:\n\nFor students with no books in their homes, the density plot is right-skewed. This suggests that a lack of books is associated with a concentration of lower academic scores. The right skewness indicates that the majority of students in this group may have below-average scores.\nAs the number of books in the home increases, the density plot becomes more left-skewed. This trend suggests a positive correlation between the abundance of books and higher academic performance. The left skewness indicates a concentration of higher scores, with more students performing above the average.\nThe observation that the mean score of students increases as the number of books in the home increases aligns with the general trend of a left-skewed density plot. This indicates that, on average, students with access to a greater number of books tend to achieve higher academic scores.\nA notable deviation from the general trend occurs for students whose homes have “more than 500 books.” In this category, the mean score decreases, contrary to the overall positive relationship observed. This suggests that there may be diminishing returns in terms of academic performance when the number of books surpasses a certain threshold.\n\nIn summary, the density plot illustrates a positive association between the number of books in the home and student performance. However, the deviation observed for the “more than 500 books” category suggests a nuanced relationship, highlighting the need to consider optimal conditions for leveraging the positive influence of books on academic outcomes.”\n\n\n\n\nClarity\n\n😊😕 (Almost) Right Visualisation. The use of density plots, with vertical lines showing the median reading scores, is useful for comparing the distributions of reading scores based on the number of books at home. However, this can be further enhanced with the use of a ridgelines plot.\n😕 Missing Key Message in Title. The key message stated in the observation is not included in the title (which is factual). Stating the key message in the title would help the reader zoom in on the intended message of the plot.\n😕 Difficult to Compare Distributions. Given that there are many categories, it is difficult to compare the distributions. It may be useful to showcase the quartile cut-offs to allow for better comparisons across different density plots.\n😕 Missing Source Caption. It would be useful to credit the source of the data directly in the plot to give the reader the appropriate context for interpreting plot.\n\nAesthetics\n\n😕 Unclear Colours Used. The plot uses an easy-to-read font. However, the choice of gradient colours is not appropriate as the answers form a categorical, not continuous, variable. The use of gradient colours does not allow the reader to differentiate the distinct categories of answers to the question in the questionnaire on the number of books at home.\n😕 Lack of Labels for Categories. The categories representing different ranges of the number of books at home requires a separate table for interpretation. This inconveniences the reader. Hence, it would be better to label each density plot directly for easier identification.\n😕 No Labels for Mean Values. The values of the dashed vertical lines are not labelled. It may be useful to label some of them to allow for direct comparison. Alternatively, to prevent clutter, it may be useful to add just two vertical lines - overall mean and overall median for broad comparisons.\n😕 Orientation of Label. The y-axis title can be rotated for easier reading.\n\n\n\n\nThe sketch of the alternative design is as follows:\n\n\n\n\n\n\n\n\nAn improved plot is made as follows:\n\nImproved PlotCode\n\n\n\n\n\n\n\n\n\n\nstu_df$Possession_book = as.character(stu_df$Possession_book)\nstu_df = stu_df %&gt;% \n  mutate(Possession_book = recode(Possession_book,\n                        \"1\" = \"0 Book\",\n                        \"2\" = \"1-10 Books\",\n                        \"3\" = \"11-25 Books\",\n                        \"4\" = \"26-100 Books\",\n                        \"5\" = \"101-200 Books\",\n                        \"6\" = \"201-500 Books\",\n                        \"7\" = \"&gt;500 Books\"))\n\nr = stu_df %&gt;% \n  select(Possession_book, Reading_Average) %&gt;%\n  na.omit() %&gt;%\n  mutate(Books = fct_relevel(Possession_book, \n                              \"0 Book\", \n                              \"1-10 Books\",\n                              \"11-25 Books\",\n                              \"26-100 Books\",\n                              \"101-200 Books\",\n                              \"201-500 Books\",\n                              \"&gt;500 Books\"))\n                        \nggplot(r, \n       aes(x = Reading_Average, \n           y = Books,\n           fill = factor(after_stat(quantile)))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 2,\n    quantile_lines = TRUE) +\n  scale_fill_brewer(type = \"seq\", palette = \"Set3\") +\n  ggtitle(label = \"More Books at Home Generally Correspond to\\nHigher Reading Scores\",\n          subtitle = \"Distribution of Reading Scores vis-a-vis Number of Books at Home\") +\n  xlab(\"Score\") + ylab(\"No.of\\nBooks\") +\n  labs(caption = \"Source: PISA 2022\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1), \n        legend.position = \"none\") +\n  geom_vline(aes(xintercept=median(Reading_Average)),\n             color=\"red\", \n             linetype=\"dashed\", \n             linewidth=1,\n             alpha = 0.5) + \n  annotate(\"text\", x=690, y=0.75, label=paste(\"Overall Median:\", round(median(r$Reading_Average))), color = \"red\") +\n  geom_vline(aes(xintercept=mean(Reading_Average)),\n             color=\"blue\", \n             linetype=\"dotdash\", \n             linewidth=1,\n             alpha = 0.5) + \n  annotate(\"text\", x=420, y=0.75, label=paste(\"Overall Mean:\", round(mean(r$Reading_Average))), color = \"blue\")\n\n\n\n\n\nNote: The same makeover approach is also applicable to Section 4.3.1 (Performance Distribution by Highest Level of Schooling of Parents) as it also uses density plots."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#conclusion",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "In conclusion, the ggplot2 package is a powerful package for exploratory data analysis through visualisation. However, it is insufficient if effort is not made to select, clean, categorise, and moderate the data properly, and the visualisation process does not take into account information design, visual encoding, and interface design.\nThe step-by-step process of a data visualisation makeover in this take-home exercise highlights the importance of paying attention to details when making a plot. A personal checklist of important points to consider that are elucidated from the makeover attempts above are summarised as follows:\n\nDoes the plot have a clear key message?\nIs the correct type of chart used?\nIs the plot telling a lie? Are the scales of the axes distorted?\nAre the components of the graph (titles, axes, labels, tick marks, legend, grid lines, caption) accurate, relevant, and easy-to-view, or are they erroneous, confusing or redundant?\nIs annotation required?\nAre colours and fonts used appropriately in the plot?\nTake note of data-ink by reducing non-data-ink, and enhancing data-ink. The use of an appropriate theme helps!\nAim for a Quadrant I plot that is clear and beautiful!"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#key-references",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#key-references",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "R for Visual Analytics.\nR for Data Science.\nFundamentals of Data Visualisation.\nPISA 2022 Database and Technical Report.\n\n~~~ End of Take-home Exercise 2 ~~~"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/data/geospatial/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex07/data/geospatial/MPSZ-2019.html",
    "title": "ISSS608",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "title": "In-class Exercise 7",
    "section": "",
    "text": "An isohyet map is a surface map of some precipitation: rain, snow, and others.\nIn order to prepare an isohyet map, spatial interpolation will be used. Spatial interpolation is the process of using points with known values to estimate values at other unknown points. For example, to make a rainfall above, we will not find enough evenly spread weather stations to cover the entire region. Spatial interpolation can estimate the rainfall at locations without recorded data by using known rainfall readings at nearby weather stations (see figure_temperature_map). This type of interpolated surface is often called a geostatistical surface. Elevation data, temperature, property prices, air quality index and population density are other types of data that can be computed using interpolation.\nThere are many interpolation methods. In this in-class exercise, two widely used spatial interpolation methods called Inverse Distance Weighting (IDW) and kriging will be introduced.\n\n\n\n\n\nIn this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nsf for importing, managing, and processing geospatial data;\nterra for spatial data analysis (a replacement of the raster package. It has a very similar, but simpler, interface, and it is faster than raster. In this hands-on exercise, it will be used to create grid (also known as raster) objects as the input and output of spatial interpolation);\ngstat for spatial and spatio-temporal geostatistical modelling, prediction and simulation.\ntmap for thematic maps;\nautomap for performing automatic variogram modelling and kriging interpolation; and\nviridis for colour-blind friendly colour maps.\n\n\npacman::p_load(sf, terra, gstat, automap,\n               tmap, viridis, tidyverse)\n\n\n\n\nThree data sets will be used in this exercise, they are:\n\nRainfallStation.csv provides location information of existing rainfall stations in Singapore. The data is downloaded from Meteological Service Singapore.\nDAILYDATA_202402.csv provides weather data are rainfall stations for the month February, 2024. The data is also downloaded from Meteological Service Singapore.\nMPSZ-2019 contains planning subzone boundary of URA Master Plan 2019. It is downloaded from data.gov.sg. The original data is in kml format.\n\n\n\nThe dataset with name, latitude, and longitude of each weather station is imported using the read_csv() function in the readr package.\n\nrfstations = read_csv(\"data/aspatial/RainfallStation.csv\")\n\n\n\n\nThe dataset with the daily rainfall data is imported using the read_csv() function in the readr package, and processed using the select(), group_by(), summarise() and ungroup() functions in the dplyr package to obtain the total monthly rainfall by station.\n\nrfdata = read_csv(\"data/aspatial/DAILYDATA_202402.csv\") %&gt;%\n  select(c(1,5)) %&gt;%\n  group_by(Station) %&gt;%\n  summarise(MONTHSUM = sum(`Daily Rainfall Total (mm)`)) %&gt;%\n  ungroup()\n\n\n\n\nThe two datasets are then combined using the left_join() function in the dplyr package by the names of the weather stations.\n\nrfdata = rfdata %&gt;%\n  left_join(rfstations)\n\nThe combined dataset is then converted to an simple feature object using the st_as_sf() function in the sf package. The longitudes and latitudes are inserted as sf geometry points in the dataset. The st_transform() function in the sf package is used to transform the coordinate system (from wgs48 to svy21 projected coordinates system). 3414 is the EPSG code of svy21, which is the official projected coordinates of Singapore.\n\nrfdata_sf = st_as_sf(rfdata,\n                     coords = c(\"Longitude\",\n                                \"Latitude\"),\n                     crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n\n\nThe spatial shapefile, MPSZ-2019, is then imported using the st_read() function in the sf package and transformed using the st_transform() function in the sf package.\n\nmpsz2019 = st_read(dsn = \"data/geospatial\",\n                   layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\jmphosis\\ISSS608\\In-class_Ex\\In-class_Ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\nIt is always a good practice to visualise the data prepared. In the code chunk below, tmap functions are used to create a dot map showing locations of rainfall station in Singapore.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\ntm_shape(rfdata_sf) +\n  tm_dots(col = \"red\")\n\n\n\n\n\ntmap_mode(\"plot\")\n\nThe functions in the tmap package are then used to plot the total monthly rainfall by weather stations based on their geographical locations.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\ntm_shape(mpsz2019) +\n  tm_borders() +\n  tm_shape(rfdata_sf) +\n  tm_dots(col = 'MONTHSUM')\n\n\n\n\n\ntmap_mode(\"plot\")\n\n\n\n\nIn order to perform spatial interpolation using the gstat package, we first need to create an object of class called gstat, using a function of the same name: gstat(). A gstat object contains all necessary information to conduct spatial interpolation, namely:\n\nThe model definition\nThe calibration data\n\nBased on its arguments, the gstat function “understands” what type of interpolation model we want to use:\n\nNo variogram model → IDW\nVariogram model, no covariates → Ordinary Kriging\nVariogram model, with covariates → Universal Kriging\n\nThe complete decision tree of the gstat() function is shown in the figure below.\n\n\n\nTo get start, we need to create a grid data object using the rast() function in the terra package.\nNext, a list called xy will be created using the xyFromCell() function in the terra package. The function gets coordinates of the center of raster cells for a row, column, or cell number of a SpatRaster. Or get row, column, or cell numbers from coordinates or from each other.\n\ngrid = terra::rast(mpsz2019,\n                   nrows = 690,\n                   ncols = 1075)\n\nxy = terra::xyFromCell(grid,\n                       1:ncell(grid))\n\nThen, a data frame, coop, is created with prediction/simulation locations.\n\ncoop = st_as_sf(as.data.frame(xy), \n                 coords = c(\"x\", \"y\"),\n                 crs = st_crs(mpsz2019))\n\ncoop = st_filter(coop, mpsz2019)\nhead(coop)\n\nSimple feature collection with 6 features and 0 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 25883.42 ymin: 50231.33 xmax: 26133.32 ymax: 50231.33\nProjected CRS: SVY21 / Singapore TM\n                   geometry\n1 POINT (25883.42 50231.33)\n2  POINT (25933.4 50231.33)\n3 POINT (25983.38 50231.33)\n4 POINT (26033.36 50231.33)\n5 POINT (26083.34 50231.33)\n6 POINT (26133.32 50231.33)\n\n\n\n\n\nIn the IDW interpolation method, the sample points are weighted during interpolation such that the influence of one point relative to another declines with distance from the unknown point you want to create.\nWeighting is assigned to sample points through the use of a weighting coefficient that controls how the weighting influence will drop off as the distance from new point increases. The greater the weighting coefficient, the less the effect points will have if they are far from the unknown point during the interpolation process. As the coefficient increases, the value of the unknown point approaches the value of the nearest observational point.\nIt is important to notice that the IDW interpolation method also has some disadvantages: the quality of the interpolation result can decrease, if the distribution of sample data points is uneven. Furthermore, maximum and minimum values in the interpolated surface can only occur at sample data points. This often results in small peaks and pits around the sample data points.\nThree parameters of the gstat() function are used:\n\nformula: The prediction “formula” specifying the dependent and the independent variables (covariates)\ndata: The calibration data\nmodel: The variogram model\n\nWe need to specify parameter names, because these three parameters are not the first three in the function definition.\n\nNote: In R, formula objects are used to specify relation between objects, in particular—the role of different data columns in statistical models. A formula object is created using the ~ operator, which separates names of dependent variables (to the left of the ~ symbol) and independent variables (to the right of the ~ symbol). Writing 1 to the right of the ~ symbol, as in ~ 1, means that there are no independent variables\n\n\nres = gstat(formula = MONTHSUM ~ 1, \n             locations = rfdata_sf, \n             nmax = 5,\n             set = list(idp = 0))\n\nNow that the model is defined, the predict() function is used to interpolate, i.e., to calculate predicted values. The predict function accepts:\n\nA raster—stars object, such as dem\nA model—gstat object, such as g\n\nThe raster serves for two purposes:\n\nSpecifying the locations where we want to make predictions (in all methods), and\nSpecifying covariate values (in Universal Kriging only).\n\n\nresp = predict(res, coop)\n\n[inverse distance weighted interpolation]\n\nresp$x = st_coordinates(resp)[,1]\nresp$y = st_coordinates(resp)[,2]\nresp$pred = resp$var1.pred\n\npred = terra::rasterize(resp, grid, \n                         field = \"pred\", \n                         fun = \"mean\")\n\nThen, the interpolated surface is mapped using tmap functions.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(pred) + \n  tm_raster(alpha = 0.6, \n            palette = \"viridis\")\n\n\n\n\n\n\n\nKriging is one of several methods that use a limited set of sampled data points to estimate the value of a variable over a continuous spatial field. An example of a value that varies across a random spatial field might be total monthly rainfall over Singapore. It differs from Inverse Distance Weighted Interpolation discussed earlier in that it uses the spatial correlation between sampled points to interpolate the values in the spatial field: the interpolation is based on the spatial arrangement of the empirical observations, rather than on a presumed model of spatial distribution. Kriging also generates estimates of the uncertainty surrounding each interpolated value.\nIn a general sense, the kriging weights are calculated such that points nearby to the location of interest are given more weight than those farther away. Clustering of points is also taken into account, so that clusters of points are weighted less heavily (in effect, they contain less information than single points). This helps to reduce bias in the predictions.\nThe kriging predictor is an “optimal linear predictor” and an exact interpolator, meaning that each interpolated value is calculated to minimize the prediction error for that point. The value that is generated from the kriging process for any actually sampled location will be equal to the observed value at this point, and all the interpolated values will be the Best Linear Unbiased Predictors (BLUPs).\nKriging will in general not be more effective than simpler methods of interpolation if there is little spatial autocorrelation among the sampled data points (that is, if the values do not co-vary in space). If there is at least moderate spatial autocorrelation, however, kriging can be a helpful method to preserve spatial variability that would be lost using a simpler method (for an example, see Auchincloss 2007, below).\nKriging can be understood as a two-step process:\n\nfirst, the spatial covariance structure of the sampled points is determined by fitting a variogram; and\nsecond, weights derived from this covariance structure are used to interpolate values for unsampled points or blocks across the spatial field.\n\nKriging methods require a variogram model. A variogram (sometimes called a “semivariogram”) is a visual depiction of the covariance exhibited between each pair of points in the sampled data. For each pair of points in the sampled data, the gamma-value or “semi-variance” (a measure of the half mean-squared difference between their values) is plotted against the distance, or “lag”, between them. The “experimental” variogram is the plot of observed values, while the “theoretical” or “model” variogram is the distributional model that best fits the data.\n\nThe empirical variogram is calcualted using the variogram() function in the gstat package. The function requires two arguments:\n\nformula, the dependent variable and the covariates; and\ndata, a point layer with the dependent variable and covariates as attributes.\n\n\nv = variogram(MONTHSUM ~ 1, \n               data = rfdata_sf)\nplot(v)\n\n\n\n\nThe theoretical plots re then compared:\n\nWith reference to the comparison above, an empirical variogram model will be fitted using the fit.variogram()function in the gstat package.\n\nfv = fit.variogram(object = v,\n                    model = vgm(\n                      psill = 0.5, \n                      model = \"Sph\",\n                      range = 5000, \n                      nugget = 0.1))\nfv\n\n  model     psill    range\n1   Nug 0.1129190    0.000\n2   Sph 0.5292397 5213.396\n\n\nWe can visualise how well the observed data fit the model by plotting fv.\n\nplot(v, fv)\n\n\n\n\nThe plot above reveals that the empirical model fits rather well. In view of this, spatial interpolation is performed using the newly derived model.\n\nk = gstat(formula = MONTHSUM ~ 1, \n           data = rfdata_sf, \n           model = fv)\nk\n\ndata:\nvar1 : formula = MONTHSUM`~`1 ; data dim = 43 x 2\nvariograms:\n        model     psill    range\nvar1[1]   Nug 0.1129190    0.000\nvar1[2]   Sph 0.5292397 5213.396\n\n\nOnce done, the predict() function in the gstat package will be used to estimate the unknown grids.\n\nresp = predict(k, coop)\n\n[using ordinary kriging]\n\nresp$x = st_coordinates(resp)[,1]\nresp$y = st_coordinates(resp)[,2]\nresp$pred = resp$var1.pred\nresp$pred = resp$pred\nresp\n\nSimple feature collection with 314019 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2692.528 ymin: 15773.73 xmax: 56371.45 ymax: 50231.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   var1.pred  var1.var                  geometry        x        y     pred\n1   131.0667 0.6608399 POINT (25883.42 50231.33) 25883.42 50231.33 131.0667\n2   130.9986 0.6610337  POINT (25933.4 50231.33) 25933.40 50231.33 130.9986\n3   130.9330 0.6612129 POINT (25983.38 50231.33) 25983.38 50231.33 130.9330\n4   130.8698 0.6613782 POINT (26033.36 50231.33) 26033.36 50231.33 130.8698\n5   130.8092 0.6615303 POINT (26083.34 50231.33) 26083.34 50231.33 130.8092\n6   130.7514 0.6616697 POINT (26133.32 50231.33) 26133.32 50231.33 130.7514\n7   130.6965 0.6617971  POINT (26183.3 50231.33) 26183.30 50231.33 130.6965\n8   130.6446 0.6619131 POINT (26233.28 50231.33) 26233.28 50231.33 130.6446\n9   130.5958 0.6620184 POINT (26283.26 50231.33) 26283.26 50231.33 130.5958\n10  132.5484 0.6542154 POINT (25033.76 50181.32) 25033.76 50181.32 132.5484\n\n\nIn order to create a raster surface data object, the rasterize() function in the terra package is used.\n\nkpred = terra::rasterize(resp, grid, \n                         field = \"pred\")\nkpred\n\nclass       : SpatRaster \ndimensions  : 690, 1075, 1  (nrow, ncol, nlyr)\nresolution  : 49.98037, 50.01103  (x, y)\nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncoord. ref. : SVY21 / Singapore TM (EPSG:3414) \nsource(s)   : memory\nname        :      last \nmin value   :  72.77826 \nmax value   : 195.53284 \n\n\nThe output object, kpred, is in SpatRaster object class with a spatial resolution of 50m x 50m. It consists of 1,075 columns and 690 rows and in SVY21 projected coordinates system.\n\n\n\nFinally, the tmap functions are used to map the interpolated rainfall raster (i.e. kpred).\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(kpred) + \n  tm_raster(alpha = 0.6, \n            palette = \"viridis\",\n            title = \"Total monthly rainfall (mm)\") +\n  tm_layout(main.title = \"Distribution of monthly rainfall, Feb 2024\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)\n\n\n\n\n\n\n\nBeside using the gstat package to perform variogram modelling manually, the autofirVariogram() function in the automap package can be used to perform varigram modelling.\n\nv_auto = autofitVariogram(MONTHSUM ~ 1, \n                           rfdata_sf)\nplot(v_auto)\n\n\n\n\n\nv_auto\n\n$exp_var\n   np      dist     gamma dir.hor dir.ver   id\n1  15  1957.436  311.9613       0       0 var1\n2  33  3307.349  707.7685       0       0 var1\n3  54  4861.368  848.1314       0       0 var1\n4 116  6716.531  730.3969       0       0 var1\n5 111  9235.708 1006.5381       0       0 var1\n6 120 11730.199 1167.5988       0       0 var1\n7 135 14384.636 1533.5903       0       0 var1\n\n$var_model\n  model    psill   range kappa\n1   Nug     0.00       0   0.0\n2   Ste 24100.71 1647955   0.3\n\n$sserr\n[1] 0.2178294\n\nattr(,\"class\")\n[1] \"autofitVariogram\" \"list\"            \n\n\n\nk = gstat(formula = MONTHSUM ~ 1, \n           model = v_auto$var_model,\n           data = rfdata_sf)\nk\n\ndata:\nvar1 : formula = MONTHSUM`~`1 ; data dim = 43 x 2\nvariograms:\n        model    psill   range kappa\nvar1[1]   Nug     0.00       0   0.0\nvar1[2]   Ste 24100.71 1647955   0.3\n\n\n\nresp = predict(k, coop)\n\n[using ordinary kriging]\n\nresp$x = st_coordinates(resp)[,1]\nresp$y = st_coordinates(resp)[,2]\nresp$pred = resp$var1.pred\nresp$pred = resp$pred\n\nkpred = terra::rasterize(resp, grid, \n                         field = \"pred\")\n\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(kpred) + \n  tm_raster(alpha = 0.6, \n            palette = \"viridis\",\n            title = \"Total monthly rainfall (mm)\") +\n  tm_layout(main.title = \"Distribution of monthly rainfall, Feb 2024\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)\n\n\n\n\n\n\n\n\nOlea, Ricardo A. (2006-07) “A six-step practical approach to semivariogram modeling”, Stochastic Environmental Research and Risk Assessment, 2006-07, Vol.20 (5), p.307-318. SMU e-journal.\n~~~ End of In-class Exercise 7 ~~~"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "Climate change is the defining global issue of our time – the world faces an increasingly urgent need to mitigate its underlying causes and adapt to its far-reaching impacts. In this regard, Singapore aims to achieve net zero emissions by 2050.\nAt the same time, Singapore is not spared from the impact of climate change. According to the National Climate Change Secretariat (NCCS), the annual mean temperature increased by 1.1°C from 26.9°C to 28.0°C between 1980 and 2020, and annual total rainfall increased at an average rate of 6.7mm per year between 1980 and 2019. In the future, NCCS expects climate change to “lead to a temperature increase of 1.4°C to 4.6°C and a rise in sea level by up to about 1m by the end of the century”. Also, the “contrast between the wet months (November to January) and dry months (February and June to September) is likely to be more pronounced” and the “intensity and frequency of heavy rainfall events is expected to increase as the world gets warmer”.\n\n\n\n\nIn this take-home exercise, the objective is to use the appropriate interactive visualisation techniques to enhance user experience in the discovery of Singapore’s weather data.\nThe key analytical questions are:\n\nHow have the mean, maximum, and minimum temperatures changed over the years?\nHow have the daily rainfall and total rainfall changed over the years?\n\n\n\n\n\n\n\nThe R packages used in this take-home exercise are:\n\ntidyverse (i.e. readr, tidyr, dplyr, ggplot2) for performing data science tasks such as importing, tidying, and wrangling data, as well as creating graphics based on The Grammar of Graphics;\nreshape2 for transforming data between wide and long formats;\nggthemr for aesthetic themes created by user, Ciarán Tobin;\nggtext for improved text rendering support for ggplot2;\nggridges for creating ridgeline plots;\nggpubr for creating publication ready ggplot2 plots;\nplotly for plotting interactive statistical graphs; and\nggstatsplot for creating visual graphics with rich statistical information.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed in the computer. If yes, they are then loaded into the R environment. If no, they are installed, and then loaded into the R environment.\n\npacman::p_load(tidyverse, reshape2,\n               ggthemr, ggtext,\n               ggridges, ggpubr,\n               plotly, ggstatsplot)\n\nThe ggthemr() function in the ggthemr package is used to set the default theme of this take-home exercise as “solarized”.\n\nggthemr(\"solarized\")\n\n\n\n\nThe Meteorological Service Singapore (MSS) provides historical daily records of temperature or rainfall data. For this take-home exercise, the month of December is chosen for the analysis given that it coincides with the Northeast Monsoon that brings about higher rainfall, and consequently, cooler temperatures. The data are taken from 1983, 1993, 2003, 2013, and 2023 (spanning 40 years). The Changi weather station is chosen for the analysis due to the comprehensive weather data collected since 1981/1982, as well as its proximity to Changi airport, which could be affected by changes in weather patterns.\n\n\n\n\n\n\nThe five datasets (one for each year) used in this take-home exercise are downloaded from MSS’ website. They are in the CSV file format.\nThe files are imported into the R environment using the read_csv() function in the readr package and stored as the R objects, weather1983, weather1993, weather2003, weather2013, and weather2023.\n\nweather1983 = read_csv(\"data/DAILYDATA_S24_198312.csv\", locale=locale(encoding=\"latin1\"))\nweather1993 = read_csv(\"data/DAILYDATA_S24_199312.csv\", locale=locale(encoding=\"latin1\"))\nweather2003 = read_csv(\"data/DAILYDATA_S24_200312.csv\", locale=locale(encoding=\"latin1\"))\nweather2013 = read_csv(\"data/DAILYDATA_S24_201312.csv\", locale=locale(encoding=\"latin1\"))\nweather2023 = read_csv(\"data/DAILYDATA_S24_202312.csv\")\n\nEach of the tibble data frames has 13 columns (variables) and 31 rows (observations).\n\n\n\nThe rbind() function in the base package is used to combine the five tibble data frames into a single tibble data frame, weather.\n\nweather = rbind(weather1983, \n                weather1993, \n                weather2003, \n                weather2013,\n                weather2023)\n\nrm(weather1983, weather1993, weather2003,\n   weather2013, weather2023)\n\nThe single tibble data frame, weather, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(weather, \"data/weather.rds\")\n\n\nweather = read_rds(\"data/weather.rds\")\n\n\n\n\nThe select() function in the dplyr package and the colnames() function in the base package are then used to obtain and rename the relevant columns respectively.\n\nweather = weather %&gt;% \n  select(c(2,4,5,9,10,11))\n\nnames = c(\"Year\", \"Day\",\n          \"Daily_Rainfall\",\n          \"Mean_Temp\",\n          \"Max_Temp\",\n          \"Min_Temp\")\n\ncolnames(weather) = names\n\nrm(names)\n\nAlso, the as.factor() function in the base package is used to convert the “Year” variable from numerical to factor data type.\n\nweather$Year = as.factor(weather$Year)\n\n\n\n\nThe dataset from MSS is expected to be relatively clean. Nevertheless, due diligence checks for duplicates and missing values are still made to confirm the assumption.\nThe duplicated() function in the base package is used to check for duplicates in weather. There are no duplicates in the tibble data frame.\n\nweather[duplicated(weather), ]\n\n# A tibble: 0 × 7\n# ℹ 7 variables: Year &lt;fct&gt;, Day &lt;dbl&gt;, Daily_Rainfall &lt;dbl&gt;, Mean_Temp &lt;dbl&gt;,\n#   Max_Temp &lt;dbl&gt;, Min_Temp &lt;dbl&gt;, Diurnal_Temp_Range &lt;dbl&gt;\n\n\nThe colSums() function in the base package is used to check for missing values in weather. There are no missing values in the tibble data frame.\n\ncolSums(is.na(weather))\n\n              Year                Day     Daily_Rainfall          Mean_Temp \n                 0                  0                  0                  0 \n          Max_Temp           Min_Temp Diurnal_Temp_Range \n                 0                  0                  0 \n\n\n\n\n\nA new variable, “Diurnal_Temp_Range” (i.e., difference between maximum and minimum daily temperatures) is then derived by subtracting the minimum daily temperatures from the maximum daily temperatures for each row.\n\nweather$Diurnal_Temp_Range = weather$Max_Temp - weather$Min_Temp\n\nThe finalised tibble data frame, weather, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(weather, \"data/weather.rds\")\n\n\nweather = read_rds(\"data/weather.rds\")\n\n\n\n\n\nExploratory data analysis (EDA) is conducted on the temperature and rainfall variables to obtain a preliminary understanding of the dataset.\n\n\nThe geom_tile() function in the ggplot2 package is used to plot the calendar heatmaps of the temperature and rainfall variables.\nSelection of Technique: The calendar heatmap is used to depict the continuous numerical values (i.e., daily temperatures or rainfall) for multiple groups (i.e., years) in chronological order from Day 1 to Day 31 in December. It provides an interesting way to visualise the variation in values within the month and across the years. They are helpful for visualising temporal trends in a compact and intuitive manner.\nDesign Principles: An informative title is provided, followed by a factual subtitle. The unit of measurement (i.e., °C or mm) is also indicated. Different colours are used for the heatmaps for different variables.\n\nMax Daily TempMin Daily TempDiurnal Temp RangeMean Daily TempDaily RainfallCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn = 1:31\n\n#Max Daily Temp\nggplot(weather,         \n       aes(Day,             \n           Year,             \n           fill = Max_Temp)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Max Temp\\n(°C)\",\n                      low = \"#FFCCCC\",\n                      high = \"#FF0000\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"More Days in Dec with Higher Max Daily Temp in 2023 than 1983\",      \n       subtitle = \"Max Temp by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5),       \n        legend.title = element_text(size = 8),       \n        legend.text = element_text(size = 6)) +   \n  scale_x_discrete(limits = c(n))\n\n#Min Daily Temp\nggplot(weather,\n       aes(Day, \n           Year,             \n           fill = Min_Temp)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Min Temp\\n(°C)\",                     \n                      low = \"light blue\",                      \n                      high = \"dark blue\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"Less Days in Dec with Lower Min Daily Temp in 2023 than 1983\",      \n       subtitle = \"Min Temp by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5),       \n        legend.title = element_text(size = 8),       \n        legend.text = element_text(size = 6)) +   \n  scale_x_discrete(limits = c(n))\n\n#Diurnal Temp Range\nggplot(weather,         \n       aes(Day,             \n           Year,             \n           fill = Diurnal_Temp_Range)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Diurnal Temp\\nRange (°C)\",                     \n                      low = \"#99CC99\",                      \n                      high = \"#006600\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"Diurnal Temp Range in Dec Largely Stable\",      \n       subtitle = \"Dirunal Temp Range by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5),       \n        legend.title = element_text(size = 8),       \n        legend.text = element_text(size = 6)) +   \n  scale_x_discrete(limits = c(n))\n\n#Mean Daily Temp\nggplot(weather,         \n       aes(Day,             \n           Year,             \n           fill = Mean_Temp)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Mean Temp\\n(°C)\",                     \n                      low = \"#CC99CC\",                      \n                      high = \"#660066\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"Mean Daily Temp in Dec Increased between 1983 and 2023\",      \n       subtitle = \"Mean Temp by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5),       \n        legend.title = element_text(size = 8),       \n        legend.text = element_text(size = 6)) +   \n  scale_x_discrete(limits = c(n))\n\n#Daily Rainfall\nggplot(weather,         \n       aes(Day,             \n           Year,             \n           fill = Daily_Rainfall)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Daily Rainfall\\n(mm)\",                     \n                      low = \"#CCCCCC\",                      \n                      high = \"black\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"Daily Rainfall in Dec Largely Stable\",      \n       subtitle = \"Daily Rainfall by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5),       \n        legend.title = element_text(size = 8),       \n        legend.text = element_text(size = 6)) +   \n  scale_x_discrete(limits = c(n))\n\n\n\n\n\nObservations:\n\nMaximum Daily Temperature: There are more days in December 2023 with higher maximum daily temperatures than in December 1983. This means that the hot period of a day in December is getting warmer over the years.\nMinimum Daily Temperature: There are less days in December 2023 with lower minimum daily temperatures than in December 1983. This means that the cool period of a day in December is getting warmer over the years.\nDiurnal Temperature Range: The diurnal temperature range in December has remained largely stable across the years. This is confirmed by the above two points that both the maximum and minimum daily temperatures have been increasing, which means that the range would remain more or less the same.\nMean Daily Temperature: The mean daily temperature in December has increased between 1983 and 2023.\nDaily Rainfall: The daily rainfall amounts in December has also remained largely stable across the years.\n\n\n\n\n\nThe melt() function in the reshape2 package is used to combine the various temperature variables’ values into a single column. The geom_jitter() function in the ggplot2 package is then used to create a dot plot of the temperature variables, with the use of different colour dots to differentiate between the different temperature variables. The geom_jitter() function is used in place of geom_point() to allow the dots to be more spread out and reduce overlaps.\nSelection of Technique: The dot plot is used to depict the individual numerical values (i.e., daily temperatures) for multiple groups (i.e., years). It provides an easy way to visualise the variation in values within the month and across the years. They are also helpful for visualising temporal trends.\nDesign Principles: An informative title is provided. Different colours are used for the dots for different temperature variables. A factual subtitle is included, which also doubles up as a legend for the different dot colours used (thereby removing the need for a legend). The unit of measurement (i.e., °C) is also indicated.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\ntemp = weather %&gt;%   \n  select(1,2,4,5,6) %&gt;%   \n  melt(id = c(\"Year\",\"Day\"))  \n\ncolnames(temp)[3] = \"Temp\"  \n\nggplot(temp,         \n       aes(x = Year,             \n           y = value)) +   \n  geom_jitter(aes(color = Temp), \n              size = 3, \n              alpha = 0.5,               \n              position = position_jitter(width = 0.2)) +   \n  labs(title = \"Overall, Between 1983 and 2023,\\nDaily Temperatures Appears To Be Increasing\",           \n       subtitle = \"&lt;span style='color:#1E90FF'&gt;Mean&lt;/span&gt;, \n       &lt;span style='color:#B22222'&gt;Max&lt;/span&gt;, and \n       &lt;span style='color:#2E8B57'&gt;Min&lt;/span&gt; Daily Temps in Dec\",           \n       x = \"Year\",           \n       y = \"Temp\\n(°C)\",        \n       colour = \"Temp\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1),         \n        plot.subtitle = element_markdown(),         \n        legend.position = \"none\" ) +   \n  scale_y_continuous(breaks = seq(20, 35))\n\n\n\n\n\nObservation: Based on the dot plot, it appears that overall, the temperatures (mean, maximum, and minimum) are increasing across the years. The dots also appear to cluster more closely in 2023 than in 1983.\n\n\n\n\nThe group_by() and summarise() functions in the dplyr package and the sum() function in the base package are used to derive a tibble data frame, rf, containing the total rainfall in December for the different years.\nThe geom_col() function in the ggplot2 package is then used to plot a bar graph of the total rainfall in December for the different years.\nSelection of Technique: The bar graph is used to depict the numerical values (i.e., total rainfall) for multiple groups (i.e., years). It provides an informative way to visualise the total values of a variable across different categories. It is also helpful for comparing the total values and for visualising temporal trends.\nDesign Principles: An informative title is provided, followed by a factual subtitle. The y-axis title is rotated for easier reading. The unit of measurement (i.e., mm) is also indicated. The exact amounts are also indicated on top of each bar for ease of reference.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nrf = weather %&gt;% \n  group_by(Year) %&gt;%\n  summarise(Total_Rainfall = sum(Daily_Rainfall))\n\nggplot(rf, \n       aes(x = Year,\n           y = Total_Rainfall)) +\n  geom_col() +\n  labs(title = \"Between 1983 and 2023,\\nTotal Rainfall in Dec has decreased\",\n          subtitle = \"Total Rainfall in Dec\",\n          x = \"Year\",\n          y = \"Total Rainfall\\n(mm)\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1)) +\n  geom_text(aes(label = Total_Rainfall), vjust = -0.5) +\n  coord_cartesian(ylim = c(0, 400))\n\n\n\n\n\nObservation: Between 1983 and 2023, the total rainfall in December has decreased. This was an interesting finding, and reflects the importance of slicing and dicing data from different angles when analysing a single issue, such as rainfall. The aggregate volume of rainfall in December has actually decreased, whereas there is widespread reports regarding increased rainfall intensities (i.e., large amounts of rain in short periods of time). This has implications for both water supply management (less rainfall overall in December) as well as drainage management (high rainfall within a short period).\n\n\n\n\nThe stat_density_ridges() function in the ggridges package are used to plot the density curves for the four temperature variables (i.e., maximum temperature, minimum temperature, diurnal temperature range, and mean temperature).\nSelection of Technique: The ridgeline plots is used to depict the distribution of continuous numerical values (i.e., temperature) for multiple groups (i.e., years). They provide a compact and informative way to visualise the distribution and shape of each group. They are helpful in identifying patterns, trends, or variations between groups.\nDesign Principles: An informative title is provided, followed by a factual subtitle. The y-axis title is rotated for easier reading. The use of “Quartiles” to colour the plots makes it easy to compare the different values such as median, 25th percentile, and 75th percentile. The unit of measurement (i.e., °C) is also indicated.\n\nMax Daily TempMin Daily TempDiurnal Temp RangeMean Daily TempCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Max Daily Temp\nggplot(weather,        \n       aes(x = Max_Temp,             \n           y = Year,             \n           fill = factor(stat(quantile)))) +   \n  stat_density_ridges(     \n    geom = \"density_ridges_gradient\",     \n    calc_ecdf = TRUE,      \n    quantiles = 4,     \n    quantile_lines = TRUE) +   \n  scale_fill_viridis_d(name = \"Quartiles\") +   \n  labs(title = \"Overall (between 1983 and 2023),\\nMax Daily Temp in Dec has increased\",      \n       subtitle = \"Distribution of Max Daily Temp\",           \n       x = \"Max Daily Temp (°C)\",           \n       y = \"Year\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Min Daily Temp\nggplot(weather,        \n       aes(x = Min_Temp,             \n           y = Year,             \n           fill = factor(stat(quantile)))) +   \n  stat_density_ridges(     \n    geom = \"density_ridges_gradient\",     \n    calc_ecdf = TRUE,      \n    quantiles = 4,     \n    quantile_lines = TRUE) +   \n  scale_fill_viridis_d(name = \"Quartiles\") +   \n  labs(title = \"Between 1983 and 2023,\\nMin Daily Temp in Dec has increased\",           \n       subtitle = \"Distribution of Min Daily Temp\",           \n       x = \"Min Daily Temp (°C)\",           \n       y = \"Year\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n#Diurnal Temp Range\nggplot(weather,        \n       aes(x = Diurnal_Temp_Range,             \n           y = Year,             \n           fill = factor(stat(quantile)))) +   \n  stat_density_ridges(     \n    geom = \"density_ridges_gradient\",     \n    calc_ecdf = TRUE,      \n    quantiles = 4,     \n    quantile_lines = TRUE) +   \n  scale_fill_viridis_d(name = \"Quartiles\") +   \n  labs(title = \"Between 1983 and 2023, Diurnal Temp Range\\nin Dec has become more concentrated at median value\",           \n       subtitle = \"Distribution of Diurnal Temp Range\",           \n       x = \"Diurnal Temp Range (°C)\",           \n       y = \"Year\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Mean Daily Temp\nggplot(weather,        \n       aes(x = Mean_Temp,             \n           y = Year,             \n           fill = factor(stat(quantile)))) +   \n  stat_density_ridges(     \n    geom = \"density_ridges_gradient\",     \n    calc_ecdf = TRUE,      \n    quantiles = 4,     \n    quantile_lines = TRUE) +   \n  scale_fill_viridis_d(name = \"Quartiles\") +   \n  labs(title = \"Between 1983 and 2023,\\nMean Daily Temp in Dec has increased\",           \n       subtitle = \"Distribution of Mean Daily Temp\",           \n       x = \"Mean Daily Temp (°C)\",           \n       y = \"Year\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n\n\n\n\nObservations:\n\nMaximum Daily Temperature: Overall (between 1983 and 2023), the maximum daily temperature in December has increased. The maximum daily temperature values were more spread out in the past, whereas the spread is now narrower.\nMinimum Daily Temperature: Between 1983 and 2023, the minimum daily temperature in December has also increased. In fact, the rise in the median minimum daily temperature is more obvious (i.e., greater) than that for median maximum daily temperature.\nDiurnal Temperature Range: Between 1983 and 2023, the median diurnal temperature range in December has not varied very much but has become more concentrated around the median value (i.e., narrower spread).\nMean Daily Temperature: Between 1983 and 2023, the daily mean temperature in December has increased. Again, the daily mean temperature values were more spread out in the past, whereas the spread is now narrower.\n\n\n\n\n\n\nConfirmatory data analysis (CDA) is then conducted on the temperature and rainfall variables to confirm the statistical significance of some of the observations obtained in the EDA.\nThe ggbetweenstats() function in the ggstatsplot package is used to conduct ANOVA tests to see if there are statistical significance for the variables across the different years.\nSelection of Technique: The combination of box and violin plots with jittered data points along with the statistical details provides confirmation and visualisation of the ANOVA tests between the values in the different years. The nonparametric test is used because, based on the calendar heatmaps and ridgeline plots, we cannot assume that the values are normally distributed.\nDesign Principles: An informative title is provided. The y-axis title is rotated for easier reading, and the unit of measurement (i.e., °C) is also indicated.\n\nMax Daily TempMin Daily TempDiurnal Temp RangeMean Daily TempDaily RainfallCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Max Daily Temp\nggbetweenstats(weather,\n               x = Year, \n               y = Max_Temp,\n               type = \"nonparametric\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE) +    \n  labs(title = \"Statistically Significant Differences in Max Daily Temp in Dec\",           \n       x = \"Year\",           \n       y = \"Max Daily\\nTemp (°C)\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Min Daily Temp\nggbetweenstats(weather,\n               x = Year, \n               y = Min_Temp,\n               type = \"nonparametric\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE) +    \n  labs(title = \"Statistically Significant Differences in Min Daily Temp in Dec\",           \n       x = \"Year\",           \n       y = \"Min Daily\\nTemp (°C)\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Diurnal Temp Range\nggbetweenstats(weather,\n               x = Year, \n               y = Diurnal_Temp_Range,\n               type = \"nonparametric\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE) +    \n  labs(title = \"No Statistically Significant Differences in Diurnal Temp Range in Dec\",           \n       x = \"Year\",           \n       y = \"Diurnal Temp\\nRange (°C)\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Mean Daily Temp\nggbetweenstats(weather,\n               x = Year, \n               y = Mean_Temp,\n               type = \"nonparametric\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE) +    \n  labs(title = \"Statistically Significant Differences in Mean Daily Temp in Dec\",\n       x = \"Year\",           \n       y = \"Mean Daily\\nTemp (°C)\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Total Rainfall\nggbetweenstats(weather,\n               x = Year, \n               y = Max_Temp,\n               type = \"nonparametric\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE) +    \n  labs(title = \"No Statistically Significant Differences in Daily Rainfall in Dec\",           \n       x = \"Year\",           \n       y = \"Daily Rainfall\\n(mm)\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n\n\n\n\nObservations:\n\nMaximum Daily Temperature: There are statistically significant differences in the maximum daily temperatures in December. Comparing pairwise, there are five pairs of years with statistically significant differences.\nMinimum Daily Temperature: There are statistically significant differences in the minimum daily temperatures in December. Comparing pairwise, there are sevenpairs of years with statistically significant differences.\nDiurnal Temperature Range: There are no statistically significant differences in the maximum daily temperatures in December.\nMean Daily Temperature: There are statistically significant differences in the mean daily temperatures in December. Comparing pairwise, there are seven pairs of years with statistically significant differences.\nDaily Rainfall: There are no statistically significant differences in the daily rainfall amounts in December.\n\n\n\n\n\nThe various plots are then put together in a single analytics-driven data visualisation to tell a story about temperature and rainfall at the Changi weather station in December across the five years. The functions used are ggarrange() and annotate_figure() from the ggpubr package.\nThe selection of techniques, design principles, and observations for each sub-plot are found at the respective sub-sections in sections 4 and 5 above.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nc = ggplot(weather,         \n       aes(Day,             \n           Year,             \n           fill = Diurnal_Temp_Range)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Diurnal Temp\\nRange (°C)\",                     \n                      low = \"#99CC99\",                      \n                      high = \"#006600\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"Diurnal Temp Range in Dec Largely Stable\",      \n       subtitle = \"Dirunal Temp Range by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5, size = 8), \n        plot.subtitle = element_text(size = 7),\n        axis.text = element_text(size = 5),          \n        legend.title = element_text(size = 5),       \n        legend.text = element_text(size = 5)) +   \n  scale_x_discrete(limits = c(n))\n\nd = ggplot(temp,         \n       aes(x = Year,             \n           y = value)) +   \n  geom_jitter(aes(color = Temp), \n              size = 3, \n              alpha = 0.5,               \n              position = position_jitter(width = 0.2)) +   \n  labs(title = \"Overall, Between 1983 and 2023,\\nDaily Temperatures Appears To Be Increasing\",           \n       subtitle = \"&lt;span style='color:#1E90FF'&gt;Mean&lt;/span&gt;, \n       &lt;span style='color:#B22222'&gt;Max&lt;/span&gt;, and \n       &lt;span style='color:#2E8B57'&gt;Min&lt;/span&gt; Daily Temps in Dec\",           \n       x = \"Year\",           \n       y = \"Temp\\n(°C)\",        \n       colour = \"Temp\") +   \n  theme(plot.title = element_text(hjust = 0.5, size = 8),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1),         \n        plot.subtitle = element_markdown(size = 7), \n        axis.text = element_text(size = 5),     \n        axis.title = element_text(size = 5),     \n        legend.position = \"none\" ) +\n  scale_y_continuous(breaks = seq(20, 35))\n\nb = ggplot(rf, \n       aes(x = Year,\n           y = Total_Rainfall)) +\n  geom_col() +\n  labs(title = \"Between 1983 and 2023,\\nTotal Rainfall in Dec has decreased\",\n          subtitle = \"Total Rainfall in Dec\",\n          x = \"Year\",\n          y = \"Total Rainfall\\n(mm)\") +\n  theme(plot.title = element_text(hjust = 0.5, size = 8),\n        plot.subtitle = element_text(size = 7),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1),\n        axis.text = element_text(size = 5),     \n        axis.title = element_text(size = 5)) +\n  geom_text(aes(label = Total_Rainfall), vjust = -0.5, size = 2.5) +\n  coord_cartesian(ylim = c(0, 400))\n\n\nr= ggplot(weather,        \n       aes(x = Max_Temp,             \n           y = Year,             \n           fill = factor(stat(quantile)))) +   \n  stat_density_ridges(     \n    geom = \"density_ridges_gradient\",     \n    calc_ecdf = TRUE,      \n    quantiles = 4,     \n    quantile_lines = TRUE) +   \n  scale_fill_viridis_d(name = \"Quartiles\") +   \n  labs(title = \"Overall (between 1983 and 2023),\\nMax Daily Temp in Dec has increased\",\n       subtitle = \"Distribution of Max Daily Temp\",           \n       x = \"Max Daily Temp (°C)\",           \n       y = \"Year\") +   \n  theme(plot.title = element_text(hjust = 0.5, size = 8),         \n        plot.subtitle = element_text(size = 7),\n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1),\n        axis.text = element_text(size = 5),     \n        axis.title = element_text(size = 5),          \n        legend.title = element_text(size = 5),       \n        legend.text = element_text(size = 5))\n\nplot = ggarrange(d, b, c, r,  \n          nrow = 2, \n          ncol = 2)\n          \nannotate_figure(plot,\n                top = text_grob('Changes in December Temperatures and Rainfall Across The Years'),\n                fig.lab.face = \"bold\")\n\n\n\n\n\n\n\nThe dot plot and calendar heatmaps are then converted into interactive plots using the ggplotly() function in the ggplotly package. This would allow users to explore the dataset in further detail. In addition, a time-series dot plot is also created to animate the changes in the mean, maximum, and minimum daily temperatures in December across the five years.\n\nDot PlotTime Series Dot PlotCalendar HeatmapCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Dot Plot\nggplotly(d)\n\n#Time Series Dot Plot\nid = ggplot(temp,         \n       aes(x = Year,             \n           y = value)) +   \n  geom_jitter(aes(color = Temp,\n                  frame = Year), \n              size = 3, \n              alpha = 0.5,               \n              position = position_jitter(width = 0.2)) +   \n  labs(title = \"Overall, Between 1983 and 2023,\\nDaily Temperatures Appears To Be Increasing\", \n       x = \"Year\",           \n       y = \"Temp\\n(°C)\",        \n       colour = \"Temp\") +\n  theme(plot.title = element_text(hjust = 0.5, size = 8),         \n        plot.subtitle = element_text(size = 7),\n        axis.title.y = element_text(angle=360,\n                                    vjust=.5, \n                                    hjust=1),\n        axis.text = element_text(size = 5),     \n        axis.title = element_text(size = 5),          \n        legend.title = element_text(size = 5),       \n        legend.text = element_text(size = 5))\n\nggplotly(id)\n\n#Calendar Heatmaps\ncal = ggplot(temp, \n       aes(Day, \n           Year, \n           fill = value)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  coord_equal() +\n  scale_fill_gradient(name = \"Temp\\n(°C)\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~Temp, ncol = 1) +\n  labs(x = NULL, y = NULL, \n     title = \"Daily Temperatures in Dec\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )\n\nggplotly(cal)\n\n\n\n\n\n\n\nIn conclusion, the ggplot2 package is a powerful package for exploratory data analysis through visualisation. The weather dataset from MSS is interesting and contains useful data for studying Singapore’s weather conditions. The insights gained from analysing the analytical questions posed in this take-home exercise provides a preview of the data analyses that can be conducted in further studies (e.g., expand to include other weather stations across Singapore) to better understand how the various weather conditions have varied across the years. The take-home exercise also highlighted the importance of examining variables from multiple angles (e.g., distribution, variance) and understanding their implications on people (e.g., maximum and minimum temperatures, and diurnal temperature ranges may reveal more about lived experiences as compared to mean temperature).\n\n\n\n\nR for Visual Analytics.\nR for Data Science.\nFundamentals of Data Visualisation.\n\n~~~ End of Take-home Exercise 3 ~~~"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#introduction",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#introduction",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "Climate change is the defining global issue of our time – the world faces an increasingly urgent need to mitigate its underlying causes and adapt to its far-reaching impacts. In this regard, Singapore aims to achieve net zero emissions by 2050.\nAt the same time, Singapore is not spared from the impact of climate change. According to the National Climate Change Secretariat (NCCS), the annual mean temperature increased by 1.1°C from 26.9°C to 28.0°C between 1980 and 2020, and annual total rainfall increased at an average rate of 6.7mm per year between 1980 and 2019. In the future, NCCS expects climate change to “lead to a temperature increase of 1.4°C to 4.6°C and a rise in sea level by up to about 1m by the end of the century”. Also, the “contrast between the wet months (November to January) and dry months (February and June to September) is likely to be more pronounced” and the “intensity and frequency of heavy rainfall events is expected to increase as the world gets warmer”.\n\n\n\n\nIn this take-home exercise, the objective is to use the appropriate interactive visualisation techniques to enhance user experience in the discovery of Singapore’s weather data.\nThe key analytical questions are:\n\nHow have the mean, maximum, and minimum temperatures changed over the years?\nHow have the daily rainfall and total rainfall changed over the years?"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#getting-started",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "The R packages used in this take-home exercise are:\n\ntidyverse (i.e. readr, tidyr, dplyr, ggplot2) for performing data science tasks such as importing, tidying, and wrangling data, as well as creating graphics based on The Grammar of Graphics;\nreshape2 for transforming data between wide and long formats;\nggthemr for aesthetic themes created by user, Ciarán Tobin;\nggtext for improved text rendering support for ggplot2;\nggridges for creating ridgeline plots;\nggpubr for creating publication ready ggplot2 plots;\nplotly for plotting interactive statistical graphs; and\nggstatsplot for creating visual graphics with rich statistical information.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed in the computer. If yes, they are then loaded into the R environment. If no, they are installed, and then loaded into the R environment.\n\npacman::p_load(tidyverse, reshape2,\n               ggthemr, ggtext,\n               ggridges, ggpubr,\n               plotly, ggstatsplot)\n\nThe ggthemr() function in the ggthemr package is used to set the default theme of this take-home exercise as “solarized”.\n\nggthemr(\"solarized\")\n\n\n\n\nThe Meteorological Service Singapore (MSS) provides historical daily records of temperature or rainfall data. For this take-home exercise, the month of December is chosen for the analysis given that it coincides with the Northeast Monsoon that brings about higher rainfall, and consequently, cooler temperatures. The data are taken from 1983, 1993, 2003, 2013, and 2023 (spanning 40 years). The Changi weather station is chosen for the analysis due to the comprehensive weather data collected since 1981/1982, as well as its proximity to Changi airport, which could be affected by changes in weather patterns."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#data-wrangling",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "The five datasets (one for each year) used in this take-home exercise are downloaded from MSS’ website. They are in the CSV file format.\nThe files are imported into the R environment using the read_csv() function in the readr package and stored as the R objects, weather1983, weather1993, weather2003, weather2013, and weather2023.\n\nweather1983 = read_csv(\"data/DAILYDATA_S24_198312.csv\", locale=locale(encoding=\"latin1\"))\nweather1993 = read_csv(\"data/DAILYDATA_S24_199312.csv\", locale=locale(encoding=\"latin1\"))\nweather2003 = read_csv(\"data/DAILYDATA_S24_200312.csv\", locale=locale(encoding=\"latin1\"))\nweather2013 = read_csv(\"data/DAILYDATA_S24_201312.csv\", locale=locale(encoding=\"latin1\"))\nweather2023 = read_csv(\"data/DAILYDATA_S24_202312.csv\")\n\nEach of the tibble data frames has 13 columns (variables) and 31 rows (observations).\n\n\n\nThe rbind() function in the base package is used to combine the five tibble data frames into a single tibble data frame, weather.\n\nweather = rbind(weather1983, \n                weather1993, \n                weather2003, \n                weather2013,\n                weather2023)\n\nrm(weather1983, weather1993, weather2003,\n   weather2013, weather2023)\n\nThe single tibble data frame, weather, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(weather, \"data/weather.rds\")\n\n\nweather = read_rds(\"data/weather.rds\")\n\n\n\n\nThe select() function in the dplyr package and the colnames() function in the base package are then used to obtain and rename the relevant columns respectively.\n\nweather = weather %&gt;% \n  select(c(2,4,5,9,10,11))\n\nnames = c(\"Year\", \"Day\",\n          \"Daily_Rainfall\",\n          \"Mean_Temp\",\n          \"Max_Temp\",\n          \"Min_Temp\")\n\ncolnames(weather) = names\n\nrm(names)\n\nAlso, the as.factor() function in the base package is used to convert the “Year” variable from numerical to factor data type.\n\nweather$Year = as.factor(weather$Year)\n\n\n\n\nThe dataset from MSS is expected to be relatively clean. Nevertheless, due diligence checks for duplicates and missing values are still made to confirm the assumption.\nThe duplicated() function in the base package is used to check for duplicates in weather. There are no duplicates in the tibble data frame.\n\nweather[duplicated(weather), ]\n\n# A tibble: 0 × 7\n# ℹ 7 variables: Year &lt;fct&gt;, Day &lt;dbl&gt;, Daily_Rainfall &lt;dbl&gt;, Mean_Temp &lt;dbl&gt;,\n#   Max_Temp &lt;dbl&gt;, Min_Temp &lt;dbl&gt;, Diurnal_Temp_Range &lt;dbl&gt;\n\n\nThe colSums() function in the base package is used to check for missing values in weather. There are no missing values in the tibble data frame.\n\ncolSums(is.na(weather))\n\n              Year                Day     Daily_Rainfall          Mean_Temp \n                 0                  0                  0                  0 \n          Max_Temp           Min_Temp Diurnal_Temp_Range \n                 0                  0                  0 \n\n\n\n\n\nA new variable, “Diurnal_Temp_Range” (i.e., difference between maximum and minimum daily temperatures) is then derived by subtracting the minimum daily temperatures from the maximum daily temperatures for each row.\n\nweather$Diurnal_Temp_Range = weather$Max_Temp - weather$Min_Temp\n\nThe finalised tibble data frame, weather, is then saved in the rds file format and imported into the R environment.\n\nwrite_rds(weather, \"data/weather.rds\")\n\n\nweather = read_rds(\"data/weather.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#exploratory-data-analysis",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#exploratory-data-analysis",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "Exploratory data analysis (EDA) is conducted on the temperature and rainfall variables to obtain a preliminary understanding of the dataset.\n\n\nThe geom_tile() function in the ggplot2 package is used to plot the calendar heatmaps of the temperature and rainfall variables.\nSelection of Technique: The calendar heatmap is used to depict the continuous numerical values (i.e., daily temperatures or rainfall) for multiple groups (i.e., years) in chronological order from Day 1 to Day 31 in December. It provides an interesting way to visualise the variation in values within the month and across the years. They are helpful for visualising temporal trends in a compact and intuitive manner.\nDesign Principles: An informative title is provided, followed by a factual subtitle. The unit of measurement (i.e., °C or mm) is also indicated. Different colours are used for the heatmaps for different variables.\n\nMax Daily TempMin Daily TempDiurnal Temp RangeMean Daily TempDaily RainfallCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn = 1:31\n\n#Max Daily Temp\nggplot(weather,         \n       aes(Day,             \n           Year,             \n           fill = Max_Temp)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Max Temp\\n(°C)\",\n                      low = \"#FFCCCC\",\n                      high = \"#FF0000\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"More Days in Dec with Higher Max Daily Temp in 2023 than 1983\",      \n       subtitle = \"Max Temp by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5),       \n        legend.title = element_text(size = 8),       \n        legend.text = element_text(size = 6)) +   \n  scale_x_discrete(limits = c(n))\n\n#Min Daily Temp\nggplot(weather,\n       aes(Day, \n           Year,             \n           fill = Min_Temp)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Min Temp\\n(°C)\",                     \n                      low = \"light blue\",                      \n                      high = \"dark blue\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"Less Days in Dec with Lower Min Daily Temp in 2023 than 1983\",      \n       subtitle = \"Min Temp by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5),       \n        legend.title = element_text(size = 8),       \n        legend.text = element_text(size = 6)) +   \n  scale_x_discrete(limits = c(n))\n\n#Diurnal Temp Range\nggplot(weather,         \n       aes(Day,             \n           Year,             \n           fill = Diurnal_Temp_Range)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Diurnal Temp\\nRange (°C)\",                     \n                      low = \"#99CC99\",                      \n                      high = \"#006600\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"Diurnal Temp Range in Dec Largely Stable\",      \n       subtitle = \"Dirunal Temp Range by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5),       \n        legend.title = element_text(size = 8),       \n        legend.text = element_text(size = 6)) +   \n  scale_x_discrete(limits = c(n))\n\n#Mean Daily Temp\nggplot(weather,         \n       aes(Day,             \n           Year,             \n           fill = Mean_Temp)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Mean Temp\\n(°C)\",                     \n                      low = \"#CC99CC\",                      \n                      high = \"#660066\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"Mean Daily Temp in Dec Increased between 1983 and 2023\",      \n       subtitle = \"Mean Temp by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5),       \n        legend.title = element_text(size = 8),       \n        legend.text = element_text(size = 6)) +   \n  scale_x_discrete(limits = c(n))\n\n#Daily Rainfall\nggplot(weather,         \n       aes(Day,             \n           Year,             \n           fill = Daily_Rainfall)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Daily Rainfall\\n(mm)\",                     \n                      low = \"#CCCCCC\",                      \n                      high = \"black\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"Daily Rainfall in Dec Largely Stable\",      \n       subtitle = \"Daily Rainfall by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5),       \n        legend.title = element_text(size = 8),       \n        legend.text = element_text(size = 6)) +   \n  scale_x_discrete(limits = c(n))\n\n\n\n\n\nObservations:\n\nMaximum Daily Temperature: There are more days in December 2023 with higher maximum daily temperatures than in December 1983. This means that the hot period of a day in December is getting warmer over the years.\nMinimum Daily Temperature: There are less days in December 2023 with lower minimum daily temperatures than in December 1983. This means that the cool period of a day in December is getting warmer over the years.\nDiurnal Temperature Range: The diurnal temperature range in December has remained largely stable across the years. This is confirmed by the above two points that both the maximum and minimum daily temperatures have been increasing, which means that the range would remain more or less the same.\nMean Daily Temperature: The mean daily temperature in December has increased between 1983 and 2023.\nDaily Rainfall: The daily rainfall amounts in December has also remained largely stable across the years.\n\n\n\n\n\nThe melt() function in the reshape2 package is used to combine the various temperature variables’ values into a single column. The geom_jitter() function in the ggplot2 package is then used to create a dot plot of the temperature variables, with the use of different colour dots to differentiate between the different temperature variables. The geom_jitter() function is used in place of geom_point() to allow the dots to be more spread out and reduce overlaps.\nSelection of Technique: The dot plot is used to depict the individual numerical values (i.e., daily temperatures) for multiple groups (i.e., years). It provides an easy way to visualise the variation in values within the month and across the years. They are also helpful for visualising temporal trends.\nDesign Principles: An informative title is provided. Different colours are used for the dots for different temperature variables. A factual subtitle is included, which also doubles up as a legend for the different dot colours used (thereby removing the need for a legend). The unit of measurement (i.e., °C) is also indicated.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\ntemp = weather %&gt;%   \n  select(1,2,4,5,6) %&gt;%   \n  melt(id = c(\"Year\",\"Day\"))  \n\ncolnames(temp)[3] = \"Temp\"  \n\nggplot(temp,         \n       aes(x = Year,             \n           y = value)) +   \n  geom_jitter(aes(color = Temp), \n              size = 3, \n              alpha = 0.5,               \n              position = position_jitter(width = 0.2)) +   \n  labs(title = \"Overall, Between 1983 and 2023,\\nDaily Temperatures Appears To Be Increasing\",           \n       subtitle = \"&lt;span style='color:#1E90FF'&gt;Mean&lt;/span&gt;, \n       &lt;span style='color:#B22222'&gt;Max&lt;/span&gt;, and \n       &lt;span style='color:#2E8B57'&gt;Min&lt;/span&gt; Daily Temps in Dec\",           \n       x = \"Year\",           \n       y = \"Temp\\n(°C)\",        \n       colour = \"Temp\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1),         \n        plot.subtitle = element_markdown(),         \n        legend.position = \"none\" ) +   \n  scale_y_continuous(breaks = seq(20, 35))\n\n\n\n\n\nObservation: Based on the dot plot, it appears that overall, the temperatures (mean, maximum, and minimum) are increasing across the years. The dots also appear to cluster more closely in 2023 than in 1983.\n\n\n\n\nThe group_by() and summarise() functions in the dplyr package and the sum() function in the base package are used to derive a tibble data frame, rf, containing the total rainfall in December for the different years.\nThe geom_col() function in the ggplot2 package is then used to plot a bar graph of the total rainfall in December for the different years.\nSelection of Technique: The bar graph is used to depict the numerical values (i.e., total rainfall) for multiple groups (i.e., years). It provides an informative way to visualise the total values of a variable across different categories. It is also helpful for comparing the total values and for visualising temporal trends.\nDesign Principles: An informative title is provided, followed by a factual subtitle. The y-axis title is rotated for easier reading. The unit of measurement (i.e., mm) is also indicated. The exact amounts are also indicated on top of each bar for ease of reference.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nrf = weather %&gt;% \n  group_by(Year) %&gt;%\n  summarise(Total_Rainfall = sum(Daily_Rainfall))\n\nggplot(rf, \n       aes(x = Year,\n           y = Total_Rainfall)) +\n  geom_col() +\n  labs(title = \"Between 1983 and 2023,\\nTotal Rainfall in Dec has decreased\",\n          subtitle = \"Total Rainfall in Dec\",\n          x = \"Year\",\n          y = \"Total Rainfall\\n(mm)\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1)) +\n  geom_text(aes(label = Total_Rainfall), vjust = -0.5) +\n  coord_cartesian(ylim = c(0, 400))\n\n\n\n\n\nObservation: Between 1983 and 2023, the total rainfall in December has decreased. This was an interesting finding, and reflects the importance of slicing and dicing data from different angles when analysing a single issue, such as rainfall. The aggregate volume of rainfall in December has actually decreased, whereas there is widespread reports regarding increased rainfall intensities (i.e., large amounts of rain in short periods of time). This has implications for both water supply management (less rainfall overall in December) as well as drainage management (high rainfall within a short period).\n\n\n\n\nThe stat_density_ridges() function in the ggridges package are used to plot the density curves for the four temperature variables (i.e., maximum temperature, minimum temperature, diurnal temperature range, and mean temperature).\nSelection of Technique: The ridgeline plots is used to depict the distribution of continuous numerical values (i.e., temperature) for multiple groups (i.e., years). They provide a compact and informative way to visualise the distribution and shape of each group. They are helpful in identifying patterns, trends, or variations between groups.\nDesign Principles: An informative title is provided, followed by a factual subtitle. The y-axis title is rotated for easier reading. The use of “Quartiles” to colour the plots makes it easy to compare the different values such as median, 25th percentile, and 75th percentile. The unit of measurement (i.e., °C) is also indicated.\n\nMax Daily TempMin Daily TempDiurnal Temp RangeMean Daily TempCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Max Daily Temp\nggplot(weather,        \n       aes(x = Max_Temp,             \n           y = Year,             \n           fill = factor(stat(quantile)))) +   \n  stat_density_ridges(     \n    geom = \"density_ridges_gradient\",     \n    calc_ecdf = TRUE,      \n    quantiles = 4,     \n    quantile_lines = TRUE) +   \n  scale_fill_viridis_d(name = \"Quartiles\") +   \n  labs(title = \"Overall (between 1983 and 2023),\\nMax Daily Temp in Dec has increased\",      \n       subtitle = \"Distribution of Max Daily Temp\",           \n       x = \"Max Daily Temp (°C)\",           \n       y = \"Year\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Min Daily Temp\nggplot(weather,        \n       aes(x = Min_Temp,             \n           y = Year,             \n           fill = factor(stat(quantile)))) +   \n  stat_density_ridges(     \n    geom = \"density_ridges_gradient\",     \n    calc_ecdf = TRUE,      \n    quantiles = 4,     \n    quantile_lines = TRUE) +   \n  scale_fill_viridis_d(name = \"Quartiles\") +   \n  labs(title = \"Between 1983 and 2023,\\nMin Daily Temp in Dec has increased\",           \n       subtitle = \"Distribution of Min Daily Temp\",           \n       x = \"Min Daily Temp (°C)\",           \n       y = \"Year\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n#Diurnal Temp Range\nggplot(weather,        \n       aes(x = Diurnal_Temp_Range,             \n           y = Year,             \n           fill = factor(stat(quantile)))) +   \n  stat_density_ridges(     \n    geom = \"density_ridges_gradient\",     \n    calc_ecdf = TRUE,      \n    quantiles = 4,     \n    quantile_lines = TRUE) +   \n  scale_fill_viridis_d(name = \"Quartiles\") +   \n  labs(title = \"Between 1983 and 2023, Diurnal Temp Range\\nin Dec has become more concentrated at median value\",           \n       subtitle = \"Distribution of Diurnal Temp Range\",           \n       x = \"Diurnal Temp Range (°C)\",           \n       y = \"Year\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Mean Daily Temp\nggplot(weather,        \n       aes(x = Mean_Temp,             \n           y = Year,             \n           fill = factor(stat(quantile)))) +   \n  stat_density_ridges(     \n    geom = \"density_ridges_gradient\",     \n    calc_ecdf = TRUE,      \n    quantiles = 4,     \n    quantile_lines = TRUE) +   \n  scale_fill_viridis_d(name = \"Quartiles\") +   \n  labs(title = \"Between 1983 and 2023,\\nMean Daily Temp in Dec has increased\",           \n       subtitle = \"Distribution of Mean Daily Temp\",           \n       x = \"Mean Daily Temp (°C)\",           \n       y = \"Year\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n\n\n\n\nObservations:\n\nMaximum Daily Temperature: Overall (between 1983 and 2023), the maximum daily temperature in December has increased. The maximum daily temperature values were more spread out in the past, whereas the spread is now narrower.\nMinimum Daily Temperature: Between 1983 and 2023, the minimum daily temperature in December has also increased. In fact, the rise in the median minimum daily temperature is more obvious (i.e., greater) than that for median maximum daily temperature.\nDiurnal Temperature Range: Between 1983 and 2023, the median diurnal temperature range in December has not varied very much but has become more concentrated around the median value (i.e., narrower spread).\nMean Daily Temperature: Between 1983 and 2023, the daily mean temperature in December has increased. Again, the daily mean temperature values were more spread out in the past, whereas the spread is now narrower."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#confirmatory-data-analysis",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#confirmatory-data-analysis",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "Confirmatory data analysis (CDA) is then conducted on the temperature and rainfall variables to confirm the statistical significance of some of the observations obtained in the EDA.\nThe ggbetweenstats() function in the ggstatsplot package is used to conduct ANOVA tests to see if there are statistical significance for the variables across the different years.\nSelection of Technique: The combination of box and violin plots with jittered data points along with the statistical details provides confirmation and visualisation of the ANOVA tests between the values in the different years. The nonparametric test is used because, based on the calendar heatmaps and ridgeline plots, we cannot assume that the values are normally distributed.\nDesign Principles: An informative title is provided. The y-axis title is rotated for easier reading, and the unit of measurement (i.e., °C) is also indicated.\n\nMax Daily TempMin Daily TempDiurnal Temp RangeMean Daily TempDaily RainfallCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Max Daily Temp\nggbetweenstats(weather,\n               x = Year, \n               y = Max_Temp,\n               type = \"nonparametric\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE) +    \n  labs(title = \"Statistically Significant Differences in Max Daily Temp in Dec\",           \n       x = \"Year\",           \n       y = \"Max Daily\\nTemp (°C)\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Min Daily Temp\nggbetweenstats(weather,\n               x = Year, \n               y = Min_Temp,\n               type = \"nonparametric\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE) +    \n  labs(title = \"Statistically Significant Differences in Min Daily Temp in Dec\",           \n       x = \"Year\",           \n       y = \"Min Daily\\nTemp (°C)\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Diurnal Temp Range\nggbetweenstats(weather,\n               x = Year, \n               y = Diurnal_Temp_Range,\n               type = \"nonparametric\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE) +    \n  labs(title = \"No Statistically Significant Differences in Diurnal Temp Range in Dec\",           \n       x = \"Year\",           \n       y = \"Diurnal Temp\\nRange (°C)\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Mean Daily Temp\nggbetweenstats(weather,\n               x = Year, \n               y = Mean_Temp,\n               type = \"nonparametric\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE) +    \n  labs(title = \"Statistically Significant Differences in Mean Daily Temp in Dec\",\n       x = \"Year\",           \n       y = \"Mean Daily\\nTemp (°C)\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n#Total Rainfall\nggbetweenstats(weather,\n               x = Year, \n               y = Max_Temp,\n               type = \"nonparametric\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE, \n               pairwise.display = \"s\",\n               p.adjust.method = \"fdr\",\n               messages = FALSE) +    \n  labs(title = \"No Statistically Significant Differences in Daily Rainfall in Dec\",           \n       x = \"Year\",           \n       y = \"Daily Rainfall\\n(mm)\") +   \n  theme(plot.title = element_text(hjust = 0.5),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1))\n\n\n\n\n\nObservations:\n\nMaximum Daily Temperature: There are statistically significant differences in the maximum daily temperatures in December. Comparing pairwise, there are five pairs of years with statistically significant differences.\nMinimum Daily Temperature: There are statistically significant differences in the minimum daily temperatures in December. Comparing pairwise, there are sevenpairs of years with statistically significant differences.\nDiurnal Temperature Range: There are no statistically significant differences in the maximum daily temperatures in December.\nMean Daily Temperature: There are statistically significant differences in the mean daily temperatures in December. Comparing pairwise, there are seven pairs of years with statistically significant differences.\nDaily Rainfall: There are no statistically significant differences in the daily rainfall amounts in December."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#composite-plot",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#composite-plot",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "The various plots are then put together in a single analytics-driven data visualisation to tell a story about temperature and rainfall at the Changi weather station in December across the five years. The functions used are ggarrange() and annotate_figure() from the ggpubr package.\nThe selection of techniques, design principles, and observations for each sub-plot are found at the respective sub-sections in sections 4 and 5 above.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nc = ggplot(weather,         \n       aes(Day,             \n           Year,             \n           fill = Diurnal_Temp_Range)) +  \n  geom_tile(color = \"white\",            \n            size = 0.1) +  \n  coord_equal() + \n  scale_fill_gradient(name = \"Diurnal Temp\\nRange (°C)\",                     \n                      low = \"#99CC99\",                      \n                      high = \"#006600\") + \n  labs(x = NULL,       \n       y = NULL,       \n       title = \"Diurnal Temp Range in Dec Largely Stable\",      \n       subtitle = \"Dirunal Temp Range by Year and Day in Dec\") + \n  theme(axis.ticks = element_blank(),       \n        plot.title = element_text(hjust = 0.5, size = 8), \n        plot.subtitle = element_text(size = 7),\n        axis.text = element_text(size = 5),          \n        legend.title = element_text(size = 5),       \n        legend.text = element_text(size = 5)) +   \n  scale_x_discrete(limits = c(n))\n\nd = ggplot(temp,         \n       aes(x = Year,             \n           y = value)) +   \n  geom_jitter(aes(color = Temp), \n              size = 3, \n              alpha = 0.5,               \n              position = position_jitter(width = 0.2)) +   \n  labs(title = \"Overall, Between 1983 and 2023,\\nDaily Temperatures Appears To Be Increasing\",           \n       subtitle = \"&lt;span style='color:#1E90FF'&gt;Mean&lt;/span&gt;, \n       &lt;span style='color:#B22222'&gt;Max&lt;/span&gt;, and \n       &lt;span style='color:#2E8B57'&gt;Min&lt;/span&gt; Daily Temps in Dec\",           \n       x = \"Year\",           \n       y = \"Temp\\n(°C)\",        \n       colour = \"Temp\") +   \n  theme(plot.title = element_text(hjust = 0.5, size = 8),         \n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1),         \n        plot.subtitle = element_markdown(size = 7), \n        axis.text = element_text(size = 5),     \n        axis.title = element_text(size = 5),     \n        legend.position = \"none\" ) +\n  scale_y_continuous(breaks = seq(20, 35))\n\nb = ggplot(rf, \n       aes(x = Year,\n           y = Total_Rainfall)) +\n  geom_col() +\n  labs(title = \"Between 1983 and 2023,\\nTotal Rainfall in Dec has decreased\",\n          subtitle = \"Total Rainfall in Dec\",\n          x = \"Year\",\n          y = \"Total Rainfall\\n(mm)\") +\n  theme(plot.title = element_text(hjust = 0.5, size = 8),\n        plot.subtitle = element_text(size = 7),\n        axis.title.y = element_text(angle=360, \n                                    vjust=.5, hjust=1),\n        axis.text = element_text(size = 5),     \n        axis.title = element_text(size = 5)) +\n  geom_text(aes(label = Total_Rainfall), vjust = -0.5, size = 2.5) +\n  coord_cartesian(ylim = c(0, 400))\n\n\nr= ggplot(weather,        \n       aes(x = Max_Temp,             \n           y = Year,             \n           fill = factor(stat(quantile)))) +   \n  stat_density_ridges(     \n    geom = \"density_ridges_gradient\",     \n    calc_ecdf = TRUE,      \n    quantiles = 4,     \n    quantile_lines = TRUE) +   \n  scale_fill_viridis_d(name = \"Quartiles\") +   \n  labs(title = \"Overall (between 1983 and 2023),\\nMax Daily Temp in Dec has increased\",\n       subtitle = \"Distribution of Max Daily Temp\",           \n       x = \"Max Daily Temp (°C)\",           \n       y = \"Year\") +   \n  theme(plot.title = element_text(hjust = 0.5, size = 8),         \n        plot.subtitle = element_text(size = 7),\n        axis.title.y = element_text(angle=360,                                      \n                                    vjust=.5, \n                                    hjust=1),\n        axis.text = element_text(size = 5),     \n        axis.title = element_text(size = 5),          \n        legend.title = element_text(size = 5),       \n        legend.text = element_text(size = 5))\n\nplot = ggarrange(d, b, c, r,  \n          nrow = 2, \n          ncol = 2)\n          \nannotate_figure(plot,\n                top = text_grob('Changes in December Temperatures and Rainfall Across The Years'),\n                fig.lab.face = \"bold\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#interactive-plots",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#interactive-plots",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "The dot plot and calendar heatmaps are then converted into interactive plots using the ggplotly() function in the ggplotly package. This would allow users to explore the dataset in further detail. In addition, a time-series dot plot is also created to animate the changes in the mean, maximum, and minimum daily temperatures in December across the five years.\n\nDot PlotTime Series Dot PlotCalendar HeatmapCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Dot Plot\nggplotly(d)\n\n#Time Series Dot Plot\nid = ggplot(temp,         \n       aes(x = Year,             \n           y = value)) +   \n  geom_jitter(aes(color = Temp,\n                  frame = Year), \n              size = 3, \n              alpha = 0.5,               \n              position = position_jitter(width = 0.2)) +   \n  labs(title = \"Overall, Between 1983 and 2023,\\nDaily Temperatures Appears To Be Increasing\", \n       x = \"Year\",           \n       y = \"Temp\\n(°C)\",        \n       colour = \"Temp\") +\n  theme(plot.title = element_text(hjust = 0.5, size = 8),         \n        plot.subtitle = element_text(size = 7),\n        axis.title.y = element_text(angle=360,\n                                    vjust=.5, \n                                    hjust=1),\n        axis.text = element_text(size = 5),     \n        axis.title = element_text(size = 5),          \n        legend.title = element_text(size = 5),       \n        legend.text = element_text(size = 5))\n\nggplotly(id)\n\n#Calendar Heatmaps\ncal = ggplot(temp, \n       aes(Day, \n           Year, \n           fill = value)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  coord_equal() +\n  scale_fill_gradient(name = \"Temp\\n(°C)\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~Temp, ncol = 1) +\n  labs(x = NULL, y = NULL, \n     title = \"Daily Temperatures in Dec\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )\n\nggplotly(cal)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#conclusion",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "In conclusion, the ggplot2 package is a powerful package for exploratory data analysis through visualisation. The weather dataset from MSS is interesting and contains useful data for studying Singapore’s weather conditions. The insights gained from analysing the analytical questions posed in this take-home exercise provides a preview of the data analyses that can be conducted in further studies (e.g., expand to include other weather stations across Singapore) to better understand how the various weather conditions have varied across the years. The take-home exercise also highlighted the importance of examining variables from multiple angles (e.g., distribution, variance) and understanding their implications on people (e.g., maximum and minimum temperatures, and diurnal temperature ranges may reveal more about lived experiences as compared to mean temperature)."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#key-references",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#key-references",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "R for Visual Analytics.\nR for Data Science.\nFundamentals of Data Visualisation.\n\n~~~ End of Take-home Exercise 3 ~~~"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#introduction",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#introduction",
    "title": "In-class Exercise 7",
    "section": "",
    "text": "An isohyet map is a surface map of some precipitation: rain, snow, and others.\nIn order to prepare an isohyet map, spatial interpolation will be used. Spatial interpolation is the process of using points with known values to estimate values at other unknown points. For example, to make a rainfall above, we will not find enough evenly spread weather stations to cover the entire region. Spatial interpolation can estimate the rainfall at locations without recorded data by using known rainfall readings at nearby weather stations (see figure_temperature_map). This type of interpolated surface is often called a geostatistical surface. Elevation data, temperature, property prices, air quality index and population density are other types of data that can be computed using interpolation.\nThere are many interpolation methods. In this in-class exercise, two widely used spatial interpolation methods called Inverse Distance Weighting (IDW) and kriging will be introduced."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#getting-started",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#getting-started",
    "title": "In-class Exercise 7",
    "section": "",
    "text": "In this hands-on exercise, the following R packages are used:\n\ntidyverse (i.e. readr, tidyr, dplyr) for performing data science tasks such as importing, tidying, and wrangling data;\nsf for importing, managing, and processing geospatial data;\nterra for spatial data analysis (a replacement of the raster package. It has a very similar, but simpler, interface, and it is faster than raster. In this hands-on exercise, it will be used to create grid (also known as raster) objects as the input and output of spatial interpolation);\ngstat for spatial and spatio-temporal geostatistical modelling, prediction and simulation.\ntmap for thematic maps;\nautomap for performing automatic variogram modelling and kriging interpolation; and\nviridis for colour-blind friendly colour maps.\n\n\npacman::p_load(sf, terra, gstat, automap,\n               tmap, viridis, tidyverse)\n\n\n\n\nThree data sets will be used in this exercise, they are:\n\nRainfallStation.csv provides location information of existing rainfall stations in Singapore. The data is downloaded from Meteological Service Singapore.\nDAILYDATA_202402.csv provides weather data are rainfall stations for the month February, 2024. The data is also downloaded from Meteological Service Singapore.\nMPSZ-2019 contains planning subzone boundary of URA Master Plan 2019. It is downloaded from data.gov.sg. The original data is in kml format.\n\n\n\nThe dataset with name, latitude, and longitude of each weather station is imported using the read_csv() function in the readr package.\n\nrfstations = read_csv(\"data/aspatial/RainfallStation.csv\")\n\n\n\n\nThe dataset with the daily rainfall data is imported using the read_csv() function in the readr package, and processed using the select(), group_by(), summarise() and ungroup() functions in the dplyr package to obtain the total monthly rainfall by station.\n\nrfdata = read_csv(\"data/aspatial/DAILYDATA_202402.csv\") %&gt;%\n  select(c(1,5)) %&gt;%\n  group_by(Station) %&gt;%\n  summarise(MONTHSUM = sum(`Daily Rainfall Total (mm)`)) %&gt;%\n  ungroup()\n\n\n\n\nThe two datasets are then combined using the left_join() function in the dplyr package by the names of the weather stations.\n\nrfdata = rfdata %&gt;%\n  left_join(rfstations)\n\nThe combined dataset is then converted to an simple feature object using the st_as_sf() function in the sf package. The longitudes and latitudes are inserted as sf geometry points in the dataset. The st_transform() function in the sf package is used to transform the coordinate system (from wgs48 to svy21 projected coordinates system). 3414 is the EPSG code of svy21, which is the official projected coordinates of Singapore.\n\nrfdata_sf = st_as_sf(rfdata,\n                     coords = c(\"Longitude\",\n                                \"Latitude\"),\n                     crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n\n\nThe spatial shapefile, MPSZ-2019, is then imported using the st_read() function in the sf package and transformed using the st_transform() function in the sf package.\n\nmpsz2019 = st_read(dsn = \"data/geospatial\",\n                   layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\jmphosis\\ISSS608\\In-class_Ex\\In-class_Ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#visualisation",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#visualisation",
    "title": "In-class Exercise 7",
    "section": "",
    "text": "It is always a good practice to visualise the data prepared. In the code chunk below, tmap functions are used to create a dot map showing locations of rainfall station in Singapore.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\ntm_shape(rfdata_sf) +\n  tm_dots(col = \"red\")\n\n\n\n\n\ntmap_mode(\"plot\")\n\nThe functions in the tmap package are then used to plot the total monthly rainfall by weather stations based on their geographical locations.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\ntm_shape(mpsz2019) +\n  tm_borders() +\n  tm_shape(rfdata_sf) +\n  tm_dots(col = 'MONTHSUM')\n\n\n\n\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#spatial-interpolation-gstat-method",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#spatial-interpolation-gstat-method",
    "title": "In-class Exercise 7",
    "section": "",
    "text": "In order to perform spatial interpolation using the gstat package, we first need to create an object of class called gstat, using a function of the same name: gstat(). A gstat object contains all necessary information to conduct spatial interpolation, namely:\n\nThe model definition\nThe calibration data\n\nBased on its arguments, the gstat function “understands” what type of interpolation model we want to use:\n\nNo variogram model → IDW\nVariogram model, no covariates → Ordinary Kriging\nVariogram model, with covariates → Universal Kriging\n\nThe complete decision tree of the gstat() function is shown in the figure below.\n\n\n\nTo get start, we need to create a grid data object using the rast() function in the terra package.\nNext, a list called xy will be created using the xyFromCell() function in the terra package. The function gets coordinates of the center of raster cells for a row, column, or cell number of a SpatRaster. Or get row, column, or cell numbers from coordinates or from each other.\n\ngrid = terra::rast(mpsz2019,\n                   nrows = 690,\n                   ncols = 1075)\n\nxy = terra::xyFromCell(grid,\n                       1:ncell(grid))\n\nThen, a data frame, coop, is created with prediction/simulation locations.\n\ncoop = st_as_sf(as.data.frame(xy), \n                 coords = c(\"x\", \"y\"),\n                 crs = st_crs(mpsz2019))\n\ncoop = st_filter(coop, mpsz2019)\nhead(coop)\n\nSimple feature collection with 6 features and 0 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 25883.42 ymin: 50231.33 xmax: 26133.32 ymax: 50231.33\nProjected CRS: SVY21 / Singapore TM\n                   geometry\n1 POINT (25883.42 50231.33)\n2  POINT (25933.4 50231.33)\n3 POINT (25983.38 50231.33)\n4 POINT (26033.36 50231.33)\n5 POINT (26083.34 50231.33)\n6 POINT (26133.32 50231.33)\n\n\n\n\n\nIn the IDW interpolation method, the sample points are weighted during interpolation such that the influence of one point relative to another declines with distance from the unknown point you want to create.\nWeighting is assigned to sample points through the use of a weighting coefficient that controls how the weighting influence will drop off as the distance from new point increases. The greater the weighting coefficient, the less the effect points will have if they are far from the unknown point during the interpolation process. As the coefficient increases, the value of the unknown point approaches the value of the nearest observational point.\nIt is important to notice that the IDW interpolation method also has some disadvantages: the quality of the interpolation result can decrease, if the distribution of sample data points is uneven. Furthermore, maximum and minimum values in the interpolated surface can only occur at sample data points. This often results in small peaks and pits around the sample data points.\nThree parameters of the gstat() function are used:\n\nformula: The prediction “formula” specifying the dependent and the independent variables (covariates)\ndata: The calibration data\nmodel: The variogram model\n\nWe need to specify parameter names, because these three parameters are not the first three in the function definition.\n\nNote: In R, formula objects are used to specify relation between objects, in particular—the role of different data columns in statistical models. A formula object is created using the ~ operator, which separates names of dependent variables (to the left of the ~ symbol) and independent variables (to the right of the ~ symbol). Writing 1 to the right of the ~ symbol, as in ~ 1, means that there are no independent variables\n\n\nres = gstat(formula = MONTHSUM ~ 1, \n             locations = rfdata_sf, \n             nmax = 5,\n             set = list(idp = 0))\n\nNow that the model is defined, the predict() function is used to interpolate, i.e., to calculate predicted values. The predict function accepts:\n\nA raster—stars object, such as dem\nA model—gstat object, such as g\n\nThe raster serves for two purposes:\n\nSpecifying the locations where we want to make predictions (in all methods), and\nSpecifying covariate values (in Universal Kriging only).\n\n\nresp = predict(res, coop)\n\n[inverse distance weighted interpolation]\n\nresp$x = st_coordinates(resp)[,1]\nresp$y = st_coordinates(resp)[,2]\nresp$pred = resp$var1.pred\n\npred = terra::rasterize(resp, grid, \n                         field = \"pred\", \n                         fun = \"mean\")\n\nThen, the interpolated surface is mapped using tmap functions.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(pred) + \n  tm_raster(alpha = 0.6, \n            palette = \"viridis\")\n\n\n\n\n\n\n\nKriging is one of several methods that use a limited set of sampled data points to estimate the value of a variable over a continuous spatial field. An example of a value that varies across a random spatial field might be total monthly rainfall over Singapore. It differs from Inverse Distance Weighted Interpolation discussed earlier in that it uses the spatial correlation between sampled points to interpolate the values in the spatial field: the interpolation is based on the spatial arrangement of the empirical observations, rather than on a presumed model of spatial distribution. Kriging also generates estimates of the uncertainty surrounding each interpolated value.\nIn a general sense, the kriging weights are calculated such that points nearby to the location of interest are given more weight than those farther away. Clustering of points is also taken into account, so that clusters of points are weighted less heavily (in effect, they contain less information than single points). This helps to reduce bias in the predictions.\nThe kriging predictor is an “optimal linear predictor” and an exact interpolator, meaning that each interpolated value is calculated to minimize the prediction error for that point. The value that is generated from the kriging process for any actually sampled location will be equal to the observed value at this point, and all the interpolated values will be the Best Linear Unbiased Predictors (BLUPs).\nKriging will in general not be more effective than simpler methods of interpolation if there is little spatial autocorrelation among the sampled data points (that is, if the values do not co-vary in space). If there is at least moderate spatial autocorrelation, however, kriging can be a helpful method to preserve spatial variability that would be lost using a simpler method (for an example, see Auchincloss 2007, below).\nKriging can be understood as a two-step process:\n\nfirst, the spatial covariance structure of the sampled points is determined by fitting a variogram; and\nsecond, weights derived from this covariance structure are used to interpolate values for unsampled points or blocks across the spatial field.\n\nKriging methods require a variogram model. A variogram (sometimes called a “semivariogram”) is a visual depiction of the covariance exhibited between each pair of points in the sampled data. For each pair of points in the sampled data, the gamma-value or “semi-variance” (a measure of the half mean-squared difference between their values) is plotted against the distance, or “lag”, between them. The “experimental” variogram is the plot of observed values, while the “theoretical” or “model” variogram is the distributional model that best fits the data.\n\nThe empirical variogram is calcualted using the variogram() function in the gstat package. The function requires two arguments:\n\nformula, the dependent variable and the covariates; and\ndata, a point layer with the dependent variable and covariates as attributes.\n\n\nv = variogram(MONTHSUM ~ 1, \n               data = rfdata_sf)\nplot(v)\n\n\n\n\nThe theoretical plots re then compared:\n\nWith reference to the comparison above, an empirical variogram model will be fitted using the fit.variogram()function in the gstat package.\n\nfv = fit.variogram(object = v,\n                    model = vgm(\n                      psill = 0.5, \n                      model = \"Sph\",\n                      range = 5000, \n                      nugget = 0.1))\nfv\n\n  model     psill    range\n1   Nug 0.1129190    0.000\n2   Sph 0.5292397 5213.396\n\n\nWe can visualise how well the observed data fit the model by plotting fv.\n\nplot(v, fv)\n\n\n\n\nThe plot above reveals that the empirical model fits rather well. In view of this, spatial interpolation is performed using the newly derived model.\n\nk = gstat(formula = MONTHSUM ~ 1, \n           data = rfdata_sf, \n           model = fv)\nk\n\ndata:\nvar1 : formula = MONTHSUM`~`1 ; data dim = 43 x 2\nvariograms:\n        model     psill    range\nvar1[1]   Nug 0.1129190    0.000\nvar1[2]   Sph 0.5292397 5213.396\n\n\nOnce done, the predict() function in the gstat package will be used to estimate the unknown grids.\n\nresp = predict(k, coop)\n\n[using ordinary kriging]\n\nresp$x = st_coordinates(resp)[,1]\nresp$y = st_coordinates(resp)[,2]\nresp$pred = resp$var1.pred\nresp$pred = resp$pred\nresp\n\nSimple feature collection with 314019 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2692.528 ymin: 15773.73 xmax: 56371.45 ymax: 50231.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   var1.pred  var1.var                  geometry        x        y     pred\n1   131.0667 0.6608399 POINT (25883.42 50231.33) 25883.42 50231.33 131.0667\n2   130.9986 0.6610337  POINT (25933.4 50231.33) 25933.40 50231.33 130.9986\n3   130.9330 0.6612129 POINT (25983.38 50231.33) 25983.38 50231.33 130.9330\n4   130.8698 0.6613782 POINT (26033.36 50231.33) 26033.36 50231.33 130.8698\n5   130.8092 0.6615303 POINT (26083.34 50231.33) 26083.34 50231.33 130.8092\n6   130.7514 0.6616697 POINT (26133.32 50231.33) 26133.32 50231.33 130.7514\n7   130.6965 0.6617971  POINT (26183.3 50231.33) 26183.30 50231.33 130.6965\n8   130.6446 0.6619131 POINT (26233.28 50231.33) 26233.28 50231.33 130.6446\n9   130.5958 0.6620184 POINT (26283.26 50231.33) 26283.26 50231.33 130.5958\n10  132.5484 0.6542154 POINT (25033.76 50181.32) 25033.76 50181.32 132.5484\n\n\nIn order to create a raster surface data object, the rasterize() function in the terra package is used.\n\nkpred = terra::rasterize(resp, grid, \n                         field = \"pred\")\nkpred\n\nclass       : SpatRaster \ndimensions  : 690, 1075, 1  (nrow, ncol, nlyr)\nresolution  : 49.98037, 50.01103  (x, y)\nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncoord. ref. : SVY21 / Singapore TM (EPSG:3414) \nsource(s)   : memory\nname        :      last \nmin value   :  72.77826 \nmax value   : 195.53284 \n\n\nThe output object, kpred, is in SpatRaster object class with a spatial resolution of 50m x 50m. It consists of 1,075 columns and 690 rows and in SVY21 projected coordinates system.\n\n\n\nFinally, the tmap functions are used to map the interpolated rainfall raster (i.e. kpred).\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(kpred) + \n  tm_raster(alpha = 0.6, \n            palette = \"viridis\",\n            title = \"Total monthly rainfall (mm)\") +\n  tm_layout(main.title = \"Distribution of monthly rainfall, Feb 2024\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)\n\n\n\n\n\n\n\nBeside using the gstat package to perform variogram modelling manually, the autofirVariogram() function in the automap package can be used to perform varigram modelling.\n\nv_auto = autofitVariogram(MONTHSUM ~ 1, \n                           rfdata_sf)\nplot(v_auto)\n\n\n\n\n\nv_auto\n\n$exp_var\n   np      dist     gamma dir.hor dir.ver   id\n1  15  1957.436  311.9613       0       0 var1\n2  33  3307.349  707.7685       0       0 var1\n3  54  4861.368  848.1314       0       0 var1\n4 116  6716.531  730.3969       0       0 var1\n5 111  9235.708 1006.5381       0       0 var1\n6 120 11730.199 1167.5988       0       0 var1\n7 135 14384.636 1533.5903       0       0 var1\n\n$var_model\n  model    psill   range kappa\n1   Nug     0.00       0   0.0\n2   Ste 24100.71 1647955   0.3\n\n$sserr\n[1] 0.2178294\n\nattr(,\"class\")\n[1] \"autofitVariogram\" \"list\"            \n\n\n\nk = gstat(formula = MONTHSUM ~ 1, \n           model = v_auto$var_model,\n           data = rfdata_sf)\nk\n\ndata:\nvar1 : formula = MONTHSUM`~`1 ; data dim = 43 x 2\nvariograms:\n        model    psill   range kappa\nvar1[1]   Nug     0.00       0   0.0\nvar1[2]   Ste 24100.71 1647955   0.3\n\n\n\nresp = predict(k, coop)\n\n[using ordinary kriging]\n\nresp$x = st_coordinates(resp)[,1]\nresp$y = st_coordinates(resp)[,2]\nresp$pred = resp$var1.pred\nresp$pred = resp$pred\n\nkpred = terra::rasterize(resp, grid, \n                         field = \"pred\")\n\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(kpred) + \n  tm_raster(alpha = 0.6, \n            palette = \"viridis\",\n            title = \"Total monthly rainfall (mm)\") +\n  tm_layout(main.title = \"Distribution of monthly rainfall, Feb 2024\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#reference",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#reference",
    "title": "In-class Exercise 7",
    "section": "",
    "text": "Olea, Ricardo A. (2006-07) “A six-step practical approach to semivariogram modeling”, Stochastic Environmental Research and Risk Assessment, 2006-07, Vol.20 (5), p.307-318. SMU e-journal.\n~~~ End of In-class Exercise 7 ~~~"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html",
    "title": "In-class Exercise 8",
    "section": "",
    "text": "&lt;placeholder&gt;\n~~~ End of In-class Exercise 8 ~~~"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html",
    "title": "In-class Exercise 9",
    "section": "",
    "text": "&lt;placeholder&gt;\n~~~ End of In-class Exercise 9 ~~~"
  }
]